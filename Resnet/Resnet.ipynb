{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "num_classes = 10\n",
    "\n",
    "valid_percentage = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)\n",
    "\n",
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104153325358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104162456690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1),需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "block = BasicBlock(64, 64)\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockWithOutShortcut(nn.Module):\n",
    "    # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlockWithOutShortcut, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 批量规范化层,对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1),需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "block = BasicBlock(64, 64)\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=1, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        # out = F.relu(self.shortcut(x) + self.bn3(self.conv3(out)))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckWithoutShortcut(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(BottleneckWithoutShortcut, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = F.relu(self.bn3(self.conv3(out)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # 考虑到CIFAR10数据集的图片尺寸太小，ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息\n",
    "        # 所以在实验中我们将7x7的降采样层和最大池化层去掉，替换为一个3x3的降采样卷积，同时减小该卷积层的步长和填充大小，\n",
    "        # 这样可以尽可能保留原始图像的信息。\n",
    "        self.conv1 = conv3x3(3, 64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet18WithoutShortcut(num_classes):\n",
    "    return ResNet(BasicBlockWithOutShortcut, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "# summary(Resnet18WithoutShortcut(num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet18(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(num_classes):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def Resnet34WithoutShortcut(num_classes):\n",
    "    return ResNet(BasicBlockWithOutShortcut, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet34(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def Resnet50WithoutShortcut(num_classes):\n",
    "    return ResNet(BottleneckWithoutShortcut, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# summary(ResNet50(num_classes=num_classes), (1, 3, 32, 32))\n",
    "# summary(Resnet50WithoutShortcut(10), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet101(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet101(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet152(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据增强变换，用于训练集\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # 随机裁剪，填充4个像素\n",
    "        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # 颜色抖动\n",
    "        transforms.RandomRotation(15),  # 随机旋转\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 用于验证集和测试集的变换\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_train)\n",
    "valid_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_test)\n",
    "test_data = CIFAR10(\"../data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_percentage * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# 无放回地按照给定的索引列表采样样本元素\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset(loader, set_name):\n",
    "    print(f\"{set_name} Set:\")\n",
    "    images, labels = next(iter(loader))\n",
    "    print(\"batch count\", len(loader))\n",
    "    print(\"image size per batch\", images.size())\n",
    "    print(\"label size per batch\", labels.size())\n",
    "\n",
    "\n",
    "check_dataset(train_loader, \"Training\")\n",
    "check_dataset(valid_loader, \"Valid\")\n",
    "check_dataset(test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    class_total = [0.0 for _ in range(num_classes)]\n",
    "    class_correct = [0.0 for _ in range(num_classes)]\n",
    "    sum_loss, num_correct, num_examples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the model output\n",
    "            outputs = model(features)\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute the correct radix\n",
    "            num_examples += targets.size(0)\n",
    "            num_correct += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # compute each class 's correct count\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_correct[label] += (predicted_labels[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = num_correct / num_examples * 100\n",
    "    avg_loss = sum_loss / num_examples\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"class_correct\": class_correct,\n",
    "        \"class_total\": class_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs= 25,\n",
    "    model_name= \"model\",\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    scheduler=None,\n",
    ") -> dict:\n",
    "    log_dict = {\n",
    "        \"train_loss_per_batch\": [],\n",
    "        \"train_acc_per_epoch\": [],\n",
    "        \"valid_acc_per_epoch\": [],\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"valid_loss_per_epoch\": [],\n",
    "        \"valid_loss_min\": np.Inf,\n",
    "        \"learning_rates\": [],\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"learning_rates\"].append(current_lr)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        ###################\n",
    "        # 训练 #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader, 0):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # step1: predict the output\n",
    "            outputs = model(features)\n",
    "\n",
    "            # step2: loss backpropagation\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # step3: update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            log_dict[\"train_loss_per_batch\"].append(loss.item())\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        #! each epoch, evaluate the model\n",
    "        ######################\n",
    "        # 验证 #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            train_eval_res = eval_model(model, train_loader)\n",
    "            train_acc = train_eval_res[\"accuracy\"]\n",
    "            train_loss = train_eval_res[\"avg_loss\"]\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Train. Acc.: {train_acc:.3f}% | Loss: {train_loss:.4f}\"\n",
    "            )\n",
    "            log_dict[\"train_loss_per_epoch\"].append(train_loss)\n",
    "            log_dict[\"train_acc_per_epoch\"].append(train_acc)\n",
    "\n",
    "            # * each epoch, evaluate the model on the validation dataset which is not used for training\n",
    "            valid_eval_res = eval_model(model, valid_loader)\n",
    "            valid_acc = valid_eval_res[\"accuracy\"]\n",
    "            valid_loss = valid_eval_res[\"avg_loss\"]\n",
    "            log_dict[\"valid_loss_per_epoch\"].append(valid_loss)\n",
    "            log_dict[\"valid_acc_per_epoch\"].append(valid_acc)\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Valid. Acc.: {valid_acc:.3f}% | Loss: {valid_loss:.4f}\"\n",
    "            )\n",
    "            # * save the model if the validation loss is decreased\n",
    "            if valid_loss <= log_dict[\"valid_loss_min\"]:\n",
    "                print(\n",
    "                    f\"**Validation loss decreased ({log_dict['valid_loss_min']:.6f} --> {valid_loss:.6f}). Saving model ...\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), f\"{model_name}_cifar.pt\")\n",
    "                log_dict[\"valid_loss_min\"] = valid_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # scheduler.step()\n",
    "            scheduler.step(valid_loss)\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "# model = ResNet18(num_classes=10).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# log_dict = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     num_epochs=20,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     scheduler=scheduler,\n",
    "#     model_name=\"ResNet34\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # model.load_state_dict(torch.load(\"model_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(log_dict: dict, num_epochs: int):\n",
    "    loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "    train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "    valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "    learning_rates = log_dict[\"learning_rates\"]\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "    train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "    valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "\n",
    "    running_avg_loss = np.convolve(loss_list, np.ones(200) / 200, mode=\"valid\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axs[0].plot(train_loss_per_epoch, label=\"Training Loss\")\n",
    "    axs[0].plot(valid_loss_per_epoch, label=\"Valid Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(f\"Loss on {model_name}\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # 标记学习率变化\n",
    "    # iterations_per_epoch = len(loss_list) // num_epochs\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[0].axvline(\n",
    "    #             x=epoch * iterations_per_epoch, linestyle=\"--\", color=\"gray\", alpha=0.8\n",
    "    #         )\n",
    "    #         axs[0].text(\n",
    "    #             epoch * iterations_per_epoch + 100,\n",
    "    #             max(loss_list),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    # plot training accuracy\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(train_acc) + 1),\n",
    "        train_acc,\n",
    "        label=\"Training Accuracy\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(valid_acc) + 1),\n",
    "        valid_acc,\n",
    "        label=\"Valid Accuracy\",\n",
    "    )\n",
    "    axs[1].xlim = (0, num_epochs + 1)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"Accuracy on {model_name}\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[1].axvline(x=epoch, linestyle=\"--\", color=\"gray\", alpha=0.8)\n",
    "    #         axs[1].text(\n",
    "    #             epoch + 0.2,\n",
    "    #             max(train_acc),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    fig.savefig(f\"{model_name}_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_list, label=\"Minibatch Loss\")\n",
    "    plt.plot(running_avg_loss, label=\"Running Average Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss on {model_name}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.savefig(f\"{model_name}_training_loss.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_training_metrics(log_dict, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_loader, model_name):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "#     test_loss = test_eval_res[\"avg_loss\"]\n",
    "#     test_acc = test_eval_res[\"accuracy\"]\n",
    "#     class_correct = test_eval_res[\"class_correct\"]\n",
    "#     class_total = test_eval_res[\"class_total\"]\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "#     for i in range(num_classes):\n",
    "#         print(\n",
    "#             \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "#             % (\n",
    "#                 classes[i],\n",
    "#                 100 * class_correct[i] / class_total[i],\n",
    "#                 np.sum(class_correct[i]),\n",
    "#                 np.sum(class_total[i]),\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, model_name):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "    test_loss = test_eval_res[\"avg_loss\"]\n",
    "    test_acc = test_eval_res[\"accuracy\"]\n",
    "    class_correct = test_eval_res[\"class_correct\"]\n",
    "    class_total = test_eval_res[\"class_total\"]\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"Model: {model_name}\")\n",
    "    output.append(f\"Test Loss: {test_loss:.4f}\")\n",
    "    output.append(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "    for i in range(num_classes):\n",
    "        output.append(\n",
    "            \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "            % (\n",
    "                classes[i],\n",
    "                100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]),\n",
    "                np.sum(class_total[i]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    for line in output:\n",
    "        print(line)\n",
    "\n",
    "    # 将结果写入文本文件\n",
    "    with open(f\"{model_name}_test_results.txt\", \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(model, data_loader, classes, model_name):\n",
    "    # step1: get 10 sample images from the data loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images[:10], labels[:10]\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # step2: get model predictions and calculate accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    correct_count = (predicted == labels).sum().item()\n",
    "    accuracy = correct_count / len(labels) * 100\n",
    "\n",
    "    # step3: plot the images with the predicted labels\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(\n",
    "        f\"10 CIFAR-10 Images on Test Dataset using {model_name}\\nAccuracy: {accuracy:.2f}%\",\n",
    "        fontsize=16,\n",
    "        fontweight=600,\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        color = \"blue\" if predicted[i] == labels[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}\",\n",
    "            fontsize=12,\n",
    "            color=color,\n",
    "            y=-0.25,\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{model_name}_cifar10_predictions.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resnet_compare(log_dicts):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    color_list = [\"#6495ED\", \"#4EEE94\", \"#EEC900\", \"#FF6347\", \"#BA55D3\", \"#00808C\"]\n",
    "    ind_color = 0\n",
    "    for log_dict in log_dicts:\n",
    "        loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "        train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "        valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "        learning_rates = log_dict[\"learning_rates\"]\n",
    "        model_name = log_dict[\"model_name\"]\n",
    "        train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "        valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "        # axs[0].scatter(\n",
    "        #     np.arange(1, len(valid_loss_per_epoch) + 1),\n",
    "        #     valid_loss_per_epoch,\n",
    "        #     label=f\"{model_name}\",\n",
    "        #     s=20,\n",
    "        #     color=next(color),\n",
    "        # )\n",
    "        axs[0].plot(\n",
    "            np.arange(1, len(valid_loss_per_epoch) + 1),\n",
    "            valid_loss_per_epoch,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "        # axs[1].scatter(\n",
    "        #     np.arange(1, len(valid_acc) + 1),\n",
    "        #     valid_acc,\n",
    "        #     label=f\"{model_name}\",\n",
    "        #     s=20,\n",
    "        #     color=next(color),\n",
    "        # )\n",
    "        axs[1].plot(\n",
    "            np.arange(1, len(valid_acc) + 1),\n",
    "            valid_acc,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "        ind_color += 1\n",
    "\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Validation Loss\")\n",
    "    axs[0].set_title(f\"CIFAR10 Validation Loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"CIFAR10 Validation Accuracy\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[1].grid(True)\n",
    "    fig.savefig(\"Resnet_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"Resnet50WithoutShortcut\": (\n",
    "            Resnet50WithoutShortcut(num_classes=10).to(device),\n",
    "            100,\n",
    "            0.1,\n",
    "        ),\n",
    "        \"ResNet50\": (ResNet50(num_classes=10).to(device), 100, 0.1),\n",
    "        \"Resnet18WithoutShortcut\": (\n",
    "            Resnet18WithoutShortcut(num_classes=10).to(device),\n",
    "            100,\n",
    "            0.1,\n",
    "        ),\n",
    "        \"ResNet18\": (ResNet18(num_classes=10).to(device), 100, 0.1),\n",
    "        \"ResNet34\": (ResNet34(num_classes=10).to(device), 100, 0.1),\n",
    "        \"Resnet34WithoutShortcut\": (\n",
    "            Resnet34WithoutShortcut(num_classes=10).to(device),\n",
    "            100,\n",
    "            0.1,\n",
    "        ),\n",
    "        # \"ResNet101\": (ResNet101(num_classes=10).to(device), 130, 0.1),\n",
    "        # \"ResNet152\": (ResNet152(num_classes=10).to(device), 130, 0.01),\n",
    "    }\n",
    "\n",
    "    log_dicts = []\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "        # optimizer = optim.SGD(\n",
    "        #     model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        # )\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=6, verbose=True\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        log_dict = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            scheduler=scheduler,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        log_dicts.append(log_dict)\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_cifar.pt\"))\n",
    "        plot_training_metrics(log_dict, num_epochs)\n",
    "        test(model, test_loader, model_name)\n",
    "        # plot_images_with_predictions(model, test_loader, classes, model_name)\n",
    "\n",
    "    return log_dicts\n",
    "\n",
    "\n",
    "# 开始训练所有模型\n",
    "log_dicts = train_all_resnet_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_resnet_compare(log_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
