{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# 在每次训练前设置随机种子\n",
    "seed = 9  # 可以每次更改这个值\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "num_classes = 10\n",
    "\n",
    "# the percentage of the training dataset to use as validation dataset\n",
    "valid_percentage = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)\n",
    "\n",
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*JEGNYy9rXMj_XN7W1Qjo9g.png)\n",
    "![](https://img-blog.csdnimg.cn/20200104153325358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104162456690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "   ReLU\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    # 二维卷积层\n",
    "    # in_channels：输入特征图的通道数。例如，对于RGB图像，in_channels为3。\n",
    "    # out_channels：输出特征图的通道数。这个值决定了卷积核的数量，即我们希望提取多少个特征。\n",
    "    # kernel_size：卷积核的大小，可以是一个整数或一个元组。例如，kernel_size=3表示使用3x3的卷积核。\n",
    "    # stride：卷积核的步幅，决定卷积核在输入特征图上移动的步长。默认值为1。\n",
    "    # padding：填充方式，为了保持特征图的尺寸，可以在输入特征图的边缘填充0。padding=1表示在所有边缘填充1个像素。\n",
    "    # bias：是否添加偏置项。默认值为True。这里不使用偏置项（bias=False），因为后面有批量归一化层。\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer.\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer.\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = (\n",
    "        1  # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    )\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Initializes the basic block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1），我们需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Define an input tensor with shape (batch_size, in_channels, height, width)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "# Create a basic block instance\n",
    "block = BasicBlock(64, 64)\n",
    "# Forward pass\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   1x1 Conv (Reduction)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   3x3 Conv\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   1x1 Conv (Expansion)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    A bottleneck residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer (1x1).\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer (3x3).\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        conv3: Third convolutional layer (1x1).\n",
    "        bn3: Batch normalization for the third convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet model.\n",
    "\n",
    "    Attributes:\n",
    "        in_channels: Number of input channels.\n",
    "        conv1: Initial convolutional layer.\n",
    "        bn1: Batch normalization for the initial convolutional layer.\n",
    "        maxpool: Max pooling layer.\n",
    "        layer1: First layer of residual blocks.\n",
    "        layer2: Second layer of residual blocks.\n",
    "        layer3: Third layer of residual blocks.\n",
    "        layer4: Fourth layer of residual blocks.\n",
    "        avgpool: Global average pooling layer.\n",
    "        fc: Fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, block: type[BasicBlock], num_blocks: list[int], num_classes: int = 1000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ResNet model.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            num_blocks: A list containing the number of blocks in each layer.\n",
    "            num_classes: Number of output classes. Default is 1000.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # 考虑到CIFAR10数据集的图片尺寸太小，ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息\n",
    "        # 所以在实验中我们将7x7的降采样层和最大池化层去掉，替换为一个3x3的降采样卷积，同时减小该卷积层的步长和填充大小，\n",
    "        # 这样可以尽可能保留原始图像的信息。\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.fc = nn.Sequential(\n",
    "            # nn.Dropout(0.5), nn.Linear(512 * block.expansion, num_classes)\n",
    "        # )\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, block: type[BasicBlock], out_channels: int, num_blocks: int, stride: int\n",
    "    ) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a layer of residual blocks.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            out_channels: Number of output channels.\n",
    "            num_blocks: Number of blocks in the layer.\n",
    "            stride: Stride for the first block.\n",
    "\n",
    "        Returns:\n",
    "            A sequential container of residual blocks.\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-18 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet18(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-34 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet34(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-50 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet50(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet101(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-101 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet101(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-152 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "\n",
    "# summary(ResNet152(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cutout import Cutout\n",
    "\n",
    "# 数据增强变换\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # 随机裁剪，填充4个像素\n",
    "        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # 颜色抖动\n",
    "        transforms.RandomRotation(15),  # 随机旋转\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 验证集和测试集变换（不进行数据增强）\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 将数据转换为torch.FloatTensor，并标准化。\n",
    "train_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_train)\n",
    "valid_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_test)\n",
    "test_data = CIFAR10(\"../data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "# random indices\n",
    "np.random.shuffle(indices)\n",
    "# the ratio of split\n",
    "split = int(np.floor(valid_percentage * num_train))\n",
    "# divide data to radin_data and valid_data\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "# 无放回地按照给定的索引列表采样样本元素\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data, batch_size=batch_size, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset size\n",
    "def check_dataset(loader, set_name):\n",
    "    print(f\"{set_name} Set:\")\n",
    "    images, labels = next(iter(loader))\n",
    "    print(\"batch count\", len(loader))\n",
    "    print(\"image size per batch\", images.size())\n",
    "    print(\"label size per batch\", labels.size())\n",
    "\n",
    "\n",
    "check_dataset(train_loader, \"Training\")\n",
    "check_dataset(valid_loader, \"Valid\")\n",
    "check_dataset(test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    class_total = [0.0 for _ in range(num_classes)]\n",
    "    class_correct = [0.0 for _ in range(num_classes)]\n",
    "    sum_loss, num_correct, num_examples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the model output\n",
    "            outputs = model(features)\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute the correct radix\n",
    "            num_examples += targets.size(0)\n",
    "            num_correct += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # compute each class 's correct count\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_correct[label] += (predicted_labels[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = num_correct / num_examples * 100\n",
    "    avg_loss = sum_loss / num_examples\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"class_correct\": class_correct,\n",
    "        \"class_total\": class_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    num_epochs: int = 25,\n",
    "    model_name: str = \"model\",\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    scheduler=None,\n",
    ") -> dict:\n",
    "    log_dict = {\n",
    "        \"train_loss_per_batch\": [],\n",
    "        \"train_acc_per_epoch\": [],\n",
    "        \"valid_acc_per_epoch\": [],\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"valid_loss_per_epoch\": [],\n",
    "        \"valid_loss_min\": np.Inf,\n",
    "        \"learning_rates\": [],\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"learning_rates\"].append(current_lr)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader, 0):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # step1: predict the output\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # step2: update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            log_dict[\"train_loss_per_batch\"].append(loss.item())\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # each epoch, evaluate the model\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            train_eval_res = eval_model(model, train_loader)\n",
    "            train_acc = train_eval_res[\"accuracy\"]\n",
    "            train_loss = train_eval_res[\"avg_loss\"]\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Train. Acc.: {train_acc:.3f}% | Loss: {train_loss:.4f}\"\n",
    "            )\n",
    "            log_dict[\"train_loss_per_epoch\"].append(train_loss)\n",
    "            log_dict[\"train_acc_per_epoch\"].append(train_acc)\n",
    "\n",
    "            # * each epoch, evaluate the model on the validation dataset which is not used for training\n",
    "            valid_eval_res = eval_model(model, valid_loader)\n",
    "            valid_acc = valid_eval_res[\"accuracy\"]\n",
    "            valid_loss = valid_eval_res[\"avg_loss\"]\n",
    "            log_dict[\"valid_loss_per_epoch\"].append(valid_loss)\n",
    "            log_dict[\"valid_acc_per_epoch\"].append(valid_acc)\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Valid. Acc.: {valid_acc:.3f}% | Loss: {valid_loss:.4f}\"\n",
    "            )\n",
    "            # * save the model if the validation loss is decreased\n",
    "            if valid_loss <= log_dict[\"valid_loss_min\"]:\n",
    "                print(\n",
    "                    f\"**Validation loss decreased ({log_dict['valid_loss_min']:.6f} --> {valid_loss:.6f}). Saving model ...\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), f\"{model_name}_cifar.pt\")\n",
    "                log_dict[\"valid_loss_min\"] = valid_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # scheduler.step()\n",
    "            scheduler.step(valid_loss)\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "# model = ResNet18(num_classes=10).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# log_dict = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     num_epochs=20,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     scheduler=scheduler,\n",
    "#     model_name=\"ResNet34\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(log_dict: dict, num_epochs: int):\n",
    "    loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "    train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "    valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "    learning_rates = log_dict[\"learning_rates\"]\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "    train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "    valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "\n",
    "    running_avg_loss = np.convolve(loss_list, np.ones(200) / 200, mode=\"valid\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].plot(train_loss_per_epoch, label=\"Training Loss\")\n",
    "    axs[0].plot(valid_loss_per_epoch, label=\"Valid Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(f\"Training and Validation Loss on {model_name}\")\n",
    "    axs[0].legend(loc='best')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # 标记学习率变化\n",
    "    # iterations_per_epoch = len(loss_list) // num_epochs\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[0].axvline(\n",
    "    #             x=epoch * iterations_per_epoch, linestyle=\"--\", color=\"gray\", alpha=0.8\n",
    "    #         )\n",
    "    #         axs[0].text(\n",
    "    #             epoch * iterations_per_epoch + 100,\n",
    "    #             max(loss_list),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    # plot training accuracy\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(train_acc) + 1),\n",
    "        train_acc,\n",
    "        label=\"Training Accuracy\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(valid_acc) + 1),\n",
    "        valid_acc,\n",
    "        label=\"Valid Accuracy\",\n",
    "    )\n",
    "    axs[1].xlim = (0, num_epochs + 1)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"Accuracy on {model_name}\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[1].axvline(x=epoch, linestyle=\"--\", color=\"gray\", alpha=0.8)\n",
    "    #         axs[1].text(\n",
    "    #             epoch + 0.2,\n",
    "    #             max(train_acc),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    fig.savefig(f\"{model_name}_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_list, label=\"Minibatch Loss\")\n",
    "    plt.plot(running_avg_loss, label=\"Running Average Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cross Entropy Loss\")\n",
    "    plt.title(f\"Training Loss on {model_name}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.savefig(f\"{model_name}_training_loss.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_training_metrics(log_dict, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_loader, model_name):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "#     test_loss = test_eval_res[\"avg_loss\"]\n",
    "#     test_acc = test_eval_res[\"accuracy\"]\n",
    "#     class_correct = test_eval_res[\"class_correct\"]\n",
    "#     class_total = test_eval_res[\"class_total\"]\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "#     for i in range(num_classes):\n",
    "#         print(\n",
    "#             \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "#             % (\n",
    "#                 classes[i],\n",
    "#                 100 * class_correct[i] / class_total[i],\n",
    "#                 np.sum(class_correct[i]),\n",
    "#                 np.sum(class_total[i]),\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, model_name):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "    test_loss = test_eval_res[\"avg_loss\"]\n",
    "    test_acc = test_eval_res[\"accuracy\"]\n",
    "    class_correct = test_eval_res[\"class_correct\"]\n",
    "    class_total = test_eval_res[\"class_total\"]\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"Model: {model_name}\")\n",
    "    output.append(f\"Test Loss: {test_loss:.4f}\")\n",
    "    output.append(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "    for i in range(num_classes):\n",
    "        output.append(\n",
    "            \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "            % (\n",
    "                classes[i],\n",
    "                100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]),\n",
    "                np.sum(class_total[i]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    for line in output:\n",
    "        print(line)\n",
    "\n",
    "    # 将结果写入文本文件\n",
    "    with open(f\"{model_name}_test_results.txt\", \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(model, data_loader, classes, model_name):\n",
    "    # step1: get 10 sample images from the data loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images[:10], labels[:10]\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # step2: get model predictions and calculate accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    correct_count = (predicted == labels).sum().item()\n",
    "    accuracy = correct_count / len(labels) * 100\n",
    "\n",
    "    # step3: plot the images with the predicted labels\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(\n",
    "        f\"10 CIFAR-10 Images on Test Dataset using {model_name}\\nAccuracy: {accuracy:.2f}%\",\n",
    "        fontsize=16,\n",
    "        fontweight=600,\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        color = \"blue\" if predicted[i] == labels[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}\",\n",
    "            fontsize=12,\n",
    "            color=color,\n",
    "            y=-0.25,\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{model_name}_cifar10_predictions.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resnet_compare(log_dicts):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    for log_dict in log_dicts:\n",
    "        loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "        train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "        valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "        learning_rates = log_dict[\"learning_rates\"]\n",
    "        model_name = log_dict[\"model_name\"]\n",
    "        train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "        valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "        axs[0].scatter(\n",
    "            np.arange(1, len(valid_loss_per_epoch) + 1),\n",
    "            valid_loss_per_epoch,\n",
    "            label=f\"{model_name} Valid Loss\",\n",
    "        )\n",
    "        axs[1].scatter(\n",
    "            np.arange(1, len(valid_acc) + 1),\n",
    "            valid_acc,\n",
    "            label=f\"{model_name} Valid Accuracy\",\n",
    "        )\n",
    "\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Validation Loss\")\n",
    "    axs[0].set_title(f\"CIFAR10 Validation Loss on ResNet Models\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Validation Accuracy (%)\")\n",
    "    axs[1].set_title(f\"CIFAR10 Accuracy on ResNet Models\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].grid(True)\n",
    "    fig.savefig(\"Resnet_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"ResNet18\": (ResNet18(num_classes=10).to(device), 100, 0.1),\n",
    "        \"ResNet34\": (ResNet34(num_classes=10).to(device), 100, 0.1),\n",
    "        \"ResNet50\": (ResNet50(num_classes=10).to(device), 100, 0.01),\n",
    "        \"ResNet101\": (ResNet101(num_classes=10).to(device), 100, 0.01),\n",
    "        # \"ResNet152\": (ResNet152(num_classes=10).to(device), 100, 0.01),\n",
    "    }\n",
    "    \n",
    "    log_dicts = []\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "        # optimizer = optim.SGD(\n",
    "        #     model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        # )\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=6, verbose=True\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        log_dict = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            scheduler=scheduler,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        log_dicts.append(log_dict)\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_cifar.pt\"))\n",
    "        plot_training_metrics(log_dict, num_epochs)\n",
    "        test(model, test_loader, model_name)\n",
    "        # plot_images_with_predictions(model, test_loader, classes, model_name)\n",
    "\n",
    "    return log_dicts\n",
    "\n",
    "\n",
    "# 开始训练所有模型\n",
    "log_dicts = train_all_resnet_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_resnet_compare(log_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
