{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "num_classes = 10\n",
    "\n",
    "valid_percentage = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)\n",
    "\n",
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*JEGNYy9rXMj_XN7W1Qjo9g.png)\n",
    "![](https://img-blog.csdnimg.cn/20200104153325358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104162456690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "   ReLU\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = (\n",
    "        1  # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    )\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 批量规范化层,对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1),需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "block = BasicBlock(64, 64)\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class BasicBlockWithOutShortcut(nn.Module):\n",
    "    expansion: int = (\n",
    "        1  # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    )\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlockWithOutShortcut, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 批量规范化层,对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1),需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "block = BasicBlock(64, 64)\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   1x1 Conv (Reduction)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   3x3 Conv\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   1x1 Conv (Expansion)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=1, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # 考虑到CIFAR10数据集的图片尺寸太小，ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息\n",
    "        # 所以在实验中我们将7x7的降采样层和最大池化层去掉，替换为一个3x3的降采样卷积，同时减小该卷积层的步长和填充大小，\n",
    "        # 这样可以尽可能保留原始图像的信息。\n",
    "        self.conv1 = conv3x3(3, 64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, block, out_channels, num_blocks, stride\n",
    "    ):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        [1, 10]                   --\n",
       "├─Conv2d: 1-1                                 [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                            [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                             [1, 64, 32, 32]           --\n",
       "│    └─BasicBlockWithOutShortcut: 2-1         [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                       [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4                  [1, 64, 32, 32]           128\n",
       "│    └─BasicBlockWithOutShortcut: 2-2         [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-5                       [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-6                  [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-7                       [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8                  [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-4                             [1, 128, 16, 16]          --\n",
       "│    └─BasicBlockWithOutShortcut: 2-3         [1, 128, 16, 16]          8,448\n",
       "│    │    └─Conv2d: 3-9                       [1, 128, 16, 16]          73,728\n",
       "│    │    └─BatchNorm2d: 3-10                 [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-11                      [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-12                 [1, 128, 16, 16]          256\n",
       "│    └─BasicBlockWithOutShortcut: 2-4         [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-13                      [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-14                 [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-15                      [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-16                 [1, 128, 16, 16]          256\n",
       "├─Sequential: 1-5                             [1, 256, 8, 8]            --\n",
       "│    └─BasicBlockWithOutShortcut: 2-5         [1, 256, 8, 8]            33,280\n",
       "│    │    └─Conv2d: 3-17                      [1, 256, 8, 8]            294,912\n",
       "│    │    └─BatchNorm2d: 3-18                 [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-19                      [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-20                 [1, 256, 8, 8]            512\n",
       "│    └─BasicBlockWithOutShortcut: 2-6         [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-21                      [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-22                 [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-23                      [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-24                 [1, 256, 8, 8]            512\n",
       "├─Sequential: 1-6                             [1, 512, 4, 4]            --\n",
       "│    └─BasicBlockWithOutShortcut: 2-7         [1, 512, 4, 4]            132,096\n",
       "│    │    └─Conv2d: 3-25                      [1, 512, 4, 4]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-26                 [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-27                      [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-28                 [1, 512, 4, 4]            1,024\n",
       "│    └─BasicBlockWithOutShortcut: 2-8         [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-29                      [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-30                 [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-31                      [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-32                 [1, 512, 4, 4]            1,024\n",
       "├─AdaptiveAvgPool2d: 1-7                      [1, 512, 1, 1]            --\n",
       "├─Linear: 1-8                                 [1, 10]                   5,130\n",
       "===============================================================================================\n",
       "Total params: 11,173,962\n",
       "Trainable params: 11,173,962\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 549.14\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 8.91\n",
       "Params size (MB): 44.00\n",
       "Estimated Total Size (MB): 52.93\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Resnet18WithoutShortcut(num_classes):\n",
    "    return ResNet(BasicBlockWithOutShortcut, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "model = Resnet18WithoutShortcut(10)\n",
    "\n",
    "summary(Resnet18WithoutShortcut(num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                        [1, 64, 32, 32]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-5              [1, 64, 32, 32]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-6                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-10             [1, 64, 32, 32]           --\n",
       "├─Sequential: 1-4                        [1, 128, 16, 16]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-11                 [1, 128, 16, 16]          73,728\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-15             [1, 128, 16, 16]          8,448\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-18                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-19            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-20             [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-5                        [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-21                 [1, 256, 8, 8]            294,912\n",
       "│    │    └─BatchNorm2d: 3-22            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-23                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-25             [1, 256, 8, 8]            33,280\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-28                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-30             [1, 256, 8, 8]            --\n",
       "├─Sequential: 1-6                        [1, 512, 4, 4]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-31                 [1, 512, 4, 4]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-33                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Sequential: 3-35             [1, 512, 4, 4]            132,096\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-36                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-38                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Sequential: 3-40             [1, 512, 4, 4]            --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-8                            [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 11,173,962\n",
       "Trainable params: 11,173,962\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 555.43\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 9.83\n",
       "Params size (MB): 44.70\n",
       "Estimated Total Size (MB): 54.54\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ResNet18(num_classes):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet18(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                        [1, 64, 32, 32]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-5              [1, 64, 32, 32]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-6                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-10             [1, 64, 32, 32]           --\n",
       "│    └─BasicBlock: 2-3                   [1, 64, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-11                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-15             [1, 64, 32, 32]           --\n",
       "├─Sequential: 1-4                        [1, 128, 16, 16]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 16, 16]          73,728\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-18                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-19            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-20             [1, 128, 16, 16]          8,448\n",
       "│    └─BasicBlock: 2-5                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-21                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-22            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-25             [1, 128, 16, 16]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-28                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-30             [1, 128, 16, 16]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 128, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-31                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-33                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-35             [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-5                        [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 8, 8]            294,912\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-38                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-40             [1, 256, 8, 8]            33,280\n",
       "│    └─BasicBlock: 2-9                   [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-41                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-42            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-43                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-45             [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-10                  [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-48                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-49            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-50             [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-11                  [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-51                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-53                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-54            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-55             [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-12                  [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-56                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-57            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-58                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-59            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-60             [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-13                  [1, 256, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-61                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-63                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 256, 8, 8]            512\n",
       "│    │    └─Sequential: 3-65             [1, 256, 8, 8]            --\n",
       "├─Sequential: 1-6                        [1, 512, 4, 4]            --\n",
       "│    └─BasicBlock: 2-14                  [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-66                 [1, 512, 4, 4]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-68                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-69            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Sequential: 3-70             [1, 512, 4, 4]            132,096\n",
       "│    └─BasicBlock: 2-15                  [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-71                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-72            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-73                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-74            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Sequential: 3-75             [1, 512, 4, 4]            --\n",
       "│    └─BasicBlock: 2-16                  [1, 512, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-76                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-78                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Sequential: 3-80             [1, 512, 4, 4]            --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-8                            [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 21,282,122\n",
       "Trainable params: 21,282,122\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.16\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 16.38\n",
       "Params size (MB): 85.13\n",
       "Estimated Total Size (MB): 101.52\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ResNet34(num_classes):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet34(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                        [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 32, 32]           4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-5                  [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-7              [1, 256, 32, 32]          16,896\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-12                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-13            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-14             [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-15                 [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-16            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-17                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-19                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-21             [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-4                        [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 32, 32]          32,768\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 32, 32]          256\n",
       "│    │    └─Conv2d: 3-24                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-25            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-26                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-28             [1, 512, 16, 16]          132,096\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-31                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-33                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-35             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-38                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-40                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-42             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-43                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-45                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-47                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-48            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-49             [1, 512, 16, 16]          --\n",
       "├─Sequential: 1-5                        [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-50                 [1, 256, 16, 16]          131,072\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 256, 16, 16]          512\n",
       "│    │    └─Conv2d: 3-52                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-54                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-56             [1, 1024, 8, 8]           526,336\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-57                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-59                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-60            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-61                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-63             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-64                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-65            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-68                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-69            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-70             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-71                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-72            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-73                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-74            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-75                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-76            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-77             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-78                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-80                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-81            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-84             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-87                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-88            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-89                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-90            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-91             [1, 1024, 8, 8]           --\n",
       "├─Sequential: 1-6                        [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-92                 [1, 512, 8, 8]            524,288\n",
       "│    │    └─BatchNorm2d: 3-93            [1, 512, 8, 8]            1,024\n",
       "│    │    └─Conv2d: 3-94                 [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-96                 [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-97            [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-98             [1, 2048, 4, 4]           2,101,248\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-99                 [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-100           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-101                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-102           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-103                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-105            [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-106                [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-108                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-109           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-110                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-111           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-112            [1, 2048, 4, 4]           --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-8                            [1, 10]                   20,490\n",
       "==========================================================================================\n",
       "Total params: 23,520,842\n",
       "Trainable params: 23,520,842\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.30\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 54.92\n",
       "Params size (MB): 94.08\n",
       "Estimated Total Size (MB): 149.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ResNet50(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet50(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                        [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 32, 32]           4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-5                  [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-7              [1, 256, 32, 32]          16,896\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-12                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-13            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-14             [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-15                 [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-16            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-17                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-19                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-21             [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-4                        [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 32, 32]          32,768\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 32, 32]          256\n",
       "│    │    └─Conv2d: 3-24                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-25            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-26                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-28             [1, 512, 16, 16]          132,096\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-31                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-33                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-35             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-38                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-40                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-42             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-43                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-45                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-47                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-48            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-49             [1, 512, 16, 16]          --\n",
       "├─Sequential: 1-5                        [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-50                 [1, 256, 16, 16]          131,072\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 256, 16, 16]          512\n",
       "│    │    └─Conv2d: 3-52                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-54                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-56             [1, 1024, 8, 8]           526,336\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-57                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-59                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-60            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-61                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-63             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-64                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-65            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-68                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-69            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-70             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-71                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-72            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-73                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-74            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-75                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-76            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-77             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-78                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-80                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-81            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-84             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-87                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-88            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-89                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-90            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-91             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-14                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-92                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-93            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-94                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-96                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-97            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-98             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-15                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-99                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-100           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-101                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-102           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-103                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-105            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-16                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-108                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-109           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-110                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-111           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-112            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-17                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-113                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-114           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-117                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-118           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-119            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-18                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-120                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-121           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-122                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-123           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-124                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-126            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-19                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-127                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-129                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-130           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-131                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-133            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-20                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-134                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-136                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-137           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-138                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-139           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-140            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-21                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-141                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-142           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-143                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-145                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-146           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-147            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-22                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-148                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-149           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-150                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-151           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-152                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-153           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-154            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-23                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-155                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-156           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-157                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-158           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-159                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-160           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-161            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-24                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-162                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-163           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-164                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-165           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-166                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-167           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-168            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-25                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-169                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-170           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-171                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-172           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-173                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-174           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-175            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-26                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-176                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-177           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-178                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-179           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-180                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-181           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-182            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-27                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-183                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-184           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-185                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-186           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-187                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-188           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-189            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-28                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-190                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-191           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-192                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-193           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-194                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-195           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-196            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-29                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-197                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-198           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-199                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-200           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-201                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-202           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-203            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-30                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-204                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-205           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-206                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-207           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-208                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-209           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-210            [1, 1024, 8, 8]           --\n",
       "├─Sequential: 1-6                        [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-31                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-211                [1, 512, 8, 8]            524,288\n",
       "│    │    └─BatchNorm2d: 3-212           [1, 512, 8, 8]            1,024\n",
       "│    │    └─Conv2d: 3-213                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-214           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-215                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-216           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-217            [1, 2048, 4, 4]           2,101,248\n",
       "│    └─Bottleneck: 2-32                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-218                [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-219           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-220                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-221           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-222                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-223           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-224            [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-33                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-225                [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-226           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-227                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-228           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-229                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-230           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-231            [1, 2048, 4, 4]           --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-8                            [1, 10]                   20,490\n",
       "==========================================================================================\n",
       "Total params: 42,512,970\n",
       "Trainable params: 42,512,970\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.51\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 81.66\n",
       "Params size (MB): 170.05\n",
       "Estimated Total Size (MB): 251.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ResNet101(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet101(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
       "├─Sequential: 1-3                        [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 32, 32]           4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-5                  [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-7              [1, 256, 32, 32]          16,896\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-12                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-13            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-14             [1, 256, 32, 32]          --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-15                 [1, 64, 32, 32]           16,384\n",
       "│    │    └─BatchNorm2d: 3-16            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-17                 [1, 64, 32, 32]           36,864\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 64, 32, 32]           128\n",
       "│    │    └─Conv2d: 3-19                 [1, 256, 32, 32]          16,384\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-21             [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-4                        [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 32, 32]          32,768\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 32, 32]          256\n",
       "│    │    └─Conv2d: 3-24                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-25            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-26                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-28             [1, 512, 16, 16]          132,096\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-31                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-33                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-35             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-38                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-40                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-42             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-43                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-45                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-47                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-48            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-49             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-8                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-50                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-52                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-54                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-56             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-9                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-59                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-60            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-61                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-63             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-10                  [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-64                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-65            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-66                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-68                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-69            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-70             [1, 512, 16, 16]          --\n",
       "│    └─Bottleneck: 2-11                  [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-71                 [1, 128, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-72            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-73                 [1, 128, 16, 16]          147,456\n",
       "│    │    └─BatchNorm2d: 3-74            [1, 128, 16, 16]          256\n",
       "│    │    └─Conv2d: 3-75                 [1, 512, 16, 16]          65,536\n",
       "│    │    └─BatchNorm2d: 3-76            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-77             [1, 512, 16, 16]          --\n",
       "├─Sequential: 1-5                        [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-78                 [1, 256, 16, 16]          131,072\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 256, 16, 16]          512\n",
       "│    │    └─Conv2d: 3-80                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-81            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-84             [1, 1024, 8, 8]           526,336\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-87                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-88            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-89                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-90            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-91             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-14                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-92                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-93            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-94                 [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-96                 [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-97            [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-98             [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-15                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-99                 [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-100           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-101                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-102           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-103                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-105            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-16                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-108                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-109           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-110                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-111           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-112            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-17                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-113                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-114           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-117                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-118           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-119            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-18                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-120                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-121           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-122                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-123           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-124                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-126            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-19                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-127                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-129                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-130           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-131                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-133            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-20                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-134                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-136                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-137           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-138                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-139           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-140            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-21                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-141                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-142           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-143                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-145                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-146           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-147            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-22                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-148                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-149           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-150                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-151           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-152                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-153           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-154            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-23                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-155                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-156           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-157                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-158           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-159                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-160           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-161            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-24                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-162                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-163           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-164                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-165           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-166                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-167           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-168            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-25                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-169                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-170           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-171                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-172           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-173                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-174           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-175            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-26                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-176                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-177           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-178                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-179           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-180                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-181           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-182            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-27                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-183                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-184           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-185                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-186           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-187                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-188           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-189            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-28                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-190                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-191           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-192                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-193           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-194                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-195           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-196            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-29                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-197                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-198           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-199                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-200           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-201                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-202           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-203            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-30                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-204                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-205           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-206                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-207           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-208                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-209           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-210            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-31                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-211                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-212           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-213                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-214           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-215                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-216           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-217            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-32                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-218                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-219           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-220                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-221           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-222                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-223           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-224            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-33                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-225                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-226           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-227                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-228           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-229                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-230           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-231            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-34                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-232                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-233           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-234                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-235           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-236                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-237           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-238            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-35                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-239                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-240           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-241                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-242           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-243                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-244           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-245            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-36                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-246                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-247           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-248                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-249           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-250                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-251           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-252            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-37                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-253                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-254           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-255                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-256           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-257                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-258           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-259            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-38                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-260                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-261           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-262                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-263           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-264                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-265           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-266            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-39                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-267                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-268           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-269                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-270           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-271                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-272           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-273            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-40                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-274                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-275           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-276                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-277           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-278                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-279           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-280            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-41                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-281                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-282           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-283                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-284           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-285                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-286           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-287            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-42                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-288                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-289           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-290                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-291           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-292                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-293           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-294            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-43                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-295                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-296           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-297                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-298           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-299                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-300           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-301            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-44                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-302                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-303           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-304                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-305           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-306                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-307           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-308            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-45                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-309                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-310           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-311                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-312           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-313                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-314           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-315            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-46                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-316                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-317           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-318                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-319           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-320                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-321           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-322            [1, 1024, 8, 8]           --\n",
       "│    └─Bottleneck: 2-47                  [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-323                [1, 256, 8, 8]            262,144\n",
       "│    │    └─BatchNorm2d: 3-324           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-325                [1, 256, 8, 8]            589,824\n",
       "│    │    └─BatchNorm2d: 3-326           [1, 256, 8, 8]            512\n",
       "│    │    └─Conv2d: 3-327                [1, 1024, 8, 8]           262,144\n",
       "│    │    └─BatchNorm2d: 3-328           [1, 1024, 8, 8]           2,048\n",
       "│    │    └─Sequential: 3-329            [1, 1024, 8, 8]           --\n",
       "├─Sequential: 1-6                        [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-48                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-330                [1, 512, 8, 8]            524,288\n",
       "│    │    └─BatchNorm2d: 3-331           [1, 512, 8, 8]            1,024\n",
       "│    │    └─Conv2d: 3-332                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-333           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-334                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-335           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-336            [1, 2048, 4, 4]           2,101,248\n",
       "│    └─Bottleneck: 2-49                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-337                [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-338           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-339                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-340           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-341                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-342           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-343            [1, 2048, 4, 4]           --\n",
       "│    └─Bottleneck: 2-50                  [1, 2048, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-344                [1, 512, 4, 4]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-345           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-346                [1, 512, 4, 4]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-347           [1, 512, 4, 4]            1,024\n",
       "│    │    └─Conv2d: 3-348                [1, 2048, 4, 4]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-349           [1, 2048, 4, 4]           4,096\n",
       "│    │    └─Sequential: 3-350            [1, 2048, 4, 4]           --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-8                            [1, 10]                   20,490\n",
       "==========================================================================================\n",
       "Total params: 58,156,618\n",
       "Trainable params: 58,156,618\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.72\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 114.69\n",
       "Params size (MB): 232.63\n",
       "Estimated Total Size (MB): 347.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ResNet152(num_classes):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet152(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 数据增强变换，用于训练集\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # 随机裁剪，填充4个像素\n",
    "        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # 颜色抖动\n",
    "        transforms.RandomRotation(15),  # 随机旋转\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 用于验证集和测试集的变换\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_train)\n",
    "valid_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_test)\n",
    "test_data = CIFAR10(\"../data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_percentage * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# 无放回地按照给定的索引列表采样样本元素\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "batch count 313\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n",
      "Valid Set:\n",
      "batch count 79\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n",
      "Testing Set:\n",
      "batch count 79\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "def check_dataset(loader, set_name):\n",
    "    print(f\"{set_name} Set:\")\n",
    "    images, labels = next(iter(loader))\n",
    "    print(\"batch count\", len(loader))\n",
    "    print(\"image size per batch\", images.size())\n",
    "    print(\"label size per batch\", labels.size())\n",
    "\n",
    "\n",
    "check_dataset(train_loader, \"Training\")\n",
    "check_dataset(valid_loader, \"Valid\")\n",
    "check_dataset(test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    class_total = [0.0 for _ in range(num_classes)]\n",
    "    class_correct = [0.0 for _ in range(num_classes)]\n",
    "    sum_loss, num_correct, num_examples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the model output\n",
    "            outputs = model(features)\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute the correct radix\n",
    "            num_examples += targets.size(0)\n",
    "            num_correct += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # compute each class 's correct count\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_correct[label] += (predicted_labels[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = num_correct / num_examples * 100\n",
    "    avg_loss = sum_loss / num_examples\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"class_correct\": class_correct,\n",
    "        \"class_total\": class_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    num_epochs: int = 25,\n",
    "    model_name: str = \"model\",\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    scheduler=None,\n",
    ") -> dict:\n",
    "    log_dict = {\n",
    "        \"train_loss_per_batch\": [],\n",
    "        \"train_acc_per_epoch\": [],\n",
    "        \"valid_acc_per_epoch\": [],\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"valid_loss_per_epoch\": [],\n",
    "        \"valid_loss_min\": np.Inf,\n",
    "        \"learning_rates\": [],\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"learning_rates\"].append(current_lr)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        ###################\n",
    "        # 训练 #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader, 0):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # step1: predict the output\n",
    "            outputs = model(features)\n",
    "\n",
    "            # step2: loss backpropagation\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # step3: update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            log_dict[\"train_loss_per_batch\"].append(loss.item())\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        #! each epoch, evaluate the model\n",
    "        ######################\n",
    "        # 验证 #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            train_eval_res = eval_model(model, train_loader)\n",
    "            train_acc = train_eval_res[\"accuracy\"]\n",
    "            train_loss = train_eval_res[\"avg_loss\"]\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Train. Acc.: {train_acc:.3f}% | Loss: {train_loss:.4f}\"\n",
    "            )\n",
    "            log_dict[\"train_loss_per_epoch\"].append(train_loss)\n",
    "            log_dict[\"train_acc_per_epoch\"].append(train_acc)\n",
    "\n",
    "            # * each epoch, evaluate the model on the validation dataset which is not used for training\n",
    "            valid_eval_res = eval_model(model, valid_loader)\n",
    "            valid_acc = valid_eval_res[\"accuracy\"]\n",
    "            valid_loss = valid_eval_res[\"avg_loss\"]\n",
    "            log_dict[\"valid_loss_per_epoch\"].append(valid_loss)\n",
    "            log_dict[\"valid_acc_per_epoch\"].append(valid_acc)\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Valid. Acc.: {valid_acc:.3f}% | Loss: {valid_loss:.4f}\"\n",
    "            )\n",
    "            # * save the model if the validation loss is decreased\n",
    "            if valid_loss <= log_dict[\"valid_loss_min\"]:\n",
    "                print(\n",
    "                    f\"**Validation loss decreased ({log_dict['valid_loss_min']:.6f} --> {valid_loss:.6f}). Saving model ...\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), f\"{model_name}_cifar.pt\")\n",
    "                log_dict[\"valid_loss_min\"] = valid_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # scheduler.step()\n",
    "            scheduler.step(valid_loss)\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "# model = ResNet18(num_classes=10).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# log_dict = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     num_epochs=20,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     scheduler=scheduler,\n",
    "#     model_name=\"ResNet34\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(log_dict: dict, num_epochs: int):\n",
    "    loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "    train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "    valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "    learning_rates = log_dict[\"learning_rates\"]\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "    train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "    valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "\n",
    "    running_avg_loss = np.convolve(loss_list, np.ones(200) / 200, mode=\"valid\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axs[0].plot(train_loss_per_epoch, label=\"Training Loss\")\n",
    "    axs[0].plot(valid_loss_per_epoch, label=\"Valid Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(f\"Loss on {model_name}\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # 标记学习率变化\n",
    "    # iterations_per_epoch = len(loss_list) // num_epochs\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[0].axvline(\n",
    "    #             x=epoch * iterations_per_epoch, linestyle=\"--\", color=\"gray\", alpha=0.8\n",
    "    #         )\n",
    "    #         axs[0].text(\n",
    "    #             epoch * iterations_per_epoch + 100,\n",
    "    #             max(loss_list),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    # plot training accuracy\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(train_acc) + 1),\n",
    "        train_acc,\n",
    "        label=\"Training Accuracy\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(valid_acc) + 1),\n",
    "        valid_acc,\n",
    "        label=\"Valid Accuracy\",\n",
    "    )\n",
    "    axs[1].xlim = (0, num_epochs + 1)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"Accuracy on {model_name}\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[1].axvline(x=epoch, linestyle=\"--\", color=\"gray\", alpha=0.8)\n",
    "    #         axs[1].text(\n",
    "    #             epoch + 0.2,\n",
    "    #             max(train_acc),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    fig.savefig(f\"{model_name}_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_list, label=\"Minibatch Loss\")\n",
    "    plt.plot(running_avg_loss, label=\"Running Average Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss on {model_name}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.savefig(f\"{model_name}_training_loss.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_training_metrics(log_dict, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_loader, model_name):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "#     test_loss = test_eval_res[\"avg_loss\"]\n",
    "#     test_acc = test_eval_res[\"accuracy\"]\n",
    "#     class_correct = test_eval_res[\"class_correct\"]\n",
    "#     class_total = test_eval_res[\"class_total\"]\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "#     for i in range(num_classes):\n",
    "#         print(\n",
    "#             \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "#             % (\n",
    "#                 classes[i],\n",
    "#                 100 * class_correct[i] / class_total[i],\n",
    "#                 np.sum(class_correct[i]),\n",
    "#                 np.sum(class_total[i]),\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, model_name):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "    test_loss = test_eval_res[\"avg_loss\"]\n",
    "    test_acc = test_eval_res[\"accuracy\"]\n",
    "    class_correct = test_eval_res[\"class_correct\"]\n",
    "    class_total = test_eval_res[\"class_total\"]\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"Model: {model_name}\")\n",
    "    output.append(f\"Test Loss: {test_loss:.4f}\")\n",
    "    output.append(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "    for i in range(num_classes):\n",
    "        output.append(\n",
    "            \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "            % (\n",
    "                classes[i],\n",
    "                100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]),\n",
    "                np.sum(class_total[i]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    for line in output:\n",
    "        print(line)\n",
    "\n",
    "    # 将结果写入文本文件\n",
    "    with open(f\"{model_name}_test_results.txt\", \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(model, data_loader, classes, model_name):\n",
    "    # step1: get 10 sample images from the data loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images[:10], labels[:10]\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # step2: get model predictions and calculate accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    correct_count = (predicted == labels).sum().item()\n",
    "    accuracy = correct_count / len(labels) * 100\n",
    "\n",
    "    # step3: plot the images with the predicted labels\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(\n",
    "        f\"10 CIFAR-10 Images on Test Dataset using {model_name}\\nAccuracy: {accuracy:.2f}%\",\n",
    "        fontsize=16,\n",
    "        fontweight=600,\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        color = \"blue\" if predicted[i] == labels[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}\",\n",
    "            fontsize=12,\n",
    "            color=color,\n",
    "            y=-0.25,\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{model_name}_cifar10_predictions.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resnet_compare(log_dicts):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    color_list = [\"#6495ED\", \"#4EEE94\", \"#EEC900\", \"#FF6347\", \"#BA55D3\"]\n",
    "    ind_color = 0\n",
    "    for log_dict in log_dicts:\n",
    "        loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "        train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "        valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "        learning_rates = log_dict[\"learning_rates\"]\n",
    "        model_name = log_dict[\"model_name\"]\n",
    "        train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "        valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "        # axs[0].scatter(\n",
    "        #     np.arange(1, len(valid_loss_per_epoch) + 1),\n",
    "        #     valid_loss_per_epoch,\n",
    "        #     label=f\"{model_name}\",\n",
    "        #     s=20,\n",
    "        #     color=next(color),\n",
    "        # )\n",
    "        axs[0].plot(\n",
    "            np.arange(1, len(valid_loss_per_epoch) + 1),\n",
    "            valid_loss_per_epoch,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "        # axs[1].scatter(\n",
    "        #     np.arange(1, len(valid_acc) + 1),\n",
    "        #     valid_acc,\n",
    "        #     label=f\"{model_name}\",\n",
    "        #     s=20,\n",
    "        #     color=next(color),\n",
    "        # )\n",
    "        axs[1].plot(\n",
    "            np.arange(1, len(valid_acc) + 1),\n",
    "            valid_acc,\n",
    "            \".--\",\n",
    "            color=color_list[ind_color],\n",
    "            label=f\"{model_name}\",\n",
    "        )\n",
    "        ind_color += 1\n",
    "\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Validation Loss\")\n",
    "    axs[0].set_title(f\"CIFAR10 Validation Loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"CIFAR10 Validation Accuracy\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[1].grid(True)\n",
    "    fig.savefig(\"Resnet_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resnet18WithoutShortcut for 130 epochs with initial learning rate 0.1...\n",
      "Epoch: 001/130 | Current Learning Rate: 0.100000\n",
      "Epoch: 001/130 | Batch 0000/0313 | Loss: 2.3213\n",
      "Epoch: 001/130 | Batch 0050/0313 | Loss: 2.0015\n",
      "Epoch: 001/130 | Batch 0100/0313 | Loss: 1.9828\n",
      "Epoch: 001/130 | Batch 0150/0313 | Loss: 1.8836\n",
      "Epoch: 001/130 | Batch 0200/0313 | Loss: 1.9592\n",
      "Epoch: 001/130 | Batch 0250/0313 | Loss: 1.7032\n",
      "Epoch: 001/130 | Batch 0300/0313 | Loss: 1.7720\n",
      "**Epoch: 001/130 | Train. Acc.: 32.398% | Loss: 1.8546\n",
      "**Epoch: 001/130 | Valid. Acc.: 36.080% | Loss: 1.7514\n",
      "**Validation loss decreased (inf --> 1.751390). Saving model ...\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 002/130 | Current Learning Rate: 0.100000\n",
      "Epoch: 002/130 | Batch 0000/0313 | Loss: 1.7004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_dicts\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 开始训练所有模型\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m log_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_all_resnet_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36mtrain_all_resnet_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     26\u001b[0m     optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 29\u001b[0m log_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m log_dicts\u001b[38;5;241m.\u001b[39mappend(log_dict)\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cifar.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[50], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, num_epochs, model_name, optimizer, loss_fn, scheduler)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# step3: update model parameters\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 48\u001b[0m log_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss_per_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"Resnet18WithoutShortcut\": (Resnet18WithoutShortcut(num_classes=10).to(device), 130, 0.1),\n",
    "        \"ResNet18\": (ResNet18(num_classes=10).to(device), 130, 0.1),\n",
    "        # \"ResNet34\": (ResNet34(num_classes=10).to(device), 130, 0.1),\n",
    "        # \"ResNet50\": (ResNet50(num_classes=10).to(device), 130, 0.1),\n",
    "        # \"ResNet101\": (ResNet101(num_classes=10).to(device), 130, 0.1),\n",
    "        # \"ResNet152\": (ResNet152(num_classes=10).to(device), 130, 0.01),\n",
    "    }\n",
    "\n",
    "    log_dicts = []\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "        # optimizer = optim.SGD(\n",
    "        #     model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        # )\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=6, verbose=True\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        log_dict = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            scheduler=scheduler,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        log_dicts.append(log_dict)\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_cifar.pt\"))\n",
    "        plot_training_metrics(log_dict, num_epochs)\n",
    "        test(model, test_loader, model_name)\n",
    "        # plot_images_with_predictions(model, test_loader, classes, model_name)\n",
    "\n",
    "    return log_dicts\n",
    "\n",
    "\n",
    "# 开始训练所有模型\n",
    "log_dicts = train_all_resnet_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_resnet_compare(log_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
