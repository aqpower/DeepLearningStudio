{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch Gradient Descent\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "num_classes = 10\n",
    "\n",
    "# the percentage of the training dataset to use as validation dataset\n",
    "valid_percentage = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)\n",
    "\n",
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*JEGNYy9rXMj_XN7W1Qjo9g.png)\n",
    "![](https://img-blog.csdnimg.cn/20200104153325358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104162456690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "   ReLU\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    # 二维卷积层\n",
    "    # in_channels：输入特征图的通道数。例如，对于RGB图像，in_channels为3。\n",
    "    # out_channels：输出特征图的通道数。这个值决定了卷积核的数量，即我们希望提取多少个特征。\n",
    "    # kernel_size：卷积核的大小，可以是一个整数或一个元组。例如，kernel_size=3表示使用3x3的卷积核。\n",
    "    # stride：卷积核的步幅，决定卷积核在输入特征图上移动的步长。默认值为1。\n",
    "    # padding：填充方式，为了保持特征图的尺寸，可以在输入特征图的边缘填充0。padding=1表示在所有边缘填充1个像素。\n",
    "    # bias：是否添加偏置项。默认值为True。这里不使用偏置项（bias=False），因为后面有批量归一化层。\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer.\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer.\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = (\n",
    "        1  # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    )\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Initializes the basic block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1），我们需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Define an input tensor with shape (batch_size, in_channels, height, width)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "# Create a basic block instance\n",
    "block = BasicBlock(64, 64)\n",
    "# Forward pass\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   1x1 Conv (Reduction)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   3x3 Conv\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   1x1 Conv (Expansion)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    A bottleneck residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer (1x1).\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer (3x3).\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        conv3: Third convolutional layer (1x1).\n",
    "        bn3: Batch normalization for the third convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet model.\n",
    "\n",
    "    Attributes:\n",
    "        in_channels: Number of input channels.\n",
    "        conv1: Initial convolutional layer.\n",
    "        bn1: Batch normalization for the initial convolutional layer.\n",
    "        maxpool: Max pooling layer.\n",
    "        layer1: First layer of residual blocks.\n",
    "        layer2: Second layer of residual blocks.\n",
    "        layer3: Third layer of residual blocks.\n",
    "        layer4: Fourth layer of residual blocks.\n",
    "        avgpool: Global average pooling layer.\n",
    "        fc: Fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, block: type[BasicBlock], num_blocks: list[int], num_classes: int = 1000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ResNet model.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            num_blocks: A list containing the number of blocks in each layer.\n",
    "            num_classes: Number of output classes. Default is 1000.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, block: type[BasicBlock], out_channels: int, num_blocks: int, stride: int\n",
    "    ) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a layer of residual blocks.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            out_channels: Number of output channels.\n",
    "            num_blocks: Number of blocks in the layer.\n",
    "            stride: Stride for the first block.\n",
    "\n",
    "        Returns:\n",
    "            A sequential container of residual blocks.\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-18 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "summary(ResNet18(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-34 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet34(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-50 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet50(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet101(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-101 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet101(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-152 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "\n",
    "\n",
    "summary(ResNet152(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_data = CIFAR10(root=\"../data\", train=True, download=True, transform=transform)\n",
    "test_data = CIFAR10(root=\"../data\", train=False, transform=transform)\n",
    "\n",
    "# make indices for spliting the training dataset into training and validation\n",
    "len_train = len(train_data)\n",
    "indices = list(range(len_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_percentage * len_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# we use mini-batch gradient descent to train the model\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset size\n",
    "def check_dataset(loader, set_name):\n",
    "    print(f\"{set_name} Set:\")\n",
    "    images, labels = next(iter(loader))\n",
    "    print(\"batch count\", len(loader))\n",
    "    print(\"image size per batch\", images.size())\n",
    "    print(\"label size per batch\", labels.size())\n",
    "\n",
    "\n",
    "check_dataset(train_loader, \"Training\")\n",
    "check_dataset(valid_loader, \"Valid\")\n",
    "check_dataset(test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    class_total = [0.0 for _ in range(num_classes)]\n",
    "    class_correct = [0.0 for _ in range(num_classes)]\n",
    "    sum_loss, num_correct, num_examples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the model output\n",
    "            outputs = model(features)\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute the correct radix\n",
    "            num_examples += targets.size(0)\n",
    "            num_correct += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # compute each class 's correct count\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_correct[label] += (predicted_labels[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = num_correct / num_examples * 100\n",
    "    avg_loss = sum_loss / num_examples\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"class_correct\": class_correct,\n",
    "        \"class_total\": class_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    num_epochs: int = 25,\n",
    "    model_name: str = \"model\",\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    scheduler=None,\n",
    ") -> dict:\n",
    "    log_dict = {\n",
    "        \"train_loss_per_batch\": [],\n",
    "        \"train_acc_per_epoch\": [],\n",
    "        \"valid_acc_per_epoch\": [],\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"valid_loss_per_epoch\": [],\n",
    "        \"valid_loss_min\": np.Inf,\n",
    "        \"learning_rates\": [],  \n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        log_dict[\"learning_rates\"].append(current_lr)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr}\"\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader, 0):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # step1: predict the output\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # step2: update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            log_dict[\"train_loss_per_batch\"].append(loss.item())\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # each epoch, evaluate the model\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            train_eval_res = eval_model(model, train_loader)\n",
    "            train_acc = train_eval_res[\"accuracy\"]\n",
    "            train_loss = train_eval_res[\"avg_loss\"]\n",
    "            print(\n",
    "                f\"#Epoch: {epoch+1:03d}/{num_epochs:03d} | Train. Acc.: {train_acc:.3f}% | Loss: {train_loss:.3f}\"\n",
    "            )\n",
    "            log_dict[\"train_loss_per_epoch\"].append(train_loss)\n",
    "            log_dict[\"train_acc_per_epoch\"].append(train_acc)\n",
    "\n",
    "            # * each epoch, evaluate the model on the validation dataset which is not used for training\n",
    "            valid_eval_res = eval_model(model, valid_loader)\n",
    "            valid_acc = valid_eval_res[\"accuracy\"]\n",
    "            valid_loss = valid_eval_res[\"avg_loss\"]\n",
    "            log_dict[\"valid_loss_per_epoch\"].append(valid_loss)\n",
    "            log_dict[\"valid_acc_per_epoch\"].append(valid_acc)\n",
    "            print(\n",
    "                f\"#Epoch: {epoch+1:03d}/{num_epochs:03d} | Valid. Acc.: {valid_acc:.3f}% | Loss: {valid_loss:.3f}\"\n",
    "            )\n",
    "            # * save the model if the validation loss is decreased\n",
    "            if valid_loss <= log_dict[\"valid_loss_min\"]:\n",
    "                print(\n",
    "                    f\"#Validation loss decreased ({log_dict['valid_loss_min']:.6f} --> {valid_loss:.6f}). Saving model ...\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), \"model_cifar.pt\")\n",
    "                log_dict[\"valid_loss_min\"] = valid_loss\n",
    "\n",
    "        if(scheduler is not None):\n",
    "            scheduler.step()\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "# model = ResNet34(num_classes=10).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# log_dict = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     scheduler=scheduler,\n",
    "#     model_name=\"ResNet34\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(log_dict: dict):\n",
    "    loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "    train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "    valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "    learning_rates = log_dict[\"learning_rates\"]\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "    train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "    valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "\n",
    "    running_avg_loss = np.convolve(loss_list, np.ones(200) / 200, mode=\"valid\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # plot training loss\n",
    "    axs[0].plot(loss_list, label=\"Minibatch Loss\", alpha=0.5)\n",
    "    axs[0].plot(running_avg_loss, label=\"Running Average Loss\", linewidth=2)\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"Cross Entropy Loss\")\n",
    "    axs[0].set_title(f\"Training Loss on {model_name}\", fontsize=14, pad=15)\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_yscale(\"log\")\n",
    "\n",
    "    # 标记学习率变化\n",
    "    iterations_per_epoch = len(loss_list) // num_epochs\n",
    "    for epoch, lr in enumerate(learning_rates):\n",
    "        if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "            axs[0].axvline(\n",
    "                x=epoch * iterations_per_epoch, linestyle=\"--\", color=\"gray\", alpha=0.8\n",
    "            )\n",
    "            axs[0].text(\n",
    "                epoch * iterations_per_epoch + 200,\n",
    "                min(loss_list),\n",
    "                f\"lr: {lr:.1e}\",\n",
    "                rotation=0,\n",
    "                verticalalignment=\"bottom\",\n",
    "            )\n",
    "\n",
    "    # plot training accuracy\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(train_acc) + 1),\n",
    "        train_acc,\n",
    "        label=\"Training Accuracy\",\n",
    "        color=\"blue\",\n",
    "        markersize=6,\n",
    "        linewidth=2,\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(valid_acc) + 1),\n",
    "        valid_acc,\n",
    "        label=\"Valid Accuracy\",\n",
    "        color=\"red\",\n",
    "        markersize=6,\n",
    "        linewidth=2,\n",
    "    )\n",
    "    axs[1].xlim = (0, num_epochs + 1)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"Accuracy on {model_name}\", fontsize=14, pad=15)\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    fig.savefig(f\"{model_name}_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_per_epoch, label=\"Train Loss\")\n",
    "    plt.plot(valid_loss_per_epoch, label=\"Valid Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss on {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name}_training_and_validation_loss.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "# plot_training_metrics(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_loader, model_name):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         test_eval_res = eval_model(model, test_loader)\n",
    "    \n",
    "#     test_loss = test_eval_res[\"avg_loss\"]\n",
    "#     test_acc = test_eval_res[\"accuracy\"]\n",
    "#     class_correct = test_eval_res[\"class_correct\"]\n",
    "#     class_total = test_eval_res[\"class_total\"]\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "#     for i in range(num_classes):\n",
    "#         print(\n",
    "#             \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "#             % (\n",
    "#                 classes[i],\n",
    "#                 100 * class_correct[i] / class_total[i],\n",
    "#                 np.sum(class_correct[i]),\n",
    "#                 np.sum(class_total[i]),\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, model_name):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "    test_loss = test_eval_res[\"avg_loss\"]\n",
    "    test_acc = test_eval_res[\"accuracy\"]\n",
    "    class_correct = test_eval_res[\"class_correct\"]\n",
    "    class_total = test_eval_res[\"class_total\"]\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"Model: {model_name}\")\n",
    "    output.append(f\"Test Loss: {test_loss:.4f}\")\n",
    "    output.append(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "    for i in range(num_classes):\n",
    "        output.append(\n",
    "            \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "            % (\n",
    "                classes[i],\n",
    "                100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]),\n",
    "                np.sum(class_total[i]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    for line in output:\n",
    "        print(line)\n",
    "\n",
    "    # 将结果写入文本文件\n",
    "    with open(f\"{model_name}_test_results.txt\", \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(model, data_loader, classes, model_name):\n",
    "    # step1: get 10 sample images from the data loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images[:10], labels[:10]\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # step2: get model predictions and calculate accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    correct_count = (predicted == labels).sum().item()\n",
    "    accuracy = correct_count / len(labels) * 100\n",
    "\n",
    "    # step3: plot the images with the predicted labels\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(\n",
    "        f\"10 CIFAR-10 Images on Test Dataset using {model_name}\\nAccuracy: {accuracy:.2f}%\",\n",
    "        fontsize=16,\n",
    "        fontweight=600,\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        color = \"blue\" if predicted[i] == labels[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}\",\n",
    "            fontsize=12,\n",
    "            color=color,\n",
    "            y=-0.25,\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{model_name}_cifar10_predictions.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"ResNet18\": (ResNet18(num_classes=10).to(device), 50, 0.1, 10),\n",
    "        \"ResNet34\": (ResNet34(num_classes=10).to(device), 50, 0.1, 10),\n",
    "        \"ResNet50\": (ResNet50(num_classes=10).to(device), 100, 0.1, 30),\n",
    "        \"ResNet101\": (ResNet101(num_classes=10).to(device), 100, 0.1, 30),\n",
    "        \"ResNet152\": (ResNet152(num_classes=10).to(device), 100, 0.1, 30),\n",
    "    }\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr, step_size) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "        log_dict = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            scheduler=scheduler,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_cifar.pt\"))\n",
    "        plot_training_metrics(log_dict, num_epochs)\n",
    "        test(model, test_loader, model_name)\n",
    "        plot_images_with_predictions(model, test_loader, classes, model_name)\n",
    "\n",
    "\n",
    "# 开始训练所有模型\n",
    "train_all_resnet_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
