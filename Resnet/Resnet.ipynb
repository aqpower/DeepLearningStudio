{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# 在每次训练前设置随机种子\n",
    "seed = 9  # 可以每次更改这个值\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "num_classes = 10\n",
    "\n",
    "# the percentage of the training dataset to use as validation dataset\n",
    "valid_percentage = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)\n",
    "\n",
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*JEGNYy9rXMj_XN7W1Qjo9g.png)\n",
    "![](https://img-blog.csdnimg.cn/20200104153325358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200104162456690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rfc29ydA==,size_16,color_FFFFFF,t_70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "   ReLU\n",
    "      |\n",
    "   Conv Layer\n",
    "      |\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    # 二维卷积层\n",
    "    # in_channels：输入特征图的通道数。例如，对于RGB图像，in_channels为3。\n",
    "    # out_channels：输出特征图的通道数。这个值决定了卷积核的数量，即我们希望提取多少个特征。\n",
    "    # kernel_size：卷积核的大小，可以是一个整数或一个元组。例如，kernel_size=3表示使用3x3的卷积核。\n",
    "    # stride：卷积核的步幅，决定卷积核在输入特征图上移动的步长。默认值为1。\n",
    "    # padding：填充方式，为了保持特征图的尺寸，可以在输入特征图的边缘填充0。padding=1表示在所有边缘填充1个像素。\n",
    "    # bias：是否添加偏置项。默认值为True。这里不使用偏置项（bias=False），因为后面有批量归一化层。\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer.\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer.\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = (\n",
    "        1  # 输出通道数相对于输入通道数的扩展倍数。对于基本块，扩展倍数为1。\n",
    "    )\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Initializes the basic block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        # 对卷积层的输出进行归一化处理。这有助于加速训练并稳定模型。\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 在残差块中，如果输入和输出的形状不一致（例如通道数不同或步幅不为1），我们需要通过一个卷积层来调整输入的形状，使其与输出形状一致。\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 如果需要，则定义一个包含1x1卷积层和批量归一化层的顺序容器。\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Define an input tensor with shape (batch_size, in_channels, height, width)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "# Create a basic block instance\n",
    "block = BasicBlock(64, 64)\n",
    "# Forward pass\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Input\n",
    "      |\n",
    "   1x1 Conv (Reduction)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   3x3 Conv\n",
    "      |\n",
    "   BatchNorm\n",
    "      |\n",
    "    ReLU\n",
    "      |\n",
    "   1x1 Conv (Expansion)\n",
    "      |\n",
    "   BatchNorm\n",
    "      |----------------\n",
    "      |               |\n",
    "     ReLU             |\n",
    "      |               |\n",
    "     Output          Input\n",
    "      |               |\n",
    "      ----------------\n",
    "           |\n",
    "         Output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    A bottleneck residual block for ResNet.\n",
    "\n",
    "    Attributes:\n",
    "        conv1: First convolutional layer (1x1).\n",
    "        bn1: Batch normalization for the first convolutional layer.\n",
    "        conv2: Second convolutional layer (3x3).\n",
    "        bn2: Batch normalization for the second convolutional layer.\n",
    "        conv3: Third convolutional layer (1x1).\n",
    "        bn3: Batch normalization for the third convolutional layer.\n",
    "        shortcut: Shortcut connection to match input and output dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the convolution. Default is 1.\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet model.\n",
    "\n",
    "    Attributes:\n",
    "        in_channels: Number of input channels.\n",
    "        conv1: Initial convolutional layer.\n",
    "        bn1: Batch normalization for the initial convolutional layer.\n",
    "        maxpool: Max pooling layer.\n",
    "        layer1: First layer of residual blocks.\n",
    "        layer2: Second layer of residual blocks.\n",
    "        layer3: Third layer of residual blocks.\n",
    "        layer4: Fourth layer of residual blocks.\n",
    "        avgpool: Global average pooling layer.\n",
    "        fc: Fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, block: type[BasicBlock], num_blocks: list[int], num_classes: int = 1000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ResNet model.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            num_blocks: A list containing the number of blocks in each layer.\n",
    "            num_classes: Number of output classes. Default is 1000.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # 考虑到CIFAR10数据集的图片尺寸太小，ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息\n",
    "        # 所以在实验中我们将7x7的降采样层和最大池化层去掉，替换为一个3x3的降采样卷积，同时减小该卷积层的步长和填充大小，\n",
    "        # 这样可以尽可能保留原始图像的信息。\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5), nn.Linear(512 * block.expansion, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, block: type[BasicBlock], out_channels: int, num_blocks: int, stride: int\n",
    "    ) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a layer of residual blocks.\n",
    "\n",
    "        Args:\n",
    "            block: A residual block.\n",
    "            out_channels: Number of output channels.\n",
    "            num_blocks: Number of blocks in the layer.\n",
    "            stride: Stride for the first block.\n",
    "\n",
    "        Returns:\n",
    "            A sequential container of residual blocks.\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-18 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet18(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-34 model.\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet34(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-50 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet50(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet101(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-101 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "# summary(ResNet101(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152(num_classes: int = 10) -> ResNet:\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        A ResNet-152 model.\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n",
    "\n",
    "# summary(ResNet152(num_classes=num_classes), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from cutout import Cutout\n",
    "\n",
    "# 数据增强变换\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # 随机裁剪，填充4个像素\n",
    "        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # 颜色抖动\n",
    "        transforms.RandomRotation(15),  # 随机旋转\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "        Cutout(n_holes=1, length=16),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 验证集和测试集变换（不进行数据增强）\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),  # 归一化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 将数据转换为torch.FloatTensor，并标准化。\n",
    "train_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_train)\n",
    "valid_data = CIFAR10(\"../data\", train=True, download=True, transform=transform_test)\n",
    "test_data = CIFAR10(\"../data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "# random indices\n",
    "np.random.shuffle(indices)\n",
    "# the ratio of split\n",
    "split = int(np.floor(valid_percentage * num_train))\n",
    "# divide data to radin_data and valid_data\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "# 无放回地按照给定的索引列表采样样本元素\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data, batch_size=batch_size, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "batch count 313\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n",
      "Valid Set:\n",
      "batch count 79\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n",
      "Testing Set:\n",
      "batch count 79\n",
      "image size per batch torch.Size([128, 3, 32, 32])\n",
      "label size per batch torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset size\n",
    "def check_dataset(loader, set_name):\n",
    "    print(f\"{set_name} Set:\")\n",
    "    images, labels = next(iter(loader))\n",
    "    print(\"batch count\", len(loader))\n",
    "    print(\"image size per batch\", images.size())\n",
    "    print(\"label size per batch\", labels.size())\n",
    "\n",
    "\n",
    "check_dataset(train_loader, \"Training\")\n",
    "check_dataset(valid_loader, \"Valid\")\n",
    "check_dataset(test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    class_total = [0.0 for _ in range(num_classes)]\n",
    "    class_correct = [0.0 for _ in range(num_classes)]\n",
    "    sum_loss, num_correct, num_examples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the model output\n",
    "            outputs = model(features)\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute the correct radix\n",
    "            num_examples += targets.size(0)\n",
    "            num_correct += (predicted_labels == targets).sum().item()\n",
    "\n",
    "            # compute each class 's correct count\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                class_correct[label] += (predicted_labels[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = num_correct / num_examples * 100\n",
    "    avg_loss = sum_loss / num_examples\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"class_correct\": class_correct,\n",
    "        \"class_total\": class_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    num_epochs: int = 25,\n",
    "    model_name: str = \"model\",\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    scheduler=None,\n",
    ") -> dict:\n",
    "    log_dict = {\n",
    "        \"train_loss_per_batch\": [],\n",
    "        \"train_acc_per_epoch\": [],\n",
    "        \"valid_acc_per_epoch\": [],\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"valid_loss_per_epoch\": [],\n",
    "        \"valid_loss_min\": np.Inf,\n",
    "        \"learning_rates\": [],\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"learning_rates\"].append(current_lr)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader, 0):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # step1: predict the output\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # step2: update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            log_dict[\"train_loss_per_batch\"].append(loss.item())\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # each epoch, evaluate the model\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            train_eval_res = eval_model(model, train_loader)\n",
    "            train_acc = train_eval_res[\"accuracy\"]\n",
    "            train_loss = train_eval_res[\"avg_loss\"]\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Train. Acc.: {train_acc:.3f}% | Loss: {train_loss:.4f}\"\n",
    "            )\n",
    "            log_dict[\"train_loss_per_epoch\"].append(train_loss)\n",
    "            log_dict[\"train_acc_per_epoch\"].append(train_acc)\n",
    "\n",
    "            # * each epoch, evaluate the model on the validation dataset which is not used for training\n",
    "            valid_eval_res = eval_model(model, valid_loader)\n",
    "            valid_acc = valid_eval_res[\"accuracy\"]\n",
    "            valid_loss = valid_eval_res[\"avg_loss\"]\n",
    "            log_dict[\"valid_loss_per_epoch\"].append(valid_loss)\n",
    "            log_dict[\"valid_acc_per_epoch\"].append(valid_acc)\n",
    "            print(\n",
    "                f\"**Epoch: {epoch+1:03d}/{num_epochs:03d} | Valid. Acc.: {valid_acc:.3f}% | Loss: {valid_loss:.4f}\"\n",
    "            )\n",
    "            # * save the model if the validation loss is decreased\n",
    "            if valid_loss <= log_dict[\"valid_loss_min\"]:\n",
    "                print(\n",
    "                    f\"**Validation loss decreased ({log_dict['valid_loss_min']:.6f} --> {valid_loss:.6f}). Saving model ...\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), f\"{model_name}_cifar.pt\")\n",
    "                log_dict[\"valid_loss_min\"] = valid_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # scheduler.step()\n",
    "            scheduler.step(valid_loss)\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "# model = ResNet18(num_classes=10).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# log_dict = train(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     valid_loader,\n",
    "#     num_epochs=20,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     scheduler=scheduler,\n",
    "#     model_name=\"ResNet34\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_cifar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(log_dict: dict, num_epochs: int):\n",
    "    loss_list = log_dict[\"train_loss_per_batch\"]\n",
    "    train_acc = log_dict[\"train_acc_per_epoch\"]\n",
    "    valid_acc = log_dict[\"valid_acc_per_epoch\"]\n",
    "    learning_rates = log_dict[\"learning_rates\"]\n",
    "    model_name = log_dict[\"model_name\"]\n",
    "    train_loss_per_epoch = log_dict[\"train_loss_per_epoch\"]\n",
    "    valid_loss_per_epoch = log_dict[\"valid_loss_per_epoch\"]\n",
    "\n",
    "    running_avg_loss = np.convolve(loss_list, np.ones(200) / 200, mode=\"valid\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # plot training loss\n",
    "    axs[0].plot(loss_list, label=\"Minibatch Loss\", alpha=0.5)\n",
    "    axs[0].plot(running_avg_loss, label=\"Running Average Loss\", linewidth=2)\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"Cross Entropy Loss\")\n",
    "    axs[0].set_title(f\"Training Loss on {model_name}\")\n",
    "    axs[0].legend(loc=\"best\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_yscale(\"log\")\n",
    "\n",
    "    # 标记学习率变化\n",
    "    # iterations_per_epoch = len(loss_list) // num_epochs\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[0].axvline(\n",
    "    #             x=epoch * iterations_per_epoch, linestyle=\"--\", color=\"gray\", alpha=0.8\n",
    "    #         )\n",
    "    #         axs[0].text(\n",
    "    #             epoch * iterations_per_epoch + 100,\n",
    "    #             max(loss_list),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    # plot training accuracy\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(train_acc) + 1),\n",
    "        train_acc,\n",
    "        label=\"Training Accuracy\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        np.arange(1, len(valid_acc) + 1),\n",
    "        valid_acc,\n",
    "        label=\"Valid Accuracy\",\n",
    "    )\n",
    "    axs[1].xlim = (0, num_epochs + 1)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axs[1].set_title(f\"Accuracy on {model_name}\")\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # for epoch, lr in enumerate(learning_rates):\n",
    "    #     if epoch == 0 or lr != learning_rates[epoch - 1]:\n",
    "    #         axs[1].axvline(x=epoch, linestyle=\"--\", color=\"gray\", alpha=0.8)\n",
    "    #         axs[1].text(\n",
    "    #             epoch + 0.2,\n",
    "    #             max(train_acc),\n",
    "    #             f\"lr: {lr:.1e}\",\n",
    "    #             rotation=0,\n",
    "    #             verticalalignment=\"bottom\",\n",
    "    #         )\n",
    "\n",
    "    fig.savefig(f\"{model_name}_training_performance.svg\", format=\"svg\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_per_epoch, label=\"Training Loss\")\n",
    "    plt.plot(valid_loss_per_epoch, label=\"Valid Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss on {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_name}_training_and_validation_loss.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_training_metrics(log_dict, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_loader, model_name):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "#     test_loss = test_eval_res[\"avg_loss\"]\n",
    "#     test_acc = test_eval_res[\"accuracy\"]\n",
    "#     class_correct = test_eval_res[\"class_correct\"]\n",
    "#     class_total = test_eval_res[\"class_total\"]\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "#     for i in range(num_classes):\n",
    "#         print(\n",
    "#             \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "#             % (\n",
    "#                 classes[i],\n",
    "#                 100 * class_correct[i] / class_total[i],\n",
    "#                 np.sum(class_correct[i]),\n",
    "#                 np.sum(class_total[i]),\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, model_name):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        test_eval_res = eval_model(model, test_loader)\n",
    "\n",
    "    test_loss = test_eval_res[\"avg_loss\"]\n",
    "    test_acc = test_eval_res[\"accuracy\"]\n",
    "    class_correct = test_eval_res[\"class_correct\"]\n",
    "    class_total = test_eval_res[\"class_total\"]\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"Model: {model_name}\")\n",
    "    output.append(f\"Test Loss: {test_loss:.4f}\")\n",
    "    output.append(f\"Test Accuracy (Overall): {test_acc:.2f}%\\n\")\n",
    "    for i in range(num_classes):\n",
    "        output.append(\n",
    "            \"Test Accuracy of %8s: %2d%% (%2d/%2d)\"\n",
    "            % (\n",
    "                classes[i],\n",
    "                100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]),\n",
    "                np.sum(class_total[i]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 打印结果到控制台\n",
    "    for line in output:\n",
    "        print(line)\n",
    "\n",
    "    # 将结果写入文本文件\n",
    "    with open(f\"{model_name}_test_results.txt\", \"w\") as f:\n",
    "        for line in output:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(model, data_loader, classes, model_name):\n",
    "    # step1: get 10 sample images from the data loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images[:10], labels[:10]\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # step2: get model predictions and calculate accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    correct_count = (predicted == labels).sum().item()\n",
    "    accuracy = correct_count / len(labels) * 100\n",
    "\n",
    "    # step3: plot the images with the predicted labels\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(\n",
    "        f\"10 CIFAR-10 Images on Test Dataset using {model_name}\\nAccuracy: {accuracy:.2f}%\",\n",
    "        fontsize=16,\n",
    "        fontweight=600,\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = np.transpose(images[i].numpy(), (1, 2, 0))\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        color = \"blue\" if predicted[i] == labels[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}\",\n",
    "            fontsize=12,\n",
    "            color=color,\n",
    "            y=-0.25,\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{model_name}_cifar10_predictions.svg\", format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m         test(model, test_loader, model_name)\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# plot_images_with_predictions(model, test_loader, classes, model_name)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 开始训练所有模型\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrain_all_resnet_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mtrain_all_resnet_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_all_resnet_models\u001b[39m():\n\u001b[1;32m      2\u001b[0m     resnet_models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet18\u001b[39m\u001b[38;5;124m\"\u001b[39m: (ResNet18(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.1\u001b[39m),\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet34\u001b[39m\u001b[38;5;124m\"\u001b[39m: (ResNet34(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.1\u001b[39m),\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet50\u001b[39m\u001b[38;5;124m\"\u001b[39m: (ResNet50(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet101\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[43mResNet101\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet152\u001b[39m\u001b[38;5;124m\"\u001b[39m: (ResNet152(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, (model, num_epochs, initial_lr) \u001b[38;5;129;01min\u001b[39;00m resnet_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs with initial learning rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mResNet101\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mResNet101\u001b[39m(num_classes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResNet:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Constructs a ResNet-101 model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        A ResNet-101 model.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mResNet.__init__\u001b[0;34m(self, block, num_blocks, num_classes)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m64\u001b[39m, num_blocks[\u001b[38;5;241m0\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m128\u001b[39m, num_blocks[\u001b[38;5;241m1\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m512\u001b[39m, num_blocks[\u001b[38;5;241m3\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m, in \u001b[0;36mResNet._make_layer\u001b[0;34m(self, block, out_channels, num_blocks, stride)\u001b[0m\n\u001b[1;32m     66\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stride \u001b[38;5;129;01min\u001b[39;00m strides:\n\u001b[0;32m---> 68\u001b[0m     layers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m=\u001b[39m out_channels \u001b[38;5;241m*\u001b[39m block\u001b[38;5;241m.\u001b[39mexpansion\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mlayers)\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mBottleneck.__init__\u001b[0;34m(self, in_channels, out_channels, stride)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\n\u001b[1;32m     28\u001b[0m     in_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(out_channels)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(out_channels)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\n\u001b[1;32m     41\u001b[0m     out_channels,\n\u001b[1;32m     42\u001b[0m     out_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpansion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:450\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    449\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:144\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:150\u001b[0m, in \u001b[0;36m_ConvNd.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_all_resnet_models():\n",
    "    resnet_models = {\n",
    "        \"ResNet18\": (ResNet18(num_classes=10).to(device), 100, 0.1),\n",
    "        \"ResNet34\": (ResNet34(num_classes=10).to(device), 100, 0.1),\n",
    "        \"ResNet50\": (ResNet50(num_classes=10).to(device), 100, 0.01),\n",
    "        \"ResNet101\": (ResNet101(num_classes=10).to(device), 100, 0.01),\n",
    "        \"ResNet152\": (ResNet152(num_classes=10).to(device), 100, 0.01),\n",
    "    }\n",
    "\n",
    "    for model_name, (model, num_epochs, initial_lr) in resnet_models.items():\n",
    "        print(\n",
    "            f\"Training {model_name} for {num_epochs} epochs with initial learning rate {initial_lr}...\"\n",
    "        )\n",
    "        # optimizer = optim.SGD(\n",
    "        #     model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        # )\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=6, verbose=True\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        log_dict = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            scheduler=scheduler,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_cifar.pt\"))\n",
    "        plot_training_metrics(log_dict, num_epochs)\n",
    "        test(model, test_loader, model_name)\n",
    "        # plot_images_with_predictions(model, test_loader, classes, model_name)\n",
    "\n",
    "\n",
    "# 开始训练所有模型\n",
    "train_all_resnet_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
