Sun Jun  9 02:56:54 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:4B:00.0 Off |                  N/A |
| 30%   28C    P8    15W / 350W |      0MiB / 24268MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
now using device:  cuda
torch.Size([1, 64, 32, 32])
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Training Set:
batch count 313
image size per batch torch.Size([128, 3, 32, 32])
label size per batch torch.Size([128])
Valid Set:
batch count 79
image size per batch torch.Size([128, 3, 32, 32])
label size per batch torch.Size([128])
Testing Set:
batch count 79
image size per batch torch.Size([128, 3, 32, 32])
label size per batch torch.Size([128])
Training ResNet18 for 130 epochs with initial learning rate 0.1...
Epoch: 001/130 | Current Learning Rate: 0.100000
Epoch: 001/130 | Batch 0000/0313 | Loss: 2.4450
Epoch: 001/130 | Batch 0050/0313 | Loss: 2.2707
Epoch: 001/130 | Batch 0100/0313 | Loss: 1.9575
Epoch: 001/130 | Batch 0150/0313 | Loss: 1.9676
Epoch: 001/130 | Batch 0200/0313 | Loss: 1.9336
Epoch: 001/130 | Batch 0250/0313 | Loss: 1.8372
Epoch: 001/130 | Batch 0300/0313 | Loss: 1.7127
**Epoch: 001/130 | Train. Acc.: 33.883% | Loss: 1.7577
**Epoch: 001/130 | Valid. Acc.: 35.850% | Loss: 1.6898
**Validation loss decreased (inf --> 1.689791). Saving model ...
Time elapsed: 0.56 min
Epoch: 002/130 | Current Learning Rate: 0.100000
Epoch: 002/130 | Batch 0000/0313 | Loss: 1.7477
Epoch: 002/130 | Batch 0050/0313 | Loss: 1.8253
Epoch: 002/130 | Batch 0100/0313 | Loss: 1.7104
Epoch: 002/130 | Batch 0150/0313 | Loss: 1.7097
Epoch: 002/130 | Batch 0200/0313 | Loss: 1.6445
Epoch: 002/130 | Batch 0250/0313 | Loss: 1.5476
Epoch: 002/130 | Batch 0300/0313 | Loss: 1.5857
**Epoch: 002/130 | Train. Acc.: 41.032% | Loss: 1.5838
**Epoch: 002/130 | Valid. Acc.: 44.270% | Loss: 1.5021
**Validation loss decreased (1.689791 --> 1.502100). Saving model ...
Time elapsed: 1.09 min
Epoch: 003/130 | Current Learning Rate: 0.100000
Epoch: 003/130 | Batch 0000/0313 | Loss: 1.6654
Epoch: 003/130 | Batch 0050/0313 | Loss: 1.3637
Epoch: 003/130 | Batch 0100/0313 | Loss: 1.4147
Epoch: 003/130 | Batch 0150/0313 | Loss: 1.3777
Epoch: 003/130 | Batch 0200/0313 | Loss: 1.3930
Epoch: 003/130 | Batch 0250/0313 | Loss: 1.5087
Epoch: 003/130 | Batch 0300/0313 | Loss: 1.3737
**Epoch: 003/130 | Train. Acc.: 50.345% | Loss: 1.3759
**Epoch: 003/130 | Valid. Acc.: 54.510% | Loss: 1.2510
**Validation loss decreased (1.502100 --> 1.250951). Saving model ...
Time elapsed: 1.61 min
Epoch: 004/130 | Current Learning Rate: 0.100000
Epoch: 004/130 | Batch 0000/0313 | Loss: 1.3886
Epoch: 004/130 | Batch 0050/0313 | Loss: 1.2623
Epoch: 004/130 | Batch 0100/0313 | Loss: 1.3526
Epoch: 004/130 | Batch 0150/0313 | Loss: 1.1798
Epoch: 004/130 | Batch 0200/0313 | Loss: 1.2284
Epoch: 004/130 | Batch 0250/0313 | Loss: 1.1026
Epoch: 004/130 | Batch 0300/0313 | Loss: 1.2855
**Epoch: 004/130 | Train. Acc.: 55.983% | Loss: 1.2335
**Epoch: 004/130 | Valid. Acc.: 58.840% | Loss: 1.1779
**Validation loss decreased (1.250951 --> 1.177864). Saving model ...
Time elapsed: 2.13 min
Epoch: 005/130 | Current Learning Rate: 0.100000
Epoch: 005/130 | Batch 0000/0313 | Loss: 1.1478
Epoch: 005/130 | Batch 0050/0313 | Loss: 1.3905
Epoch: 005/130 | Batch 0100/0313 | Loss: 1.2751
Epoch: 005/130 | Batch 0150/0313 | Loss: 1.1097
Epoch: 005/130 | Batch 0200/0313 | Loss: 1.0245
Epoch: 005/130 | Batch 0250/0313 | Loss: 1.0308
Epoch: 005/130 | Batch 0300/0313 | Loss: 1.0955
**Epoch: 005/130 | Train. Acc.: 59.322% | Loss: 1.1418
**Epoch: 005/130 | Valid. Acc.: 62.980% | Loss: 1.0404
**Validation loss decreased (1.177864 --> 1.040446). Saving model ...
Time elapsed: 2.66 min
Epoch: 006/130 | Current Learning Rate: 0.100000
Epoch: 006/130 | Batch 0000/0313 | Loss: 1.1442
Epoch: 006/130 | Batch 0050/0313 | Loss: 1.1654
Epoch: 006/130 | Batch 0100/0313 | Loss: 1.0768
Epoch: 006/130 | Batch 0150/0313 | Loss: 1.0550
Epoch: 006/130 | Batch 0200/0313 | Loss: 0.9672
Epoch: 006/130 | Batch 0250/0313 | Loss: 1.1116
Epoch: 006/130 | Batch 0300/0313 | Loss: 1.0563
**Epoch: 006/130 | Train. Acc.: 62.110% | Loss: 1.0683
**Epoch: 006/130 | Valid. Acc.: 68.630% | Loss: 0.9207
**Validation loss decreased (1.040446 --> 0.920727). Saving model ...
Time elapsed: 3.19 min
Epoch: 007/130 | Current Learning Rate: 0.100000
Epoch: 007/130 | Batch 0000/0313 | Loss: 0.9428
Epoch: 007/130 | Batch 0050/0313 | Loss: 1.0029
Epoch: 007/130 | Batch 0100/0313 | Loss: 0.9703
Epoch: 007/130 | Batch 0150/0313 | Loss: 0.9561
Epoch: 007/130 | Batch 0200/0313 | Loss: 0.9978
Epoch: 007/130 | Batch 0250/0313 | Loss: 0.8230
Epoch: 007/130 | Batch 0300/0313 | Loss: 0.7743
**Epoch: 007/130 | Train. Acc.: 68.245% | Loss: 0.9149
**Epoch: 007/130 | Valid. Acc.: 71.590% | Loss: 0.8071
**Validation loss decreased (0.920727 --> 0.807066). Saving model ...
Time elapsed: 3.72 min
Epoch: 008/130 | Current Learning Rate: 0.100000
Epoch: 008/130 | Batch 0000/0313 | Loss: 0.9323
Epoch: 008/130 | Batch 0050/0313 | Loss: 0.7092
Epoch: 008/130 | Batch 0100/0313 | Loss: 0.7928
Epoch: 008/130 | Batch 0150/0313 | Loss: 0.7662
Epoch: 008/130 | Batch 0200/0313 | Loss: 0.8993
Epoch: 008/130 | Batch 0250/0313 | Loss: 0.7331
Epoch: 008/130 | Batch 0300/0313 | Loss: 1.0022
**Epoch: 008/130 | Train. Acc.: 71.870% | Loss: 0.8071
**Epoch: 008/130 | Valid. Acc.: 74.960% | Loss: 0.7290
**Validation loss decreased (0.807066 --> 0.729021). Saving model ...
Time elapsed: 4.26 min
Epoch: 009/130 | Current Learning Rate: 0.100000
Epoch: 009/130 | Batch 0000/0313 | Loss: 0.7605
Epoch: 009/130 | Batch 0050/0313 | Loss: 0.8050
Epoch: 009/130 | Batch 0100/0313 | Loss: 0.6943
Epoch: 009/130 | Batch 0150/0313 | Loss: 0.7378
Epoch: 009/130 | Batch 0200/0313 | Loss: 0.9263
Epoch: 009/130 | Batch 0250/0313 | Loss: 0.8045
Epoch: 009/130 | Batch 0300/0313 | Loss: 0.7837
**Epoch: 009/130 | Train. Acc.: 72.380% | Loss: 0.8125
**Epoch: 009/130 | Valid. Acc.: 75.960% | Loss: 0.7129
**Validation loss decreased (0.729021 --> 0.712922). Saving model ...
Time elapsed: 4.79 min
Epoch: 010/130 | Current Learning Rate: 0.100000
Epoch: 010/130 | Batch 0000/0313 | Loss: 0.7393
Epoch: 010/130 | Batch 0050/0313 | Loss: 0.8221
Epoch: 010/130 | Batch 0100/0313 | Loss: 0.7551
Epoch: 010/130 | Batch 0150/0313 | Loss: 0.8435
Epoch: 010/130 | Batch 0200/0313 | Loss: 0.8033
Epoch: 010/130 | Batch 0250/0313 | Loss: 0.6600
Epoch: 010/130 | Batch 0300/0313 | Loss: 0.8245
**Epoch: 010/130 | Train. Acc.: 67.787% | Loss: 0.8972
**Epoch: 010/130 | Valid. Acc.: 70.240% | Loss: 0.8400
Time elapsed: 5.33 min
Epoch: 011/130 | Current Learning Rate: 0.100000
Epoch: 011/130 | Batch 0000/0313 | Loss: 0.6644
Epoch: 011/130 | Batch 0050/0313 | Loss: 0.7611
Epoch: 011/130 | Batch 0100/0313 | Loss: 0.7872
Epoch: 011/130 | Batch 0150/0313 | Loss: 0.7763
Epoch: 011/130 | Batch 0200/0313 | Loss: 0.5905
Epoch: 011/130 | Batch 0250/0313 | Loss: 0.7160
Epoch: 011/130 | Batch 0300/0313 | Loss: 0.7974
**Epoch: 011/130 | Train. Acc.: 69.582% | Loss: 0.8795
**Epoch: 011/130 | Valid. Acc.: 73.630% | Loss: 0.7852
Time elapsed: 5.86 min
Epoch: 012/130 | Current Learning Rate: 0.100000
Epoch: 012/130 | Batch 0000/0313 | Loss: 0.6274
Epoch: 012/130 | Batch 0050/0313 | Loss: 0.6036
Epoch: 012/130 | Batch 0100/0313 | Loss: 0.8095
Epoch: 012/130 | Batch 0150/0313 | Loss: 0.7072
Epoch: 012/130 | Batch 0200/0313 | Loss: 0.6704
Epoch: 012/130 | Batch 0250/0313 | Loss: 0.7298
Epoch: 012/130 | Batch 0300/0313 | Loss: 0.5507
**Epoch: 012/130 | Train. Acc.: 74.695% | Loss: 0.7251
**Epoch: 012/130 | Valid. Acc.: 78.020% | Loss: 0.6429
**Validation loss decreased (0.712922 --> 0.642893). Saving model ...
Time elapsed: 6.40 min
Epoch: 013/130 | Current Learning Rate: 0.100000
Epoch: 013/130 | Batch 0000/0313 | Loss: 0.6758
Epoch: 013/130 | Batch 0050/0313 | Loss: 0.6236
Epoch: 013/130 | Batch 0100/0313 | Loss: 0.6284
Epoch: 013/130 | Batch 0150/0313 | Loss: 0.6095
Epoch: 013/130 | Batch 0200/0313 | Loss: 0.6851
Epoch: 013/130 | Batch 0250/0313 | Loss: 0.9748
Epoch: 013/130 | Batch 0300/0313 | Loss: 0.7197
**Epoch: 013/130 | Train. Acc.: 75.157% | Loss: 0.7158
**Epoch: 013/130 | Valid. Acc.: 79.250% | Loss: 0.6120
**Validation loss decreased (0.642893 --> 0.612043). Saving model ...
Time elapsed: 6.92 min
Epoch: 014/130 | Current Learning Rate: 0.100000
Epoch: 014/130 | Batch 0000/0313 | Loss: 0.6050
Epoch: 014/130 | Batch 0050/0313 | Loss: 0.6027
Epoch: 014/130 | Batch 0100/0313 | Loss: 0.5275
Epoch: 014/130 | Batch 0150/0313 | Loss: 0.6465
Epoch: 014/130 | Batch 0200/0313 | Loss: 0.6333
Epoch: 014/130 | Batch 0250/0313 | Loss: 0.5692
Epoch: 014/130 | Batch 0300/0313 | Loss: 0.5168
**Epoch: 014/130 | Train. Acc.: 78.125% | Loss: 0.6361
**Epoch: 014/130 | Valid. Acc.: 80.330% | Loss: 0.5756
**Validation loss decreased (0.612043 --> 0.575638). Saving model ...
Time elapsed: 7.46 min
Epoch: 015/130 | Current Learning Rate: 0.100000
Epoch: 015/130 | Batch 0000/0313 | Loss: 0.6915
Epoch: 015/130 | Batch 0050/0313 | Loss: 0.6324
Epoch: 015/130 | Batch 0100/0313 | Loss: 0.5749
Epoch: 015/130 | Batch 0150/0313 | Loss: 0.7936
Epoch: 015/130 | Batch 0200/0313 | Loss: 0.5712
Epoch: 015/130 | Batch 0250/0313 | Loss: 0.6450
Epoch: 015/130 | Batch 0300/0313 | Loss: 0.6645
**Epoch: 015/130 | Train. Acc.: 77.873% | Loss: 0.6376
**Epoch: 015/130 | Valid. Acc.: 80.930% | Loss: 0.5617
**Validation loss decreased (0.575638 --> 0.561747). Saving model ...
Time elapsed: 7.99 min
Epoch: 016/130 | Current Learning Rate: 0.100000
Epoch: 016/130 | Batch 0000/0313 | Loss: 0.5317
Epoch: 016/130 | Batch 0050/0313 | Loss: 0.6810
Epoch: 016/130 | Batch 0100/0313 | Loss: 0.5680
Epoch: 016/130 | Batch 0150/0313 | Loss: 0.6398
Epoch: 016/130 | Batch 0200/0313 | Loss: 0.6275
Epoch: 016/130 | Batch 0250/0313 | Loss: 0.6660
Epoch: 016/130 | Batch 0300/0313 | Loss: 0.7744
**Epoch: 016/130 | Train. Acc.: 78.935% | Loss: 0.6082
**Epoch: 016/130 | Valid. Acc.: 81.730% | Loss: 0.5274
**Validation loss decreased (0.561747 --> 0.527405). Saving model ...
Time elapsed: 8.52 min
Epoch: 017/130 | Current Learning Rate: 0.100000
Epoch: 017/130 | Batch 0000/0313 | Loss: 0.6918
Epoch: 017/130 | Batch 0050/0313 | Loss: 0.4537
Epoch: 017/130 | Batch 0100/0313 | Loss: 0.6391
Epoch: 017/130 | Batch 0150/0313 | Loss: 0.4660
Epoch: 017/130 | Batch 0200/0313 | Loss: 0.5016
Epoch: 017/130 | Batch 0250/0313 | Loss: 0.5800
Epoch: 017/130 | Batch 0300/0313 | Loss: 0.5672
**Epoch: 017/130 | Train. Acc.: 76.915% | Loss: 0.6722
**Epoch: 017/130 | Valid. Acc.: 79.940% | Loss: 0.5941
Time elapsed: 9.04 min
Epoch: 018/130 | Current Learning Rate: 0.100000
Epoch: 018/130 | Batch 0000/0313 | Loss: 0.5256
Epoch: 018/130 | Batch 0050/0313 | Loss: 0.6710
Epoch: 018/130 | Batch 0100/0313 | Loss: 0.6174
Epoch: 018/130 | Batch 0150/0313 | Loss: 0.5743
Epoch: 018/130 | Batch 0200/0313 | Loss: 0.6439
Epoch: 018/130 | Batch 0250/0313 | Loss: 0.7268
Epoch: 018/130 | Batch 0300/0313 | Loss: 0.5195
**Epoch: 018/130 | Train. Acc.: 66.850% | Loss: 1.0058
**Epoch: 018/130 | Valid. Acc.: 71.340% | Loss: 0.9242
Time elapsed: 9.57 min
Epoch: 019/130 | Current Learning Rate: 0.100000
Epoch: 019/130 | Batch 0000/0313 | Loss: 0.5806
Epoch: 019/130 | Batch 0050/0313 | Loss: 0.5082
Epoch: 019/130 | Batch 0100/0313 | Loss: 0.5587
Epoch: 019/130 | Batch 0150/0313 | Loss: 0.6964
Epoch: 019/130 | Batch 0200/0313 | Loss: 0.6270
Epoch: 019/130 | Batch 0250/0313 | Loss: 0.5257
Epoch: 019/130 | Batch 0300/0313 | Loss: 0.6588
**Epoch: 019/130 | Train. Acc.: 78.135% | Loss: 0.6279
**Epoch: 019/130 | Valid. Acc.: 80.530% | Loss: 0.5733
Time elapsed: 10.09 min
Epoch: 020/130 | Current Learning Rate: 0.100000
Epoch: 020/130 | Batch 0000/0313 | Loss: 0.5352
Epoch: 020/130 | Batch 0050/0313 | Loss: 0.5088
Epoch: 020/130 | Batch 0100/0313 | Loss: 0.5697
Epoch: 020/130 | Batch 0150/0313 | Loss: 0.4903
Epoch: 020/130 | Batch 0200/0313 | Loss: 0.6102
Epoch: 020/130 | Batch 0250/0313 | Loss: 0.5871
Epoch: 020/130 | Batch 0300/0313 | Loss: 0.5837
**Epoch: 020/130 | Train. Acc.: 79.218% | Loss: 0.6038
**Epoch: 020/130 | Valid. Acc.: 81.970% | Loss: 0.5246
**Validation loss decreased (0.527405 --> 0.524589). Saving model ...
Time elapsed: 10.62 min
Epoch: 021/130 | Current Learning Rate: 0.100000
Epoch: 021/130 | Batch 0000/0313 | Loss: 0.4507
Epoch: 021/130 | Batch 0050/0313 | Loss: 0.5635
Epoch: 021/130 | Batch 0100/0313 | Loss: 0.5671
Epoch: 021/130 | Batch 0150/0313 | Loss: 0.7388
Epoch: 021/130 | Batch 0200/0313 | Loss: 0.4734
Epoch: 021/130 | Batch 0250/0313 | Loss: 0.6430
Epoch: 021/130 | Batch 0300/0313 | Loss: 0.5716
**Epoch: 021/130 | Train. Acc.: 78.088% | Loss: 0.6513
**Epoch: 021/130 | Valid. Acc.: 81.590% | Loss: 0.5541
Time elapsed: 11.15 min
Epoch: 022/130 | Current Learning Rate: 0.100000
Epoch: 022/130 | Batch 0000/0313 | Loss: 0.6863
Epoch: 022/130 | Batch 0050/0313 | Loss: 0.5485
Epoch: 022/130 | Batch 0100/0313 | Loss: 0.6000
Epoch: 022/130 | Batch 0150/0313 | Loss: 0.5949
Epoch: 022/130 | Batch 0200/0313 | Loss: 0.5224
Epoch: 022/130 | Batch 0250/0313 | Loss: 0.6213
Epoch: 022/130 | Batch 0300/0313 | Loss: 0.6616
**Epoch: 022/130 | Train. Acc.: 77.650% | Loss: 0.6393
**Epoch: 022/130 | Valid. Acc.: 81.080% | Loss: 0.5513
Time elapsed: 11.68 min
Epoch: 023/130 | Current Learning Rate: 0.100000
Epoch: 023/130 | Batch 0000/0313 | Loss: 0.5687
Epoch: 023/130 | Batch 0050/0313 | Loss: 0.5160
Epoch: 023/130 | Batch 0100/0313 | Loss: 0.5347
Epoch: 023/130 | Batch 0150/0313 | Loss: 0.4997
Epoch: 023/130 | Batch 0200/0313 | Loss: 0.5265
Epoch: 023/130 | Batch 0250/0313 | Loss: 0.4340
Epoch: 023/130 | Batch 0300/0313 | Loss: 0.5398
**Epoch: 023/130 | Train. Acc.: 80.812% | Loss: 0.5549
**Epoch: 023/130 | Valid. Acc.: 81.810% | Loss: 0.5349
Time elapsed: 12.21 min
Epoch: 024/130 | Current Learning Rate: 0.100000
Epoch: 024/130 | Batch 0000/0313 | Loss: 0.4610
Epoch: 024/130 | Batch 0050/0313 | Loss: 0.6031
Epoch: 024/130 | Batch 0100/0313 | Loss: 0.4498
Epoch: 024/130 | Batch 0150/0313 | Loss: 0.5655
Epoch: 024/130 | Batch 0200/0313 | Loss: 0.4625
Epoch: 024/130 | Batch 0250/0313 | Loss: 0.4280
Epoch: 024/130 | Batch 0300/0313 | Loss: 0.5040
**Epoch: 024/130 | Train. Acc.: 77.817% | Loss: 0.6521
**Epoch: 024/130 | Valid. Acc.: 80.510% | Loss: 0.5941
Time elapsed: 12.74 min
Epoch: 025/130 | Current Learning Rate: 0.100000
Epoch: 025/130 | Batch 0000/0313 | Loss: 0.4942
Epoch: 025/130 | Batch 0050/0313 | Loss: 0.3401
Epoch: 025/130 | Batch 0100/0313 | Loss: 0.5343
Epoch: 025/130 | Batch 0150/0313 | Loss: 0.8555
Epoch: 025/130 | Batch 0200/0313 | Loss: 0.3888
Epoch: 025/130 | Batch 0250/0313 | Loss: 0.5466
Epoch: 025/130 | Batch 0300/0313 | Loss: 0.4503
**Epoch: 025/130 | Train. Acc.: 78.062% | Loss: 0.6485
**Epoch: 025/130 | Valid. Acc.: 81.160% | Loss: 0.5702
Time elapsed: 13.29 min
Epoch: 026/130 | Current Learning Rate: 0.100000
Epoch: 026/130 | Batch 0000/0313 | Loss: 0.4795
Epoch: 026/130 | Batch 0050/0313 | Loss: 0.5271
Epoch: 026/130 | Batch 0100/0313 | Loss: 0.5896
Epoch: 026/130 | Batch 0150/0313 | Loss: 0.4862
Epoch: 026/130 | Batch 0200/0313 | Loss: 0.6418
Epoch: 026/130 | Batch 0250/0313 | Loss: 0.5772
Epoch: 026/130 | Batch 0300/0313 | Loss: 0.6947
**Epoch: 026/130 | Train. Acc.: 77.465% | Loss: 0.6523
**Epoch: 026/130 | Valid. Acc.: 79.060% | Loss: 0.6070
Time elapsed: 13.82 min
Epoch: 027/130 | Current Learning Rate: 0.100000
Epoch: 027/130 | Batch 0000/0313 | Loss: 0.4486
Epoch: 027/130 | Batch 0050/0313 | Loss: 0.6800
Epoch: 027/130 | Batch 0100/0313 | Loss: 0.6722
Epoch: 027/130 | Batch 0150/0313 | Loss: 0.4558
Epoch: 027/130 | Batch 0200/0313 | Loss: 0.5953
Epoch: 027/130 | Batch 0250/0313 | Loss: 0.5511
Epoch: 027/130 | Batch 0300/0313 | Loss: 0.5972
**Epoch: 027/130 | Train. Acc.: 75.987% | Loss: 0.6758
**Epoch: 027/130 | Valid. Acc.: 78.360% | Loss: 0.6297
Epoch 00027: reducing learning rate of group 0 to 5.0000e-02.
Time elapsed: 14.34 min
Epoch: 028/130 | Current Learning Rate: 0.050000
Epoch: 028/130 | Batch 0000/0313 | Loss: 0.4934
Epoch: 028/130 | Batch 0050/0313 | Loss: 0.4748
Epoch: 028/130 | Batch 0100/0313 | Loss: 0.2902
Epoch: 028/130 | Batch 0150/0313 | Loss: 0.4011
Epoch: 028/130 | Batch 0200/0313 | Loss: 0.3088
Epoch: 028/130 | Batch 0250/0313 | Loss: 0.3671
Epoch: 028/130 | Batch 0300/0313 | Loss: 0.4438
**Epoch: 028/130 | Train. Acc.: 86.500% | Loss: 0.3884
**Epoch: 028/130 | Valid. Acc.: 87.140% | Loss: 0.3844
**Validation loss decreased (0.524589 --> 0.384425). Saving model ...
Time elapsed: 14.88 min
Epoch: 029/130 | Current Learning Rate: 0.050000
Epoch: 029/130 | Batch 0000/0313 | Loss: 0.4258
Epoch: 029/130 | Batch 0050/0313 | Loss: 0.3300
Epoch: 029/130 | Batch 0100/0313 | Loss: 0.2253
Epoch: 029/130 | Batch 0150/0313 | Loss: 0.4476
Epoch: 029/130 | Batch 0200/0313 | Loss: 0.4435
Epoch: 029/130 | Batch 0250/0313 | Loss: 0.3416
Epoch: 029/130 | Batch 0300/0313 | Loss: 0.5600
**Epoch: 029/130 | Train. Acc.: 85.980% | Loss: 0.4026
**Epoch: 029/130 | Valid. Acc.: 85.980% | Loss: 0.4149
Time elapsed: 15.41 min
Epoch: 030/130 | Current Learning Rate: 0.050000
Epoch: 030/130 | Batch 0000/0313 | Loss: 0.3717
Epoch: 030/130 | Batch 0050/0313 | Loss: 0.3693
Epoch: 030/130 | Batch 0100/0313 | Loss: 0.4332
Epoch: 030/130 | Batch 0150/0313 | Loss: 0.3100
Epoch: 030/130 | Batch 0200/0313 | Loss: 0.3502
Epoch: 030/130 | Batch 0250/0313 | Loss: 0.3464
Epoch: 030/130 | Batch 0300/0313 | Loss: 0.4190
**Epoch: 030/130 | Train. Acc.: 87.095% | Loss: 0.3700
**Epoch: 030/130 | Valid. Acc.: 87.010% | Loss: 0.3772
**Validation loss decreased (0.384425 --> 0.377168). Saving model ...
Time elapsed: 15.94 min
Epoch: 031/130 | Current Learning Rate: 0.050000
Epoch: 031/130 | Batch 0000/0313 | Loss: 0.3265
Epoch: 031/130 | Batch 0050/0313 | Loss: 0.4030
Epoch: 031/130 | Batch 0100/0313 | Loss: 0.4320
Epoch: 031/130 | Batch 0150/0313 | Loss: 0.4354
Epoch: 031/130 | Batch 0200/0313 | Loss: 0.3406
Epoch: 031/130 | Batch 0250/0313 | Loss: 0.3915
Epoch: 031/130 | Batch 0300/0313 | Loss: 0.4871
**Epoch: 031/130 | Train. Acc.: 84.315% | Loss: 0.4509
**Epoch: 031/130 | Valid. Acc.: 86.000% | Loss: 0.4248
Time elapsed: 16.47 min
Epoch: 032/130 | Current Learning Rate: 0.050000
Epoch: 032/130 | Batch 0000/0313 | Loss: 0.3071
Epoch: 032/130 | Batch 0050/0313 | Loss: 0.3729
Epoch: 032/130 | Batch 0100/0313 | Loss: 0.4235
Epoch: 032/130 | Batch 0150/0313 | Loss: 0.3677
Epoch: 032/130 | Batch 0200/0313 | Loss: 0.2837
Epoch: 032/130 | Batch 0250/0313 | Loss: 0.4898
Epoch: 032/130 | Batch 0300/0313 | Loss: 0.4515
**Epoch: 032/130 | Train. Acc.: 82.883% | Loss: 0.4912
**Epoch: 032/130 | Valid. Acc.: 83.320% | Loss: 0.5070
Time elapsed: 17.01 min
Epoch: 033/130 | Current Learning Rate: 0.050000
Epoch: 033/130 | Batch 0000/0313 | Loss: 0.3320
Epoch: 033/130 | Batch 0050/0313 | Loss: 0.3148
Epoch: 033/130 | Batch 0100/0313 | Loss: 0.3887
Epoch: 033/130 | Batch 0150/0313 | Loss: 0.2879
Epoch: 033/130 | Batch 0200/0313 | Loss: 0.5130
Epoch: 033/130 | Batch 0250/0313 | Loss: 0.2908
Epoch: 033/130 | Batch 0300/0313 | Loss: 0.3284
**Epoch: 033/130 | Train. Acc.: 87.587% | Loss: 0.3675
**Epoch: 033/130 | Valid. Acc.: 88.040% | Loss: 0.3637
**Validation loss decreased (0.377168 --> 0.363738). Saving model ...
Time elapsed: 17.54 min
Epoch: 034/130 | Current Learning Rate: 0.050000
Epoch: 034/130 | Batch 0000/0313 | Loss: 0.3686
Epoch: 034/130 | Batch 0050/0313 | Loss: 0.4093
Epoch: 034/130 | Batch 0100/0313 | Loss: 0.3112
Epoch: 034/130 | Batch 0150/0313 | Loss: 0.4047
Epoch: 034/130 | Batch 0200/0313 | Loss: 0.4330
Epoch: 034/130 | Batch 0250/0313 | Loss: 0.4961
Epoch: 034/130 | Batch 0300/0313 | Loss: 0.4475
**Epoch: 034/130 | Train. Acc.: 85.890% | Loss: 0.4059
**Epoch: 034/130 | Valid. Acc.: 86.840% | Loss: 0.3929
Time elapsed: 18.06 min
Epoch: 035/130 | Current Learning Rate: 0.050000
Epoch: 035/130 | Batch 0000/0313 | Loss: 0.3104
Epoch: 035/130 | Batch 0050/0313 | Loss: 0.5524
Epoch: 035/130 | Batch 0100/0313 | Loss: 0.3679
Epoch: 035/130 | Batch 0150/0313 | Loss: 0.1863
Epoch: 035/130 | Batch 0200/0313 | Loss: 0.3505
Epoch: 035/130 | Batch 0250/0313 | Loss: 0.4977
Epoch: 035/130 | Batch 0300/0313 | Loss: 0.4391
**Epoch: 035/130 | Train. Acc.: 86.213% | Loss: 0.3957
**Epoch: 035/130 | Valid. Acc.: 86.540% | Loss: 0.3979
Time elapsed: 18.59 min
Epoch: 036/130 | Current Learning Rate: 0.050000
Epoch: 036/130 | Batch 0000/0313 | Loss: 0.2970
Epoch: 036/130 | Batch 0050/0313 | Loss: 0.5253
Epoch: 036/130 | Batch 0100/0313 | Loss: 0.2435
Epoch: 036/130 | Batch 0150/0313 | Loss: 0.2733
Epoch: 036/130 | Batch 0200/0313 | Loss: 0.4846
Epoch: 036/130 | Batch 0250/0313 | Loss: 0.3373
Epoch: 036/130 | Batch 0300/0313 | Loss: 0.4015
**Epoch: 036/130 | Train. Acc.: 86.928% | Loss: 0.3792
**Epoch: 036/130 | Valid. Acc.: 87.250% | Loss: 0.3832
Time elapsed: 19.11 min
Epoch: 037/130 | Current Learning Rate: 0.050000
Epoch: 037/130 | Batch 0000/0313 | Loss: 0.3090
Epoch: 037/130 | Batch 0050/0313 | Loss: 0.2073
Epoch: 037/130 | Batch 0100/0313 | Loss: 0.4293
Epoch: 037/130 | Batch 0150/0313 | Loss: 0.3243
Epoch: 037/130 | Batch 0200/0313 | Loss: 0.4198
Epoch: 037/130 | Batch 0250/0313 | Loss: 0.4492
Epoch: 037/130 | Batch 0300/0313 | Loss: 0.4772
**Epoch: 037/130 | Train. Acc.: 87.418% | Loss: 0.3638
**Epoch: 037/130 | Valid. Acc.: 87.930% | Loss: 0.3602
**Validation loss decreased (0.363738 --> 0.360152). Saving model ...
Time elapsed: 19.65 min
Epoch: 038/130 | Current Learning Rate: 0.050000
Epoch: 038/130 | Batch 0000/0313 | Loss: 0.2824
Epoch: 038/130 | Batch 0050/0313 | Loss: 0.3316
Epoch: 038/130 | Batch 0100/0313 | Loss: 0.4756
Epoch: 038/130 | Batch 0150/0313 | Loss: 0.5728
Epoch: 038/130 | Batch 0200/0313 | Loss: 0.3340
Epoch: 038/130 | Batch 0250/0313 | Loss: 0.4600
Epoch: 038/130 | Batch 0300/0313 | Loss: 0.3018
**Epoch: 038/130 | Train. Acc.: 86.100% | Loss: 0.3981
**Epoch: 038/130 | Valid. Acc.: 86.440% | Loss: 0.4015
Time elapsed: 20.18 min
Epoch: 039/130 | Current Learning Rate: 0.050000
Epoch: 039/130 | Batch 0000/0313 | Loss: 0.3595
Epoch: 039/130 | Batch 0050/0313 | Loss: 0.3548
Epoch: 039/130 | Batch 0100/0313 | Loss: 0.3478
Epoch: 039/130 | Batch 0150/0313 | Loss: 0.3986
Epoch: 039/130 | Batch 0200/0313 | Loss: 0.4743
Epoch: 039/130 | Batch 0250/0313 | Loss: 0.3186
Epoch: 039/130 | Batch 0300/0313 | Loss: 0.4364
**Epoch: 039/130 | Train. Acc.: 87.630% | Loss: 0.3574
**Epoch: 039/130 | Valid. Acc.: 88.070% | Loss: 0.3538
**Validation loss decreased (0.360152 --> 0.353779). Saving model ...
Time elapsed: 20.72 min
Epoch: 040/130 | Current Learning Rate: 0.050000
Epoch: 040/130 | Batch 0000/0313 | Loss: 0.3018
Epoch: 040/130 | Batch 0050/0313 | Loss: 0.2454
Epoch: 040/130 | Batch 0100/0313 | Loss: 0.3410
Epoch: 040/130 | Batch 0150/0313 | Loss: 0.3999
Epoch: 040/130 | Batch 0200/0313 | Loss: 0.4009
Epoch: 040/130 | Batch 0250/0313 | Loss: 0.3403
Epoch: 040/130 | Batch 0300/0313 | Loss: 0.4119
**Epoch: 040/130 | Train. Acc.: 85.343% | Loss: 0.4179
**Epoch: 040/130 | Valid. Acc.: 85.450% | Loss: 0.4348
Time elapsed: 21.26 min
Epoch: 041/130 | Current Learning Rate: 0.050000
Epoch: 041/130 | Batch 0000/0313 | Loss: 0.2832
Epoch: 041/130 | Batch 0050/0313 | Loss: 0.3337
Epoch: 041/130 | Batch 0100/0313 | Loss: 0.3766
Epoch: 041/130 | Batch 0150/0313 | Loss: 0.4190
Epoch: 041/130 | Batch 0200/0313 | Loss: 0.3145
Epoch: 041/130 | Batch 0250/0313 | Loss: 0.2798
Epoch: 041/130 | Batch 0300/0313 | Loss: 0.3515
**Epoch: 041/130 | Train. Acc.: 85.440% | Loss: 0.4189
**Epoch: 041/130 | Valid. Acc.: 86.290% | Loss: 0.4196
Time elapsed: 21.78 min
Epoch: 042/130 | Current Learning Rate: 0.050000
Epoch: 042/130 | Batch 0000/0313 | Loss: 0.2861
Epoch: 042/130 | Batch 0050/0313 | Loss: 0.3075
Epoch: 042/130 | Batch 0100/0313 | Loss: 0.2468
Epoch: 042/130 | Batch 0150/0313 | Loss: 0.2810
Epoch: 042/130 | Batch 0200/0313 | Loss: 0.2909
Epoch: 042/130 | Batch 0250/0313 | Loss: 0.4455
Epoch: 042/130 | Batch 0300/0313 | Loss: 0.4895
**Epoch: 042/130 | Train. Acc.: 84.757% | Loss: 0.4475
**Epoch: 042/130 | Valid. Acc.: 85.210% | Loss: 0.4519
Time elapsed: 22.32 min
Epoch: 043/130 | Current Learning Rate: 0.050000
Epoch: 043/130 | Batch 0000/0313 | Loss: 0.3095
Epoch: 043/130 | Batch 0050/0313 | Loss: 0.3088
Epoch: 043/130 | Batch 0100/0313 | Loss: 0.2896
Epoch: 043/130 | Batch 0150/0313 | Loss: 0.3385
Epoch: 043/130 | Batch 0200/0313 | Loss: 0.4351
Epoch: 043/130 | Batch 0250/0313 | Loss: 0.2666
Epoch: 043/130 | Batch 0300/0313 | Loss: 0.4034
**Epoch: 043/130 | Train. Acc.: 85.317% | Loss: 0.4232
**Epoch: 043/130 | Valid. Acc.: 85.150% | Loss: 0.4604
Time elapsed: 22.84 min
Epoch: 044/130 | Current Learning Rate: 0.050000
Epoch: 044/130 | Batch 0000/0313 | Loss: 0.3930
Epoch: 044/130 | Batch 0050/0313 | Loss: 0.2619
Epoch: 044/130 | Batch 0100/0313 | Loss: 0.3492
Epoch: 044/130 | Batch 0150/0313 | Loss: 0.3573
Epoch: 044/130 | Batch 0200/0313 | Loss: 0.4033
Epoch: 044/130 | Batch 0250/0313 | Loss: 0.2815
Epoch: 044/130 | Batch 0300/0313 | Loss: 0.3276
**Epoch: 044/130 | Train. Acc.: 86.850% | Loss: 0.3796
**Epoch: 044/130 | Valid. Acc.: 87.400% | Loss: 0.3763
Time elapsed: 23.38 min
Epoch: 045/130 | Current Learning Rate: 0.050000
Epoch: 045/130 | Batch 0000/0313 | Loss: 0.3159
Epoch: 045/130 | Batch 0050/0313 | Loss: 0.2904
Epoch: 045/130 | Batch 0100/0313 | Loss: 0.4257
Epoch: 045/130 | Batch 0150/0313 | Loss: 0.3458
Epoch: 045/130 | Batch 0200/0313 | Loss: 0.2247
Epoch: 045/130 | Batch 0250/0313 | Loss: 0.4186
Epoch: 045/130 | Batch 0300/0313 | Loss: 0.2146
**Epoch: 045/130 | Train. Acc.: 87.105% | Loss: 0.3769
**Epoch: 045/130 | Valid. Acc.: 86.800% | Loss: 0.3939
Time elapsed: 23.91 min
Epoch: 046/130 | Current Learning Rate: 0.050000
Epoch: 046/130 | Batch 0000/0313 | Loss: 0.3658
Epoch: 046/130 | Batch 0050/0313 | Loss: 0.2626
Epoch: 046/130 | Batch 0100/0313 | Loss: 0.3653
Epoch: 046/130 | Batch 0150/0313 | Loss: 0.2811
Epoch: 046/130 | Batch 0200/0313 | Loss: 0.3604
Epoch: 046/130 | Batch 0250/0313 | Loss: 0.3366
Epoch: 046/130 | Batch 0300/0313 | Loss: 0.3721
**Epoch: 046/130 | Train. Acc.: 88.420% | Loss: 0.3407
**Epoch: 046/130 | Valid. Acc.: 88.410% | Loss: 0.3533
**Validation loss decreased (0.353779 --> 0.353327). Saving model ...
Time elapsed: 24.45 min
Epoch: 047/130 | Current Learning Rate: 0.050000
Epoch: 047/130 | Batch 0000/0313 | Loss: 0.2433
Epoch: 047/130 | Batch 0050/0313 | Loss: 0.1744
Epoch: 047/130 | Batch 0100/0313 | Loss: 0.1795
Epoch: 047/130 | Batch 0150/0313 | Loss: 0.2997
Epoch: 047/130 | Batch 0200/0313 | Loss: 0.2983
Epoch: 047/130 | Batch 0250/0313 | Loss: 0.3139
Epoch: 047/130 | Batch 0300/0313 | Loss: 0.2711
**Epoch: 047/130 | Train. Acc.: 86.355% | Loss: 0.3867
**Epoch: 047/130 | Valid. Acc.: 86.910% | Loss: 0.3810
Time elapsed: 24.98 min
Epoch: 048/130 | Current Learning Rate: 0.050000
Epoch: 048/130 | Batch 0000/0313 | Loss: 0.2964
Epoch: 048/130 | Batch 0050/0313 | Loss: 0.2694
Epoch: 048/130 | Batch 0100/0313 | Loss: 0.2756
Epoch: 048/130 | Batch 0150/0313 | Loss: 0.2178
Epoch: 048/130 | Batch 0200/0313 | Loss: 0.3510
Epoch: 048/130 | Batch 0250/0313 | Loss: 0.2701
Epoch: 048/130 | Batch 0300/0313 | Loss: 0.4242
**Epoch: 048/130 | Train. Acc.: 86.803% | Loss: 0.3782
**Epoch: 048/130 | Valid. Acc.: 87.150% | Loss: 0.3886
Time elapsed: 25.51 min
Epoch: 049/130 | Current Learning Rate: 0.050000
Epoch: 049/130 | Batch 0000/0313 | Loss: 0.4137
Epoch: 049/130 | Batch 0050/0313 | Loss: 0.4105
Epoch: 049/130 | Batch 0100/0313 | Loss: 0.3652
Epoch: 049/130 | Batch 0150/0313 | Loss: 0.3015
Epoch: 049/130 | Batch 0200/0313 | Loss: 0.4432
Epoch: 049/130 | Batch 0250/0313 | Loss: 0.3414
Epoch: 049/130 | Batch 0300/0313 | Loss: 0.3563
**Epoch: 049/130 | Train. Acc.: 85.858% | Loss: 0.4016
**Epoch: 049/130 | Valid. Acc.: 86.640% | Loss: 0.4044
Time elapsed: 26.04 min
Epoch: 050/130 | Current Learning Rate: 0.050000
Epoch: 050/130 | Batch 0000/0313 | Loss: 0.3549
Epoch: 050/130 | Batch 0050/0313 | Loss: 0.3547
Epoch: 050/130 | Batch 0100/0313 | Loss: 0.3097
Epoch: 050/130 | Batch 0150/0313 | Loss: 0.2240
Epoch: 050/130 | Batch 0200/0313 | Loss: 0.3948
Epoch: 050/130 | Batch 0250/0313 | Loss: 0.3930
Epoch: 050/130 | Batch 0300/0313 | Loss: 0.3136
**Epoch: 050/130 | Train. Acc.: 84.770% | Loss: 0.4547
**Epoch: 050/130 | Valid. Acc.: 85.660% | Loss: 0.4488
Time elapsed: 26.57 min
Epoch: 051/130 | Current Learning Rate: 0.050000
Epoch: 051/130 | Batch 0000/0313 | Loss: 0.3249
Epoch: 051/130 | Batch 0050/0313 | Loss: 0.2147
Epoch: 051/130 | Batch 0100/0313 | Loss: 0.2225
Epoch: 051/130 | Batch 0150/0313 | Loss: 0.3071
Epoch: 051/130 | Batch 0200/0313 | Loss: 0.3096
Epoch: 051/130 | Batch 0250/0313 | Loss: 0.2501
Epoch: 051/130 | Batch 0300/0313 | Loss: 0.3379
**Epoch: 051/130 | Train. Acc.: 86.725% | Loss: 0.3857
**Epoch: 051/130 | Valid. Acc.: 87.320% | Loss: 0.3949
Time elapsed: 27.11 min
Epoch: 052/130 | Current Learning Rate: 0.050000
Epoch: 052/130 | Batch 0000/0313 | Loss: 0.2760
Epoch: 052/130 | Batch 0050/0313 | Loss: 0.3800
Epoch: 052/130 | Batch 0100/0313 | Loss: 0.3185
Epoch: 052/130 | Batch 0150/0313 | Loss: 0.3178
Epoch: 052/130 | Batch 0200/0313 | Loss: 0.3610
Epoch: 052/130 | Batch 0250/0313 | Loss: 0.3547
Epoch: 052/130 | Batch 0300/0313 | Loss: 0.1764
**Epoch: 052/130 | Train. Acc.: 87.195% | Loss: 0.3775
**Epoch: 052/130 | Valid. Acc.: 87.320% | Loss: 0.3901
Time elapsed: 27.63 min
Epoch: 053/130 | Current Learning Rate: 0.050000
Epoch: 053/130 | Batch 0000/0313 | Loss: 0.3252
Epoch: 053/130 | Batch 0050/0313 | Loss: 0.3206
Epoch: 053/130 | Batch 0100/0313 | Loss: 0.4446
Epoch: 053/130 | Batch 0150/0313 | Loss: 0.3124
Epoch: 053/130 | Batch 0200/0313 | Loss: 0.4394
Epoch: 053/130 | Batch 0250/0313 | Loss: 0.4681
Epoch: 053/130 | Batch 0300/0313 | Loss: 0.2931
**Epoch: 053/130 | Train. Acc.: 87.785% | Loss: 0.3452
**Epoch: 053/130 | Valid. Acc.: 87.910% | Loss: 0.3642
Epoch 00053: reducing learning rate of group 0 to 2.5000e-02.
Time elapsed: 28.16 min
Epoch: 054/130 | Current Learning Rate: 0.025000
Epoch: 054/130 | Batch 0000/0313 | Loss: 0.3434
Epoch: 054/130 | Batch 0050/0313 | Loss: 0.2518
Epoch: 054/130 | Batch 0100/0313 | Loss: 0.2491
Epoch: 054/130 | Batch 0150/0313 | Loss: 0.3514
Epoch: 054/130 | Batch 0200/0313 | Loss: 0.2834
Epoch: 054/130 | Batch 0250/0313 | Loss: 0.2210
Epoch: 054/130 | Batch 0300/0313 | Loss: 0.2594
**Epoch: 054/130 | Train. Acc.: 93.200% | Loss: 0.1970
**Epoch: 054/130 | Valid. Acc.: 91.180% | Loss: 0.2684
**Validation loss decreased (0.353327 --> 0.268441). Saving model ...
Time elapsed: 28.70 min
Epoch: 055/130 | Current Learning Rate: 0.025000
Epoch: 055/130 | Batch 0000/0313 | Loss: 0.2642
Epoch: 055/130 | Batch 0050/0313 | Loss: 0.2025
Epoch: 055/130 | Batch 0100/0313 | Loss: 0.1794
Epoch: 055/130 | Batch 0150/0313 | Loss: 0.1251
Epoch: 055/130 | Batch 0200/0313 | Loss: 0.1824
Epoch: 055/130 | Batch 0250/0313 | Loss: 0.1546
Epoch: 055/130 | Batch 0300/0313 | Loss: 0.2668
**Epoch: 055/130 | Train. Acc.: 93.293% | Loss: 0.1914
**Epoch: 055/130 | Valid. Acc.: 91.510% | Loss: 0.2709
Time elapsed: 29.23 min
Epoch: 056/130 | Current Learning Rate: 0.025000
Epoch: 056/130 | Batch 0000/0313 | Loss: 0.2228
Epoch: 056/130 | Batch 0050/0313 | Loss: 0.1937
Epoch: 056/130 | Batch 0100/0313 | Loss: 0.2162
Epoch: 056/130 | Batch 0150/0313 | Loss: 0.1464
Epoch: 056/130 | Batch 0200/0313 | Loss: 0.3150
Epoch: 056/130 | Batch 0250/0313 | Loss: 0.1863
Epoch: 056/130 | Batch 0300/0313 | Loss: 0.1825
**Epoch: 056/130 | Train. Acc.: 92.705% | Loss: 0.2116
**Epoch: 056/130 | Valid. Acc.: 90.530% | Loss: 0.2909
Time elapsed: 29.77 min
Epoch: 057/130 | Current Learning Rate: 0.025000
Epoch: 057/130 | Batch 0000/0313 | Loss: 0.1469
Epoch: 057/130 | Batch 0050/0313 | Loss: 0.1764
Epoch: 057/130 | Batch 0100/0313 | Loss: 0.1751
Epoch: 057/130 | Batch 0150/0313 | Loss: 0.0927
Epoch: 057/130 | Batch 0200/0313 | Loss: 0.2381
Epoch: 057/130 | Batch 0250/0313 | Loss: 0.1713
Epoch: 057/130 | Batch 0300/0313 | Loss: 0.1398
**Epoch: 057/130 | Train. Acc.: 92.970% | Loss: 0.2041
**Epoch: 057/130 | Valid. Acc.: 91.160% | Loss: 0.2827
Time elapsed: 30.30 min
Epoch: 058/130 | Current Learning Rate: 0.025000
Epoch: 058/130 | Batch 0000/0313 | Loss: 0.2106
Epoch: 058/130 | Batch 0050/0313 | Loss: 0.1563
Epoch: 058/130 | Batch 0100/0313 | Loss: 0.1803
Epoch: 058/130 | Batch 0150/0313 | Loss: 0.1166
Epoch: 058/130 | Batch 0200/0313 | Loss: 0.3442
Epoch: 058/130 | Batch 0250/0313 | Loss: 0.3142
Epoch: 058/130 | Batch 0300/0313 | Loss: 0.4286
**Epoch: 058/130 | Train. Acc.: 92.763% | Loss: 0.2090
**Epoch: 058/130 | Valid. Acc.: 90.300% | Loss: 0.3047
Time elapsed: 30.83 min
Epoch: 059/130 | Current Learning Rate: 0.025000
Epoch: 059/130 | Batch 0000/0313 | Loss: 0.1710
Epoch: 059/130 | Batch 0050/0313 | Loss: 0.3299
Epoch: 059/130 | Batch 0100/0313 | Loss: 0.1424
Epoch: 059/130 | Batch 0150/0313 | Loss: 0.2780
Epoch: 059/130 | Batch 0200/0313 | Loss: 0.1513
Epoch: 059/130 | Batch 0250/0313 | Loss: 0.1980
Epoch: 059/130 | Batch 0300/0313 | Loss: 0.1925
**Epoch: 059/130 | Train. Acc.: 93.468% | Loss: 0.1942
**Epoch: 059/130 | Valid. Acc.: 90.720% | Loss: 0.2920
Time elapsed: 31.36 min
Epoch: 060/130 | Current Learning Rate: 0.025000
Epoch: 060/130 | Batch 0000/0313 | Loss: 0.2824
Epoch: 060/130 | Batch 0050/0313 | Loss: 0.1078
Epoch: 060/130 | Batch 0100/0313 | Loss: 0.1392
Epoch: 060/130 | Batch 0150/0313 | Loss: 0.2133
Epoch: 060/130 | Batch 0200/0313 | Loss: 0.1267
Epoch: 060/130 | Batch 0250/0313 | Loss: 0.1189
Epoch: 060/130 | Batch 0300/0313 | Loss: 0.3526
**Epoch: 060/130 | Train. Acc.: 93.290% | Loss: 0.1974
**Epoch: 060/130 | Valid. Acc.: 90.060% | Loss: 0.3064
Time elapsed: 31.89 min
Epoch: 061/130 | Current Learning Rate: 0.025000
Epoch: 061/130 | Batch 0000/0313 | Loss: 0.1910
Epoch: 061/130 | Batch 0050/0313 | Loss: 0.1521
Epoch: 061/130 | Batch 0100/0313 | Loss: 0.1948
Epoch: 061/130 | Batch 0150/0313 | Loss: 0.2017
Epoch: 061/130 | Batch 0200/0313 | Loss: 0.2289
Epoch: 061/130 | Batch 0250/0313 | Loss: 0.2597
Epoch: 061/130 | Batch 0300/0313 | Loss: 0.1610
**Epoch: 061/130 | Train. Acc.: 91.163% | Loss: 0.2547
**Epoch: 061/130 | Valid. Acc.: 88.500% | Loss: 0.3693
Epoch 00061: reducing learning rate of group 0 to 1.2500e-02.
Time elapsed: 32.41 min
Epoch: 062/130 | Current Learning Rate: 0.012500
Epoch: 062/130 | Batch 0000/0313 | Loss: 0.1773
Epoch: 062/130 | Batch 0050/0313 | Loss: 0.1769
Epoch: 062/130 | Batch 0100/0313 | Loss: 0.1642
Epoch: 062/130 | Batch 0150/0313 | Loss: 0.1419
Epoch: 062/130 | Batch 0200/0313 | Loss: 0.0835
Epoch: 062/130 | Batch 0250/0313 | Loss: 0.1231
Epoch: 062/130 | Batch 0300/0313 | Loss: 0.1006
**Epoch: 062/130 | Train. Acc.: 95.390% | Loss: 0.1322
**Epoch: 062/130 | Valid. Acc.: 92.020% | Loss: 0.2593
**Validation loss decreased (0.268441 --> 0.259300). Saving model ...
Time elapsed: 32.94 min
Epoch: 063/130 | Current Learning Rate: 0.012500
Epoch: 063/130 | Batch 0000/0313 | Loss: 0.1056
Epoch: 063/130 | Batch 0050/0313 | Loss: 0.0881
Epoch: 063/130 | Batch 0100/0313 | Loss: 0.1624
Epoch: 063/130 | Batch 0150/0313 | Loss: 0.1197
Epoch: 063/130 | Batch 0200/0313 | Loss: 0.1028
Epoch: 063/130 | Batch 0250/0313 | Loss: 0.1107
Epoch: 063/130 | Batch 0300/0313 | Loss: 0.0837
**Epoch: 063/130 | Train. Acc.: 96.510% | Loss: 0.1030
**Epoch: 063/130 | Valid. Acc.: 92.800% | Loss: 0.2329
**Validation loss decreased (0.259300 --> 0.232909). Saving model ...
Time elapsed: 33.47 min
Epoch: 064/130 | Current Learning Rate: 0.012500
Epoch: 064/130 | Batch 0000/0313 | Loss: 0.0889
Epoch: 064/130 | Batch 0050/0313 | Loss: 0.1109
Epoch: 064/130 | Batch 0100/0313 | Loss: 0.1140
Epoch: 064/130 | Batch 0150/0313 | Loss: 0.0713
Epoch: 064/130 | Batch 0200/0313 | Loss: 0.0603
Epoch: 064/130 | Batch 0250/0313 | Loss: 0.0722
Epoch: 064/130 | Batch 0300/0313 | Loss: 0.1106
**Epoch: 064/130 | Train. Acc.: 96.303% | Loss: 0.1084
**Epoch: 064/130 | Valid. Acc.: 92.190% | Loss: 0.2548
Time elapsed: 33.99 min
Epoch: 065/130 | Current Learning Rate: 0.012500
Epoch: 065/130 | Batch 0000/0313 | Loss: 0.1039
Epoch: 065/130 | Batch 0050/0313 | Loss: 0.0894
Epoch: 065/130 | Batch 0100/0313 | Loss: 0.2191
Epoch: 065/130 | Batch 0150/0313 | Loss: 0.1248
Epoch: 065/130 | Batch 0200/0313 | Loss: 0.1610
Epoch: 065/130 | Batch 0250/0313 | Loss: 0.0643
Epoch: 065/130 | Batch 0300/0313 | Loss: 0.1447
**Epoch: 065/130 | Train. Acc.: 96.635% | Loss: 0.0979
**Epoch: 065/130 | Valid. Acc.: 92.290% | Loss: 0.2561
Time elapsed: 34.52 min
Epoch: 066/130 | Current Learning Rate: 0.012500
Epoch: 066/130 | Batch 0000/0313 | Loss: 0.1116
Epoch: 066/130 | Batch 0050/0313 | Loss: 0.0844
Epoch: 066/130 | Batch 0100/0313 | Loss: 0.0965
Epoch: 066/130 | Batch 0150/0313 | Loss: 0.1534
Epoch: 066/130 | Batch 0200/0313 | Loss: 0.0947
Epoch: 066/130 | Batch 0250/0313 | Loss: 0.1206
Epoch: 066/130 | Batch 0300/0313 | Loss: 0.1252
**Epoch: 066/130 | Train. Acc.: 96.608% | Loss: 0.1006
**Epoch: 066/130 | Valid. Acc.: 92.480% | Loss: 0.2444
Time elapsed: 35.04 min
Epoch: 067/130 | Current Learning Rate: 0.012500
Epoch: 067/130 | Batch 0000/0313 | Loss: 0.1521
Epoch: 067/130 | Batch 0050/0313 | Loss: 0.1017
Epoch: 067/130 | Batch 0100/0313 | Loss: 0.1012
Epoch: 067/130 | Batch 0150/0313 | Loss: 0.1752
Epoch: 067/130 | Batch 0200/0313 | Loss: 0.0671
Epoch: 067/130 | Batch 0250/0313 | Loss: 0.1107
Epoch: 067/130 | Batch 0300/0313 | Loss: 0.1701
**Epoch: 067/130 | Train. Acc.: 96.475% | Loss: 0.1003
**Epoch: 067/130 | Valid. Acc.: 91.940% | Loss: 0.2669
Time elapsed: 35.57 min
Epoch: 068/130 | Current Learning Rate: 0.012500
Epoch: 068/130 | Batch 0000/0313 | Loss: 0.1246
Epoch: 068/130 | Batch 0050/0313 | Loss: 0.1176
Epoch: 068/130 | Batch 0100/0313 | Loss: 0.1015
Epoch: 068/130 | Batch 0150/0313 | Loss: 0.1684
Epoch: 068/130 | Batch 0200/0313 | Loss: 0.1346
Epoch: 068/130 | Batch 0250/0313 | Loss: 0.0962
Epoch: 068/130 | Batch 0300/0313 | Loss: 0.1014
**Epoch: 068/130 | Train. Acc.: 96.455% | Loss: 0.1022
**Epoch: 068/130 | Valid. Acc.: 92.260% | Loss: 0.2579
Time elapsed: 36.10 min
Epoch: 069/130 | Current Learning Rate: 0.012500
Epoch: 069/130 | Batch 0000/0313 | Loss: 0.1123
Epoch: 069/130 | Batch 0050/0313 | Loss: 0.1572
Epoch: 069/130 | Batch 0100/0313 | Loss: 0.1451
Epoch: 069/130 | Batch 0150/0313 | Loss: 0.1579
Epoch: 069/130 | Batch 0200/0313 | Loss: 0.0800
Epoch: 069/130 | Batch 0250/0313 | Loss: 0.1102
Epoch: 069/130 | Batch 0300/0313 | Loss: 0.1107
**Epoch: 069/130 | Train. Acc.: 96.317% | Loss: 0.1074
**Epoch: 069/130 | Valid. Acc.: 91.940% | Loss: 0.2705
Time elapsed: 36.64 min
Epoch: 070/130 | Current Learning Rate: 0.012500
Epoch: 070/130 | Batch 0000/0313 | Loss: 0.0616
Epoch: 070/130 | Batch 0050/0313 | Loss: 0.1180
Epoch: 070/130 | Batch 0100/0313 | Loss: 0.0717
Epoch: 070/130 | Batch 0150/0313 | Loss: 0.1263
Epoch: 070/130 | Batch 0200/0313 | Loss: 0.0790
Epoch: 070/130 | Batch 0250/0313 | Loss: 0.0873
Epoch: 070/130 | Batch 0300/0313 | Loss: 0.1562
**Epoch: 070/130 | Train. Acc.: 96.312% | Loss: 0.1063
**Epoch: 070/130 | Valid. Acc.: 91.640% | Loss: 0.2808
Epoch 00070: reducing learning rate of group 0 to 6.2500e-03.
Time elapsed: 37.17 min
Epoch: 071/130 | Current Learning Rate: 0.006250
Epoch: 071/130 | Batch 0000/0313 | Loss: 0.0722
Epoch: 071/130 | Batch 0050/0313 | Loss: 0.0681
Epoch: 071/130 | Batch 0100/0313 | Loss: 0.0715
Epoch: 071/130 | Batch 0150/0313 | Loss: 0.1512
Epoch: 071/130 | Batch 0200/0313 | Loss: 0.0296
Epoch: 071/130 | Batch 0250/0313 | Loss: 0.1038
Epoch: 071/130 | Batch 0300/0313 | Loss: 0.0598
**Epoch: 071/130 | Train. Acc.: 98.382% | Loss: 0.0528
**Epoch: 071/130 | Valid. Acc.: 93.310% | Loss: 0.2231
**Validation loss decreased (0.232909 --> 0.223147). Saving model ...
Time elapsed: 37.70 min
Epoch: 072/130 | Current Learning Rate: 0.006250
Epoch: 072/130 | Batch 0000/0313 | Loss: 0.0473
Epoch: 072/130 | Batch 0050/0313 | Loss: 0.0524
Epoch: 072/130 | Batch 0100/0313 | Loss: 0.1165
Epoch: 072/130 | Batch 0150/0313 | Loss: 0.0929
Epoch: 072/130 | Batch 0200/0313 | Loss: 0.0544
Epoch: 072/130 | Batch 0250/0313 | Loss: 0.0419
Epoch: 072/130 | Batch 0300/0313 | Loss: 0.0512
**Epoch: 072/130 | Train. Acc.: 98.282% | Loss: 0.0516
**Epoch: 072/130 | Valid. Acc.: 93.470% | Loss: 0.2235
Time elapsed: 38.23 min
Epoch: 073/130 | Current Learning Rate: 0.006250
Epoch: 073/130 | Batch 0000/0313 | Loss: 0.0179
Epoch: 073/130 | Batch 0050/0313 | Loss: 0.0498
Epoch: 073/130 | Batch 0100/0313 | Loss: 0.0125
Epoch: 073/130 | Batch 0150/0313 | Loss: 0.0521
Epoch: 073/130 | Batch 0200/0313 | Loss: 0.0594
Epoch: 073/130 | Batch 0250/0313 | Loss: 0.0640
Epoch: 073/130 | Batch 0300/0313 | Loss: 0.0315
**Epoch: 073/130 | Train. Acc.: 98.440% | Loss: 0.0489
**Epoch: 073/130 | Valid. Acc.: 93.210% | Loss: 0.2339
Time elapsed: 38.76 min
Epoch: 074/130 | Current Learning Rate: 0.006250
Epoch: 074/130 | Batch 0000/0313 | Loss: 0.0927
Epoch: 074/130 | Batch 0050/0313 | Loss: 0.0383
Epoch: 074/130 | Batch 0100/0313 | Loss: 0.0225
Epoch: 074/130 | Batch 0150/0313 | Loss: 0.0602
Epoch: 074/130 | Batch 0200/0313 | Loss: 0.0350
Epoch: 074/130 | Batch 0250/0313 | Loss: 0.1454
Epoch: 074/130 | Batch 0300/0313 | Loss: 0.0709
**Epoch: 074/130 | Train. Acc.: 98.303% | Loss: 0.0528
**Epoch: 074/130 | Valid. Acc.: 92.910% | Loss: 0.2508
Time elapsed: 39.29 min
Epoch: 075/130 | Current Learning Rate: 0.006250
Epoch: 075/130 | Batch 0000/0313 | Loss: 0.0402
Epoch: 075/130 | Batch 0050/0313 | Loss: 0.0799
Epoch: 075/130 | Batch 0100/0313 | Loss: 0.0432
Epoch: 075/130 | Batch 0150/0313 | Loss: 0.0349
Epoch: 075/130 | Batch 0200/0313 | Loss: 0.0703
Epoch: 075/130 | Batch 0250/0313 | Loss: 0.0641
Epoch: 075/130 | Batch 0300/0313 | Loss: 0.0278
**Epoch: 075/130 | Train. Acc.: 98.550% | Loss: 0.0443
**Epoch: 075/130 | Valid. Acc.: 93.060% | Loss: 0.2465
Time elapsed: 39.82 min
Epoch: 076/130 | Current Learning Rate: 0.006250
Epoch: 076/130 | Batch 0000/0313 | Loss: 0.0233
Epoch: 076/130 | Batch 0050/0313 | Loss: 0.0427
Epoch: 076/130 | Batch 0100/0313 | Loss: 0.0636
Epoch: 076/130 | Batch 0150/0313 | Loss: 0.0918
Epoch: 076/130 | Batch 0200/0313 | Loss: 0.0351
Epoch: 076/130 | Batch 0250/0313 | Loss: 0.0362
Epoch: 076/130 | Batch 0300/0313 | Loss: 0.0851
**Epoch: 076/130 | Train. Acc.: 98.445% | Loss: 0.0475
**Epoch: 076/130 | Valid. Acc.: 92.820% | Loss: 0.2460
Time elapsed: 40.34 min
Epoch: 077/130 | Current Learning Rate: 0.006250
Epoch: 077/130 | Batch 0000/0313 | Loss: 0.0180
Epoch: 077/130 | Batch 0050/0313 | Loss: 0.0472
Epoch: 077/130 | Batch 0100/0313 | Loss: 0.0375
Epoch: 077/130 | Batch 0150/0313 | Loss: 0.0178
Epoch: 077/130 | Batch 0200/0313 | Loss: 0.0295
Epoch: 077/130 | Batch 0250/0313 | Loss: 0.0441
Epoch: 077/130 | Batch 0300/0313 | Loss: 0.0370
**Epoch: 077/130 | Train. Acc.: 98.808% | Loss: 0.0382
**Epoch: 077/130 | Valid. Acc.: 93.560% | Loss: 0.2312
Time elapsed: 40.86 min
Epoch: 078/130 | Current Learning Rate: 0.006250
Epoch: 078/130 | Batch 0000/0313 | Loss: 0.0439
Epoch: 078/130 | Batch 0050/0313 | Loss: 0.0628
Epoch: 078/130 | Batch 0100/0313 | Loss: 0.0522
Epoch: 078/130 | Batch 0150/0313 | Loss: 0.0413
Epoch: 078/130 | Batch 0200/0313 | Loss: 0.0256
Epoch: 078/130 | Batch 0250/0313 | Loss: 0.0597
Epoch: 078/130 | Batch 0300/0313 | Loss: 0.0782
**Epoch: 078/130 | Train. Acc.: 98.580% | Loss: 0.0448
**Epoch: 078/130 | Valid. Acc.: 93.250% | Loss: 0.2434
Epoch 00078: reducing learning rate of group 0 to 3.1250e-03.
Time elapsed: 41.38 min
Epoch: 079/130 | Current Learning Rate: 0.003125
Epoch: 079/130 | Batch 0000/0313 | Loss: 0.0720
Epoch: 079/130 | Batch 0050/0313 | Loss: 0.0636
Epoch: 079/130 | Batch 0100/0313 | Loss: 0.0170
Epoch: 079/130 | Batch 0150/0313 | Loss: 0.0441
Epoch: 079/130 | Batch 0200/0313 | Loss: 0.0339
Epoch: 079/130 | Batch 0250/0313 | Loss: 0.0181
Epoch: 079/130 | Batch 0300/0313 | Loss: 0.0505
**Epoch: 079/130 | Train. Acc.: 99.115% | Loss: 0.0287
**Epoch: 079/130 | Valid. Acc.: 93.760% | Loss: 0.2214
**Validation loss decreased (0.223147 --> 0.221398). Saving model ...
Time elapsed: 41.91 min
Epoch: 080/130 | Current Learning Rate: 0.003125
Epoch: 080/130 | Batch 0000/0313 | Loss: 0.0230
Epoch: 080/130 | Batch 0050/0313 | Loss: 0.0167
Epoch: 080/130 | Batch 0100/0313 | Loss: 0.0363
Epoch: 080/130 | Batch 0150/0313 | Loss: 0.0260
Epoch: 080/130 | Batch 0200/0313 | Loss: 0.0067
Epoch: 080/130 | Batch 0250/0313 | Loss: 0.0209
Epoch: 080/130 | Batch 0300/0313 | Loss: 0.0933
**Epoch: 080/130 | Train. Acc.: 99.160% | Loss: 0.0284
**Epoch: 080/130 | Valid. Acc.: 93.380% | Loss: 0.2257
Time elapsed: 42.45 min
Epoch: 081/130 | Current Learning Rate: 0.003125
Epoch: 081/130 | Batch 0000/0313 | Loss: 0.0252
Epoch: 081/130 | Batch 0050/0313 | Loss: 0.0511
Epoch: 081/130 | Batch 0100/0313 | Loss: 0.0394
Epoch: 081/130 | Batch 0150/0313 | Loss: 0.0281
Epoch: 081/130 | Batch 0200/0313 | Loss: 0.0153
Epoch: 081/130 | Batch 0250/0313 | Loss: 0.0334
Epoch: 081/130 | Batch 0300/0313 | Loss: 0.0321
**Epoch: 081/130 | Train. Acc.: 99.252% | Loss: 0.0243
**Epoch: 081/130 | Valid. Acc.: 93.870% | Loss: 0.2202
**Validation loss decreased (0.221398 --> 0.220167). Saving model ...
Time elapsed: 42.98 min
Epoch: 082/130 | Current Learning Rate: 0.003125
Epoch: 082/130 | Batch 0000/0313 | Loss: 0.0218
Epoch: 082/130 | Batch 0050/0313 | Loss: 0.0342
Epoch: 082/130 | Batch 0100/0313 | Loss: 0.0226
Epoch: 082/130 | Batch 0150/0313 | Loss: 0.0124
Epoch: 082/130 | Batch 0200/0313 | Loss: 0.0361
Epoch: 082/130 | Batch 0250/0313 | Loss: 0.0353
Epoch: 082/130 | Batch 0300/0313 | Loss: 0.0582
**Epoch: 082/130 | Train. Acc.: 99.218% | Loss: 0.0252
**Epoch: 082/130 | Valid. Acc.: 93.700% | Loss: 0.2256
Time elapsed: 43.51 min
Epoch: 083/130 | Current Learning Rate: 0.003125
Epoch: 083/130 | Batch 0000/0313 | Loss: 0.0306
Epoch: 083/130 | Batch 0050/0313 | Loss: 0.0636
Epoch: 083/130 | Batch 0100/0313 | Loss: 0.0421
Epoch: 083/130 | Batch 0150/0313 | Loss: 0.0560
Epoch: 083/130 | Batch 0200/0313 | Loss: 0.0364
Epoch: 083/130 | Batch 0250/0313 | Loss: 0.0256
Epoch: 083/130 | Batch 0300/0313 | Loss: 0.0106
**Epoch: 083/130 | Train. Acc.: 99.358% | Loss: 0.0221
**Epoch: 083/130 | Valid. Acc.: 93.560% | Loss: 0.2295
Time elapsed: 44.04 min
Epoch: 084/130 | Current Learning Rate: 0.003125
Epoch: 084/130 | Batch 0000/0313 | Loss: 0.0254
Epoch: 084/130 | Batch 0050/0313 | Loss: 0.0122
Epoch: 084/130 | Batch 0100/0313 | Loss: 0.0141
Epoch: 084/130 | Batch 0150/0313 | Loss: 0.0079
Epoch: 084/130 | Batch 0200/0313 | Loss: 0.0387
Epoch: 084/130 | Batch 0250/0313 | Loss: 0.0076
Epoch: 084/130 | Batch 0300/0313 | Loss: 0.0114
**Epoch: 084/130 | Train. Acc.: 99.398% | Loss: 0.0209
**Epoch: 084/130 | Valid. Acc.: 93.820% | Loss: 0.2269
Time elapsed: 44.58 min
Epoch: 085/130 | Current Learning Rate: 0.003125
Epoch: 085/130 | Batch 0000/0313 | Loss: 0.0084
Epoch: 085/130 | Batch 0050/0313 | Loss: 0.0616
Epoch: 085/130 | Batch 0100/0313 | Loss: 0.0186
Epoch: 085/130 | Batch 0150/0313 | Loss: 0.0159
Epoch: 085/130 | Batch 0200/0313 | Loss: 0.0554
Epoch: 085/130 | Batch 0250/0313 | Loss: 0.0115
Epoch: 085/130 | Batch 0300/0313 | Loss: 0.0102
**Epoch: 085/130 | Train. Acc.: 99.272% | Loss: 0.0248
**Epoch: 085/130 | Valid. Acc.: 93.750% | Loss: 0.2309
Time elapsed: 45.11 min
Epoch: 086/130 | Current Learning Rate: 0.003125
Epoch: 086/130 | Batch 0000/0313 | Loss: 0.0179
Epoch: 086/130 | Batch 0050/0313 | Loss: 0.0237
Epoch: 086/130 | Batch 0100/0313 | Loss: 0.0156
Epoch: 086/130 | Batch 0150/0313 | Loss: 0.0446
Epoch: 086/130 | Batch 0200/0313 | Loss: 0.0320
Epoch: 086/130 | Batch 0250/0313 | Loss: 0.0059
Epoch: 086/130 | Batch 0300/0313 | Loss: 0.0181
**Epoch: 086/130 | Train. Acc.: 99.343% | Loss: 0.0224
**Epoch: 086/130 | Valid. Acc.: 93.680% | Loss: 0.2355
Time elapsed: 45.63 min
Epoch: 087/130 | Current Learning Rate: 0.003125
Epoch: 087/130 | Batch 0000/0313 | Loss: 0.0277
Epoch: 087/130 | Batch 0050/0313 | Loss: 0.0197
Epoch: 087/130 | Batch 0100/0313 | Loss: 0.0288
Epoch: 087/130 | Batch 0150/0313 | Loss: 0.0353
Epoch: 087/130 | Batch 0200/0313 | Loss: 0.0122
Epoch: 087/130 | Batch 0250/0313 | Loss: 0.0092
Epoch: 087/130 | Batch 0300/0313 | Loss: 0.0244
**Epoch: 087/130 | Train. Acc.: 99.460% | Loss: 0.0207
**Epoch: 087/130 | Valid. Acc.: 93.770% | Loss: 0.2262
Time elapsed: 46.16 min
Epoch: 088/130 | Current Learning Rate: 0.003125
Epoch: 088/130 | Batch 0000/0313 | Loss: 0.0297
Epoch: 088/130 | Batch 0050/0313 | Loss: 0.0267
Epoch: 088/130 | Batch 0100/0313 | Loss: 0.0346
Epoch: 088/130 | Batch 0150/0313 | Loss: 0.0397
Epoch: 088/130 | Batch 0200/0313 | Loss: 0.0496
Epoch: 088/130 | Batch 0250/0313 | Loss: 0.0178
Epoch: 088/130 | Batch 0300/0313 | Loss: 0.0436
**Epoch: 088/130 | Train. Acc.: 99.352% | Loss: 0.0223
**Epoch: 088/130 | Valid. Acc.: 93.280% | Loss: 0.2398
Epoch 00088: reducing learning rate of group 0 to 1.5625e-03.
Time elapsed: 46.70 min
Epoch: 089/130 | Current Learning Rate: 0.001563
Epoch: 089/130 | Batch 0000/0313 | Loss: 0.0912
Epoch: 089/130 | Batch 0050/0313 | Loss: 0.0110
Epoch: 089/130 | Batch 0100/0313 | Loss: 0.0033
Epoch: 089/130 | Batch 0150/0313 | Loss: 0.0317
Epoch: 089/130 | Batch 0200/0313 | Loss: 0.0185
Epoch: 089/130 | Batch 0250/0313 | Loss: 0.0324
Epoch: 089/130 | Batch 0300/0313 | Loss: 0.0257
**Epoch: 089/130 | Train. Acc.: 99.603% | Loss: 0.0155
**Epoch: 089/130 | Valid. Acc.: 93.890% | Loss: 0.2233
Time elapsed: 47.23 min
Epoch: 090/130 | Current Learning Rate: 0.001563
Epoch: 090/130 | Batch 0000/0313 | Loss: 0.0431
Epoch: 090/130 | Batch 0050/0313 | Loss: 0.0220
Epoch: 090/130 | Batch 0100/0313 | Loss: 0.0048
Epoch: 090/130 | Batch 0150/0313 | Loss: 0.0111
Epoch: 090/130 | Batch 0200/0313 | Loss: 0.0164
Epoch: 090/130 | Batch 0250/0313 | Loss: 0.0100
Epoch: 090/130 | Batch 0300/0313 | Loss: 0.0229
**Epoch: 090/130 | Train. Acc.: 99.555% | Loss: 0.0161
**Epoch: 090/130 | Valid. Acc.: 93.860% | Loss: 0.2245
Time elapsed: 47.75 min
Epoch: 091/130 | Current Learning Rate: 0.001563
Epoch: 091/130 | Batch 0000/0313 | Loss: 0.0229
Epoch: 091/130 | Batch 0050/0313 | Loss: 0.0080
Epoch: 091/130 | Batch 0100/0313 | Loss: 0.0123
Epoch: 091/130 | Batch 0150/0313 | Loss: 0.0242
Epoch: 091/130 | Batch 0200/0313 | Loss: 0.0174
Epoch: 091/130 | Batch 0250/0313 | Loss: 0.0408
Epoch: 091/130 | Batch 0300/0313 | Loss: 0.0418
**Epoch: 091/130 | Train. Acc.: 99.595% | Loss: 0.0146
**Epoch: 091/130 | Valid. Acc.: 94.060% | Loss: 0.2239
Time elapsed: 48.28 min
Epoch: 092/130 | Current Learning Rate: 0.001563
Epoch: 092/130 | Batch 0000/0313 | Loss: 0.0387
Epoch: 092/130 | Batch 0050/0313 | Loss: 0.0664
Epoch: 092/130 | Batch 0100/0313 | Loss: 0.0148
Epoch: 092/130 | Batch 0150/0313 | Loss: 0.0270
Epoch: 092/130 | Batch 0200/0313 | Loss: 0.0200
Epoch: 092/130 | Batch 0250/0313 | Loss: 0.0082
Epoch: 092/130 | Batch 0300/0313 | Loss: 0.0117
**Epoch: 092/130 | Train. Acc.: 99.598% | Loss: 0.0151
**Epoch: 092/130 | Valid. Acc.: 93.890% | Loss: 0.2247
Time elapsed: 48.81 min
Epoch: 093/130 | Current Learning Rate: 0.001563
Epoch: 093/130 | Batch 0000/0313 | Loss: 0.0252
Epoch: 093/130 | Batch 0050/0313 | Loss: 0.0078
Epoch: 093/130 | Batch 0100/0313 | Loss: 0.0058
Epoch: 093/130 | Batch 0150/0313 | Loss: 0.0187
Epoch: 093/130 | Batch 0200/0313 | Loss: 0.0191
Epoch: 093/130 | Batch 0250/0313 | Loss: 0.0194
Epoch: 093/130 | Batch 0300/0313 | Loss: 0.0297
**Epoch: 093/130 | Train. Acc.: 99.560% | Loss: 0.0152
**Epoch: 093/130 | Valid. Acc.: 93.970% | Loss: 0.2254
Time elapsed: 49.34 min
Epoch: 094/130 | Current Learning Rate: 0.001563
Epoch: 094/130 | Batch 0000/0313 | Loss: 0.0205
Epoch: 094/130 | Batch 0050/0313 | Loss: 0.0129
Epoch: 094/130 | Batch 0100/0313 | Loss: 0.0048
Epoch: 094/130 | Batch 0150/0313 | Loss: 0.0176
Epoch: 094/130 | Batch 0200/0313 | Loss: 0.0100
Epoch: 094/130 | Batch 0250/0313 | Loss: 0.0054
Epoch: 094/130 | Batch 0300/0313 | Loss: 0.0153
**Epoch: 094/130 | Train. Acc.: 99.630% | Loss: 0.0135
**Epoch: 094/130 | Valid. Acc.: 94.000% | Loss: 0.2265
Time elapsed: 49.87 min
Epoch: 095/130 | Current Learning Rate: 0.001563
Epoch: 095/130 | Batch 0000/0313 | Loss: 0.0133
Epoch: 095/130 | Batch 0050/0313 | Loss: 0.0084
Epoch: 095/130 | Batch 0100/0313 | Loss: 0.0173
Epoch: 095/130 | Batch 0150/0313 | Loss: 0.0083
Epoch: 095/130 | Batch 0200/0313 | Loss: 0.0249
Epoch: 095/130 | Batch 0250/0313 | Loss: 0.0090
Epoch: 095/130 | Batch 0300/0313 | Loss: 0.0077
**Epoch: 095/130 | Train. Acc.: 99.700% | Loss: 0.0123
**Epoch: 095/130 | Valid. Acc.: 94.020% | Loss: 0.2239
Epoch 00095: reducing learning rate of group 0 to 7.8125e-04.
Time elapsed: 50.39 min
Epoch: 096/130 | Current Learning Rate: 0.000781
Epoch: 096/130 | Batch 0000/0313 | Loss: 0.0096
Epoch: 096/130 | Batch 0050/0313 | Loss: 0.0057
Epoch: 096/130 | Batch 0100/0313 | Loss: 0.0082
Epoch: 096/130 | Batch 0150/0313 | Loss: 0.0094
Epoch: 096/130 | Batch 0200/0313 | Loss: 0.0110
Epoch: 096/130 | Batch 0250/0313 | Loss: 0.0087
Epoch: 096/130 | Batch 0300/0313 | Loss: 0.0039
**Epoch: 096/130 | Train. Acc.: 99.650% | Loss: 0.0132
**Epoch: 096/130 | Valid. Acc.: 93.970% | Loss: 0.2230
Time elapsed: 50.91 min
Epoch: 097/130 | Current Learning Rate: 0.000781
Epoch: 097/130 | Batch 0000/0313 | Loss: 0.0098
Epoch: 097/130 | Batch 0050/0313 | Loss: 0.0094
Epoch: 097/130 | Batch 0100/0313 | Loss: 0.0288
Epoch: 097/130 | Batch 0150/0313 | Loss: 0.0257
Epoch: 097/130 | Batch 0200/0313 | Loss: 0.0259
Epoch: 097/130 | Batch 0250/0313 | Loss: 0.0189
Epoch: 097/130 | Batch 0300/0313 | Loss: 0.0178
**Epoch: 097/130 | Train. Acc.: 99.692% | Loss: 0.0124
**Epoch: 097/130 | Valid. Acc.: 94.080% | Loss: 0.2240
Time elapsed: 51.45 min
Epoch: 098/130 | Current Learning Rate: 0.000781
Epoch: 098/130 | Batch 0000/0313 | Loss: 0.0064
Epoch: 098/130 | Batch 0050/0313 | Loss: 0.0098
Epoch: 098/130 | Batch 0100/0313 | Loss: 0.0270
Epoch: 098/130 | Batch 0150/0313 | Loss: 0.0124
Epoch: 098/130 | Batch 0200/0313 | Loss: 0.0109
Epoch: 098/130 | Batch 0250/0313 | Loss: 0.0136
Epoch: 098/130 | Batch 0300/0313 | Loss: 0.0145
**Epoch: 098/130 | Train. Acc.: 99.703% | Loss: 0.0117
**Epoch: 098/130 | Valid. Acc.: 93.960% | Loss: 0.2226
Time elapsed: 51.98 min
Epoch: 099/130 | Current Learning Rate: 0.000781
Epoch: 099/130 | Batch 0000/0313 | Loss: 0.0310
Epoch: 099/130 | Batch 0050/0313 | Loss: 0.0116
Epoch: 099/130 | Batch 0100/0313 | Loss: 0.0451
Epoch: 099/130 | Batch 0150/0313 | Loss: 0.0099
Epoch: 099/130 | Batch 0200/0313 | Loss: 0.0090
Epoch: 099/130 | Batch 0250/0313 | Loss: 0.0094
Epoch: 099/130 | Batch 0300/0313 | Loss: 0.0179
**Epoch: 099/130 | Train. Acc.: 99.640% | Loss: 0.0127
**Epoch: 099/130 | Valid. Acc.: 94.090% | Loss: 0.2199
**Validation loss decreased (0.220167 --> 0.219909). Saving model ...
Time elapsed: 52.51 min
Epoch: 100/130 | Current Learning Rate: 0.000781
Epoch: 100/130 | Batch 0000/0313 | Loss: 0.0126
Epoch: 100/130 | Batch 0050/0313 | Loss: 0.0127
Epoch: 100/130 | Batch 0100/0313 | Loss: 0.0207
Epoch: 100/130 | Batch 0150/0313 | Loss: 0.0052
Epoch: 100/130 | Batch 0200/0313 | Loss: 0.0098
Epoch: 100/130 | Batch 0250/0313 | Loss: 0.0159
Epoch: 100/130 | Batch 0300/0313 | Loss: 0.0161
**Epoch: 100/130 | Train. Acc.: 99.713% | Loss: 0.0119
**Epoch: 100/130 | Valid. Acc.: 93.960% | Loss: 0.2197
**Validation loss decreased (0.219909 --> 0.219673). Saving model ...
Time elapsed: 53.04 min
Epoch: 101/130 | Current Learning Rate: 0.000781
Epoch: 101/130 | Batch 0000/0313 | Loss: 0.0088
Epoch: 101/130 | Batch 0050/0313 | Loss: 0.0134
Epoch: 101/130 | Batch 0100/0313 | Loss: 0.0078
Epoch: 101/130 | Batch 0150/0313 | Loss: 0.0131
Epoch: 101/130 | Batch 0200/0313 | Loss: 0.0191
Epoch: 101/130 | Batch 0250/0313 | Loss: 0.0072
Epoch: 101/130 | Batch 0300/0313 | Loss: 0.0067
**Epoch: 101/130 | Train. Acc.: 99.765% | Loss: 0.0103
**Epoch: 101/130 | Valid. Acc.: 94.060% | Loss: 0.2217
Time elapsed: 53.58 min
Epoch: 102/130 | Current Learning Rate: 0.000781
Epoch: 102/130 | Batch 0000/0313 | Loss: 0.0270
Epoch: 102/130 | Batch 0050/0313 | Loss: 0.0044
Epoch: 102/130 | Batch 0100/0313 | Loss: 0.0296
Epoch: 102/130 | Batch 0150/0313 | Loss: 0.0058
Epoch: 102/130 | Batch 0200/0313 | Loss: 0.0289
Epoch: 102/130 | Batch 0250/0313 | Loss: 0.0088
Epoch: 102/130 | Batch 0300/0313 | Loss: 0.0126
**Epoch: 102/130 | Train. Acc.: 99.780% | Loss: 0.0103
**Epoch: 102/130 | Valid. Acc.: 94.060% | Loss: 0.2202
Time elapsed: 54.10 min
Epoch: 103/130 | Current Learning Rate: 0.000781
Epoch: 103/130 | Batch 0000/0313 | Loss: 0.0290
Epoch: 103/130 | Batch 0050/0313 | Loss: 0.0134
Epoch: 103/130 | Batch 0100/0313 | Loss: 0.0118
Epoch: 103/130 | Batch 0150/0313 | Loss: 0.0097
Epoch: 103/130 | Batch 0200/0313 | Loss: 0.0047
Epoch: 103/130 | Batch 0250/0313 | Loss: 0.0102
Epoch: 103/130 | Batch 0300/0313 | Loss: 0.0060
**Epoch: 103/130 | Train. Acc.: 99.743% | Loss: 0.0104
**Epoch: 103/130 | Valid. Acc.: 94.200% | Loss: 0.2175
**Validation loss decreased (0.219673 --> 0.217477). Saving model ...
Time elapsed: 54.64 min
Epoch: 104/130 | Current Learning Rate: 0.000781
Epoch: 104/130 | Batch 0000/0313 | Loss: 0.0076
Epoch: 104/130 | Batch 0050/0313 | Loss: 0.0050
Epoch: 104/130 | Batch 0100/0313 | Loss: 0.0224
Epoch: 104/130 | Batch 0150/0313 | Loss: 0.0069
Epoch: 104/130 | Batch 0200/0313 | Loss: 0.0166
Epoch: 104/130 | Batch 0250/0313 | Loss: 0.0786
Epoch: 104/130 | Batch 0300/0313 | Loss: 0.0080
**Epoch: 104/130 | Train. Acc.: 99.780% | Loss: 0.0100
**Epoch: 104/130 | Valid. Acc.: 94.060% | Loss: 0.2221
Time elapsed: 55.16 min
Epoch: 105/130 | Current Learning Rate: 0.000781
Epoch: 105/130 | Batch 0000/0313 | Loss: 0.0135
Epoch: 105/130 | Batch 0050/0313 | Loss: 0.0032
Epoch: 105/130 | Batch 0100/0313 | Loss: 0.0193
Epoch: 105/130 | Batch 0150/0313 | Loss: 0.0073
Epoch: 105/130 | Batch 0200/0313 | Loss: 0.0042
Epoch: 105/130 | Batch 0250/0313 | Loss: 0.0058
Epoch: 105/130 | Batch 0300/0313 | Loss: 0.0110
**Epoch: 105/130 | Train. Acc.: 99.722% | Loss: 0.0106
**Epoch: 105/130 | Valid. Acc.: 94.000% | Loss: 0.2239
Time elapsed: 55.69 min
Epoch: 106/130 | Current Learning Rate: 0.000781
Epoch: 106/130 | Batch 0000/0313 | Loss: 0.0209
Epoch: 106/130 | Batch 0050/0313 | Loss: 0.0363
Epoch: 106/130 | Batch 0100/0313 | Loss: 0.0172
Epoch: 106/130 | Batch 0150/0313 | Loss: 0.0078
Epoch: 106/130 | Batch 0200/0313 | Loss: 0.0079
Epoch: 106/130 | Batch 0250/0313 | Loss: 0.0131
Epoch: 106/130 | Batch 0300/0313 | Loss: 0.0081
**Epoch: 106/130 | Train. Acc.: 99.737% | Loss: 0.0109
**Epoch: 106/130 | Valid. Acc.: 93.990% | Loss: 0.2196
Time elapsed: 56.22 min
Epoch: 107/130 | Current Learning Rate: 0.000781
Epoch: 107/130 | Batch 0000/0313 | Loss: 0.0089
Epoch: 107/130 | Batch 0050/0313 | Loss: 0.0232
Epoch: 107/130 | Batch 0100/0313 | Loss: 0.0062
Epoch: 107/130 | Batch 0150/0313 | Loss: 0.0062
Epoch: 107/130 | Batch 0200/0313 | Loss: 0.0129
Epoch: 107/130 | Batch 0250/0313 | Loss: 0.0058
Epoch: 107/130 | Batch 0300/0313 | Loss: 0.0042
**Epoch: 107/130 | Train. Acc.: 99.750% | Loss: 0.0101
**Epoch: 107/130 | Valid. Acc.: 94.050% | Loss: 0.2197
Time elapsed: 56.76 min
Epoch: 108/130 | Current Learning Rate: 0.000781
Epoch: 108/130 | Batch 0000/0313 | Loss: 0.0062
Epoch: 108/130 | Batch 0050/0313 | Loss: 0.0142
Epoch: 108/130 | Batch 0100/0313 | Loss: 0.0241
Epoch: 108/130 | Batch 0150/0313 | Loss: 0.0156
Epoch: 108/130 | Batch 0200/0313 | Loss: 0.0134
Epoch: 108/130 | Batch 0250/0313 | Loss: 0.0141
Epoch: 108/130 | Batch 0300/0313 | Loss: 0.0076
**Epoch: 108/130 | Train. Acc.: 99.740% | Loss: 0.0104
**Epoch: 108/130 | Valid. Acc.: 94.110% | Loss: 0.2194
Time elapsed: 57.27 min
Epoch: 109/130 | Current Learning Rate: 0.000781
Epoch: 109/130 | Batch 0000/0313 | Loss: 0.0052
Epoch: 109/130 | Batch 0050/0313 | Loss: 0.0080
Epoch: 109/130 | Batch 0100/0313 | Loss: 0.0059
Epoch: 109/130 | Batch 0150/0313 | Loss: 0.0308
Epoch: 109/130 | Batch 0200/0313 | Loss: 0.0087
Epoch: 109/130 | Batch 0250/0313 | Loss: 0.0030
Epoch: 109/130 | Batch 0300/0313 | Loss: 0.0903
**Epoch: 109/130 | Train. Acc.: 99.733% | Loss: 0.0107
**Epoch: 109/130 | Valid. Acc.: 94.110% | Loss: 0.2217
Time elapsed: 57.80 min
Epoch: 110/130 | Current Learning Rate: 0.000781
Epoch: 110/130 | Batch 0000/0313 | Loss: 0.0097
Epoch: 110/130 | Batch 0050/0313 | Loss: 0.0068
Epoch: 110/130 | Batch 0100/0313 | Loss: 0.0203
Epoch: 110/130 | Batch 0150/0313 | Loss: 0.0081
Epoch: 110/130 | Batch 0200/0313 | Loss: 0.0049
Epoch: 110/130 | Batch 0250/0313 | Loss: 0.0055
Epoch: 110/130 | Batch 0300/0313 | Loss: 0.0168
**Epoch: 110/130 | Train. Acc.: 99.718% | Loss: 0.0103
**Epoch: 110/130 | Valid. Acc.: 94.230% | Loss: 0.2205
Epoch 00110: reducing learning rate of group 0 to 3.9063e-04.
Time elapsed: 58.32 min
Epoch: 111/130 | Current Learning Rate: 0.000391
Epoch: 111/130 | Batch 0000/0313 | Loss: 0.0125
Epoch: 111/130 | Batch 0050/0313 | Loss: 0.0064
Epoch: 111/130 | Batch 0100/0313 | Loss: 0.0151
Epoch: 111/130 | Batch 0150/0313 | Loss: 0.0063
Epoch: 111/130 | Batch 0200/0313 | Loss: 0.0483
Epoch: 111/130 | Batch 0250/0313 | Loss: 0.0293
Epoch: 111/130 | Batch 0300/0313 | Loss: 0.0133
**Epoch: 111/130 | Train. Acc.: 99.843% | Loss: 0.0085
**Epoch: 111/130 | Valid. Acc.: 94.100% | Loss: 0.2191
Time elapsed: 58.85 min
Epoch: 112/130 | Current Learning Rate: 0.000391
Epoch: 112/130 | Batch 0000/0313 | Loss: 0.0119
Epoch: 112/130 | Batch 0050/0313 | Loss: 0.0397
Epoch: 112/130 | Batch 0100/0313 | Loss: 0.0089
Epoch: 112/130 | Batch 0150/0313 | Loss: 0.0153
Epoch: 112/130 | Batch 0200/0313 | Loss: 0.0042
Epoch: 112/130 | Batch 0250/0313 | Loss: 0.0253
Epoch: 112/130 | Batch 0300/0313 | Loss: 0.0084
**Epoch: 112/130 | Train. Acc.: 99.735% | Loss: 0.0098
**Epoch: 112/130 | Valid. Acc.: 94.200% | Loss: 0.2215
Time elapsed: 59.38 min
Epoch: 113/130 | Current Learning Rate: 0.000391
Epoch: 113/130 | Batch 0000/0313 | Loss: 0.0052
Epoch: 113/130 | Batch 0050/0313 | Loss: 0.0095
Epoch: 113/130 | Batch 0100/0313 | Loss: 0.0107
Epoch: 113/130 | Batch 0150/0313 | Loss: 0.0150
Epoch: 113/130 | Batch 0200/0313 | Loss: 0.0189
Epoch: 113/130 | Batch 0250/0313 | Loss: 0.0033
Epoch: 113/130 | Batch 0300/0313 | Loss: 0.0116
**Epoch: 113/130 | Train. Acc.: 99.792% | Loss: 0.0094
**Epoch: 113/130 | Valid. Acc.: 94.020% | Loss: 0.2202
Time elapsed: 59.90 min
Epoch: 114/130 | Current Learning Rate: 0.000391
Epoch: 114/130 | Batch 0000/0313 | Loss: 0.0094
Epoch: 114/130 | Batch 0050/0313 | Loss: 0.0101
Epoch: 114/130 | Batch 0100/0313 | Loss: 0.0129
Epoch: 114/130 | Batch 0150/0313 | Loss: 0.0118
Epoch: 114/130 | Batch 0200/0313 | Loss: 0.0048
Epoch: 114/130 | Batch 0250/0313 | Loss: 0.0287
Epoch: 114/130 | Batch 0300/0313 | Loss: 0.0063
**Epoch: 114/130 | Train. Acc.: 99.720% | Loss: 0.0104
**Epoch: 114/130 | Valid. Acc.: 94.120% | Loss: 0.2204
Time elapsed: 60.43 min
Epoch: 115/130 | Current Learning Rate: 0.000391
Epoch: 115/130 | Batch 0000/0313 | Loss: 0.0036
Epoch: 115/130 | Batch 0050/0313 | Loss: 0.0069
Epoch: 115/130 | Batch 0100/0313 | Loss: 0.0083
Epoch: 115/130 | Batch 0150/0313 | Loss: 0.0052
Epoch: 115/130 | Batch 0200/0313 | Loss: 0.0060
Epoch: 115/130 | Batch 0250/0313 | Loss: 0.0029
Epoch: 115/130 | Batch 0300/0313 | Loss: 0.0116
**Epoch: 115/130 | Train. Acc.: 99.772% | Loss: 0.0092
**Epoch: 115/130 | Valid. Acc.: 94.220% | Loss: 0.2174
**Validation loss decreased (0.217477 --> 0.217403). Saving model ...
Time elapsed: 60.96 min
Epoch: 116/130 | Current Learning Rate: 0.000391
Epoch: 116/130 | Batch 0000/0313 | Loss: 0.0039
Epoch: 116/130 | Batch 0050/0313 | Loss: 0.0105
Epoch: 116/130 | Batch 0100/0313 | Loss: 0.0454
Epoch: 116/130 | Batch 0150/0313 | Loss: 0.0096
Epoch: 116/130 | Batch 0200/0313 | Loss: 0.0065
Epoch: 116/130 | Batch 0250/0313 | Loss: 0.0305
Epoch: 116/130 | Batch 0300/0313 | Loss: 0.0204
**Epoch: 116/130 | Train. Acc.: 99.778% | Loss: 0.0093
**Epoch: 116/130 | Valid. Acc.: 94.150% | Loss: 0.2183
Time elapsed: 61.49 min
Epoch: 117/130 | Current Learning Rate: 0.000391
Epoch: 117/130 | Batch 0000/0313 | Loss: 0.0109
Epoch: 117/130 | Batch 0050/0313 | Loss: 0.0205
Epoch: 117/130 | Batch 0100/0313 | Loss: 0.0051
Epoch: 117/130 | Batch 0150/0313 | Loss: 0.0337
Epoch: 117/130 | Batch 0200/0313 | Loss: 0.0067
Epoch: 117/130 | Batch 0250/0313 | Loss: 0.0175
Epoch: 117/130 | Batch 0300/0313 | Loss: 0.0096
**Epoch: 117/130 | Train. Acc.: 99.810% | Loss: 0.0089
**Epoch: 117/130 | Valid. Acc.: 94.130% | Loss: 0.2189
Time elapsed: 62.02 min
Epoch: 118/130 | Current Learning Rate: 0.000391
Epoch: 118/130 | Batch 0000/0313 | Loss: 0.0186
Epoch: 118/130 | Batch 0050/0313 | Loss: 0.0434
Epoch: 118/130 | Batch 0100/0313 | Loss: 0.0094
Epoch: 118/130 | Batch 0150/0313 | Loss: 0.0128
Epoch: 118/130 | Batch 0200/0313 | Loss: 0.0029
Epoch: 118/130 | Batch 0250/0313 | Loss: 0.0174
Epoch: 118/130 | Batch 0300/0313 | Loss: 0.0255
**Epoch: 118/130 | Train. Acc.: 99.778% | Loss: 0.0097
**Epoch: 118/130 | Valid. Acc.: 94.080% | Loss: 0.2199
Time elapsed: 62.55 min
Epoch: 119/130 | Current Learning Rate: 0.000391
Epoch: 119/130 | Batch 0000/0313 | Loss: 0.0073
Epoch: 119/130 | Batch 0050/0313 | Loss: 0.0020
Epoch: 119/130 | Batch 0100/0313 | Loss: 0.0105
Epoch: 119/130 | Batch 0150/0313 | Loss: 0.0201
Epoch: 119/130 | Batch 0200/0313 | Loss: 0.0073
Epoch: 119/130 | Batch 0250/0313 | Loss: 0.0755
Epoch: 119/130 | Batch 0300/0313 | Loss: 0.0089
**Epoch: 119/130 | Train. Acc.: 99.782% | Loss: 0.0090
**Epoch: 119/130 | Valid. Acc.: 94.140% | Loss: 0.2186
Time elapsed: 63.08 min
Epoch: 120/130 | Current Learning Rate: 0.000391
Epoch: 120/130 | Batch 0000/0313 | Loss: 0.0123
Epoch: 120/130 | Batch 0050/0313 | Loss: 0.0113
Epoch: 120/130 | Batch 0100/0313 | Loss: 0.0096
Epoch: 120/130 | Batch 0150/0313 | Loss: 0.0097
Epoch: 120/130 | Batch 0200/0313 | Loss: 0.0061
Epoch: 120/130 | Batch 0250/0313 | Loss: 0.0018
Epoch: 120/130 | Batch 0300/0313 | Loss: 0.0096
**Epoch: 120/130 | Train. Acc.: 99.825% | Loss: 0.0085
**Epoch: 120/130 | Valid. Acc.: 94.250% | Loss: 0.2171
**Validation loss decreased (0.217403 --> 0.217127). Saving model ...
Time elapsed: 63.61 min
Epoch: 121/130 | Current Learning Rate: 0.000391
Epoch: 121/130 | Batch 0000/0313 | Loss: 0.0147
Epoch: 121/130 | Batch 0050/0313 | Loss: 0.0068
Epoch: 121/130 | Batch 0100/0313 | Loss: 0.0501
Epoch: 121/130 | Batch 0150/0313 | Loss: 0.0082
Epoch: 121/130 | Batch 0200/0313 | Loss: 0.0115
Epoch: 121/130 | Batch 0250/0313 | Loss: 0.0049
Epoch: 121/130 | Batch 0300/0313 | Loss: 0.0090
**Epoch: 121/130 | Train. Acc.: 99.782% | Loss: 0.0089
**Epoch: 121/130 | Valid. Acc.: 94.230% | Loss: 0.2202
Time elapsed: 64.15 min
Epoch: 122/130 | Current Learning Rate: 0.000391
Epoch: 122/130 | Batch 0000/0313 | Loss: 0.0062
Epoch: 122/130 | Batch 0050/0313 | Loss: 0.0082
Epoch: 122/130 | Batch 0100/0313 | Loss: 0.0123
Epoch: 122/130 | Batch 0150/0313 | Loss: 0.0171
Epoch: 122/130 | Batch 0200/0313 | Loss: 0.0111
Epoch: 122/130 | Batch 0250/0313 | Loss: 0.0065
Epoch: 122/130 | Batch 0300/0313 | Loss: 0.0037
**Epoch: 122/130 | Train. Acc.: 99.765% | Loss: 0.0093
**Epoch: 122/130 | Valid. Acc.: 94.110% | Loss: 0.2208
Time elapsed: 64.67 min
Epoch: 123/130 | Current Learning Rate: 0.000391
Epoch: 123/130 | Batch 0000/0313 | Loss: 0.0088
Epoch: 123/130 | Batch 0050/0313 | Loss: 0.0293
Epoch: 123/130 | Batch 0100/0313 | Loss: 0.0025
Epoch: 123/130 | Batch 0150/0313 | Loss: 0.0077
Epoch: 123/130 | Batch 0200/0313 | Loss: 0.0065
Epoch: 123/130 | Batch 0250/0313 | Loss: 0.0061
Epoch: 123/130 | Batch 0300/0313 | Loss: 0.0114
**Epoch: 123/130 | Train. Acc.: 99.767% | Loss: 0.0092
**Epoch: 123/130 | Valid. Acc.: 94.100% | Loss: 0.2176
Time elapsed: 65.20 min
Epoch: 124/130 | Current Learning Rate: 0.000391
Epoch: 124/130 | Batch 0000/0313 | Loss: 0.0055
Epoch: 124/130 | Batch 0050/0313 | Loss: 0.0206
Epoch: 124/130 | Batch 0100/0313 | Loss: 0.0064
Epoch: 124/130 | Batch 0150/0313 | Loss: 0.0253
Epoch: 124/130 | Batch 0200/0313 | Loss: 0.0045
Epoch: 124/130 | Batch 0250/0313 | Loss: 0.0033
Epoch: 124/130 | Batch 0300/0313 | Loss: 0.0189
**Epoch: 124/130 | Train. Acc.: 99.792% | Loss: 0.0087
**Epoch: 124/130 | Valid. Acc.: 94.190% | Loss: 0.2174
Time elapsed: 65.73 min
Epoch: 125/130 | Current Learning Rate: 0.000391
Epoch: 125/130 | Batch 0000/0313 | Loss: 0.0093
Epoch: 125/130 | Batch 0050/0313 | Loss: 0.0074
Epoch: 125/130 | Batch 0100/0313 | Loss: 0.0145
Epoch: 125/130 | Batch 0150/0313 | Loss: 0.0043
Epoch: 125/130 | Batch 0200/0313 | Loss: 0.0032
Epoch: 125/130 | Batch 0250/0313 | Loss: 0.0321
Epoch: 125/130 | Batch 0300/0313 | Loss: 0.0070
**Epoch: 125/130 | Train. Acc.: 99.808% | Loss: 0.0089
**Epoch: 125/130 | Valid. Acc.: 94.170% | Loss: 0.2180
Time elapsed: 66.26 min
Epoch: 126/130 | Current Learning Rate: 0.000391
Epoch: 126/130 | Batch 0000/0313 | Loss: 0.0059
Epoch: 126/130 | Batch 0050/0313 | Loss: 0.0103
Epoch: 126/130 | Batch 0100/0313 | Loss: 0.0148
Epoch: 126/130 | Batch 0150/0313 | Loss: 0.0120
Epoch: 126/130 | Batch 0200/0313 | Loss: 0.0191
Epoch: 126/130 | Batch 0250/0313 | Loss: 0.0078
Epoch: 126/130 | Batch 0300/0313 | Loss: 0.0069
**Epoch: 126/130 | Train. Acc.: 99.818% | Loss: 0.0086
**Epoch: 126/130 | Valid. Acc.: 94.080% | Loss: 0.2177
Time elapsed: 66.79 min
Epoch: 127/130 | Current Learning Rate: 0.000391
Epoch: 127/130 | Batch 0000/0313 | Loss: 0.0064
Epoch: 127/130 | Batch 0050/0313 | Loss: 0.0058
Epoch: 127/130 | Batch 0100/0313 | Loss: 0.0137
Epoch: 127/130 | Batch 0150/0313 | Loss: 0.0107
Epoch: 127/130 | Batch 0200/0313 | Loss: 0.0034
Epoch: 127/130 | Batch 0250/0313 | Loss: 0.0059
Epoch: 127/130 | Batch 0300/0313 | Loss: 0.0046
**Epoch: 127/130 | Train. Acc.: 99.740% | Loss: 0.0103
**Epoch: 127/130 | Valid. Acc.: 94.050% | Loss: 0.2206
Epoch 00127: reducing learning rate of group 0 to 1.9531e-04.
Time elapsed: 67.32 min
Epoch: 128/130 | Current Learning Rate: 0.000195
Epoch: 128/130 | Batch 0000/0313 | Loss: 0.0204
Epoch: 128/130 | Batch 0050/0313 | Loss: 0.0070
Epoch: 128/130 | Batch 0100/0313 | Loss: 0.0036
Epoch: 128/130 | Batch 0150/0313 | Loss: 0.0074
Epoch: 128/130 | Batch 0200/0313 | Loss: 0.0058
Epoch: 128/130 | Batch 0250/0313 | Loss: 0.0178
Epoch: 128/130 | Batch 0300/0313 | Loss: 0.0280
**Epoch: 128/130 | Train. Acc.: 99.810% | Loss: 0.0084
**Epoch: 128/130 | Valid. Acc.: 94.070% | Loss: 0.2203
Time elapsed: 67.85 min
Epoch: 129/130 | Current Learning Rate: 0.000195
Epoch: 129/130 | Batch 0000/0313 | Loss: 0.0019
Epoch: 129/130 | Batch 0050/0313 | Loss: 0.0130
Epoch: 129/130 | Batch 0100/0313 | Loss: 0.0129
Epoch: 129/130 | Batch 0150/0313 | Loss: 0.0226
Epoch: 129/130 | Batch 0200/0313 | Loss: 0.0456
Epoch: 129/130 | Batch 0250/0313 | Loss: 0.0055
Epoch: 129/130 | Batch 0300/0313 | Loss: 0.0313
**Epoch: 129/130 | Train. Acc.: 99.828% | Loss: 0.0080
**Epoch: 129/130 | Valid. Acc.: 94.050% | Loss: 0.2188
Time elapsed: 68.37 min
Epoch: 130/130 | Current Learning Rate: 0.000195
Epoch: 130/130 | Batch 0000/0313 | Loss: 0.0024
Epoch: 130/130 | Batch 0050/0313 | Loss: 0.0262
Epoch: 130/130 | Batch 0100/0313 | Loss: 0.0068
Epoch: 130/130 | Batch 0150/0313 | Loss: 0.0046
Epoch: 130/130 | Batch 0200/0313 | Loss: 0.0090
Epoch: 130/130 | Batch 0250/0313 | Loss: 0.0200
Epoch: 130/130 | Batch 0300/0313 | Loss: 0.0088
**Epoch: 130/130 | Train. Acc.: 99.818% | Loss: 0.0087
**Epoch: 130/130 | Valid. Acc.: 94.100% | Loss: 0.2173
Time elapsed: 68.90 min
Total Training Time: 68.90 min
Model: ResNet18
Test Loss: 0.2351
Test Accuracy (Overall): 93.91%

Test Accuracy of Airplane: 94% (944/1000)
Test Accuracy of      Car: 97% (974/1000)
Test Accuracy of     Bird: 90% (909/1000)
Test Accuracy of      Cat: 87% (878/1000)
Test Accuracy of     Deer: 95% (950/1000)
Test Accuracy of      Dog: 88% (886/1000)
Test Accuracy of     Frog: 96% (962/1000)
Test Accuracy of    Horse: 96% (964/1000)
Test Accuracy of     Ship: 96% (966/1000)
Test Accuracy of    Truck: 95% (958/1000)
Training ResNet34 for 130 epochs with initial learning rate 0.1...
Epoch: 001/130 | Current Learning Rate: 0.100000
Epoch: 001/130 | Batch 0000/0313 | Loss: 2.5435
Epoch: 001/130 | Batch 0050/0313 | Loss: 2.2249
Epoch: 001/130 | Batch 0100/0313 | Loss: 2.1807
Epoch: 001/130 | Batch 0150/0313 | Loss: 1.9544
Epoch: 001/130 | Batch 0200/0313 | Loss: 2.1244
Epoch: 001/130 | Batch 0250/0313 | Loss: 2.1184
Epoch: 001/130 | Batch 0300/0313 | Loss: 1.9338
**Epoch: 001/130 | Train. Acc.: 26.880% | Loss: 1.9588
**Epoch: 001/130 | Valid. Acc.: 30.440% | Loss: 1.8620
**Validation loss decreased (inf --> 1.862013). Saving model ...
Time elapsed: 0.58 min
Epoch: 002/130 | Current Learning Rate: 0.100000
Epoch: 002/130 | Batch 0000/0313 | Loss: 2.0642
Epoch: 002/130 | Batch 0050/0313 | Loss: 1.9881
Epoch: 002/130 | Batch 0100/0313 | Loss: 1.6945
Epoch: 002/130 | Batch 0150/0313 | Loss: 1.9583
Epoch: 002/130 | Batch 0200/0313 | Loss: 1.9175
Epoch: 002/130 | Batch 0250/0313 | Loss: 1.9073
Epoch: 002/130 | Batch 0300/0313 | Loss: 1.7549
**Epoch: 002/130 | Train. Acc.: 34.995% | Loss: 1.7613
**Epoch: 002/130 | Valid. Acc.: 37.710% | Loss: 1.6709
**Validation loss decreased (1.862013 --> 1.670914). Saving model ...
Time elapsed: 1.18 min
Epoch: 003/130 | Current Learning Rate: 0.100000
Epoch: 003/130 | Batch 0000/0313 | Loss: 1.6026
Epoch: 003/130 | Batch 0050/0313 | Loss: 1.6938
Epoch: 003/130 | Batch 0100/0313 | Loss: 1.7210
Epoch: 003/130 | Batch 0150/0313 | Loss: 1.7104
Epoch: 003/130 | Batch 0200/0313 | Loss: 1.8268
Epoch: 003/130 | Batch 0250/0313 | Loss: 1.5589
Epoch: 003/130 | Batch 0300/0313 | Loss: 1.5281
**Epoch: 003/130 | Train. Acc.: 39.102% | Loss: 1.6496
**Epoch: 003/130 | Valid. Acc.: 40.830% | Loss: 1.5796
**Validation loss decreased (1.670914 --> 1.579566). Saving model ...
Time elapsed: 1.76 min
Epoch: 004/130 | Current Learning Rate: 0.100000
Epoch: 004/130 | Batch 0000/0313 | Loss: 1.6470
Epoch: 004/130 | Batch 0050/0313 | Loss: 1.5437
Epoch: 004/130 | Batch 0100/0313 | Loss: 1.7505
Epoch: 004/130 | Batch 0150/0313 | Loss: 1.4491
Epoch: 004/130 | Batch 0200/0313 | Loss: 1.4597
Epoch: 004/130 | Batch 0250/0313 | Loss: 1.5410
Epoch: 004/130 | Batch 0300/0313 | Loss: 1.5446
**Epoch: 004/130 | Train. Acc.: 41.252% | Loss: 1.6309
**Epoch: 004/130 | Valid. Acc.: 44.480% | Loss: 1.5346
**Validation loss decreased (1.579566 --> 1.534552). Saving model ...
Time elapsed: 2.36 min
Epoch: 005/130 | Current Learning Rate: 0.100000
Epoch: 005/130 | Batch 0000/0313 | Loss: 1.7212
Epoch: 005/130 | Batch 0050/0313 | Loss: 1.5161
Epoch: 005/130 | Batch 0100/0313 | Loss: 1.6514
Epoch: 005/130 | Batch 0150/0313 | Loss: 1.5449
Epoch: 005/130 | Batch 0200/0313 | Loss: 1.5217
Epoch: 005/130 | Batch 0250/0313 | Loss: 1.4103
Epoch: 005/130 | Batch 0300/0313 | Loss: 1.2579
**Epoch: 005/130 | Train. Acc.: 46.625% | Loss: 1.4628
**Epoch: 005/130 | Valid. Acc.: 50.750% | Loss: 1.3586
**Validation loss decreased (1.534552 --> 1.358579). Saving model ...
Time elapsed: 2.94 min
Epoch: 006/130 | Current Learning Rate: 0.100000
Epoch: 006/130 | Batch 0000/0313 | Loss: 1.3678
Epoch: 006/130 | Batch 0050/0313 | Loss: 1.3495
Epoch: 006/130 | Batch 0100/0313 | Loss: 1.4861
Epoch: 006/130 | Batch 0150/0313 | Loss: 1.4393
Epoch: 006/130 | Batch 0200/0313 | Loss: 1.3689
Epoch: 006/130 | Batch 0250/0313 | Loss: 1.1900
Epoch: 006/130 | Batch 0300/0313 | Loss: 1.2499
**Epoch: 006/130 | Train. Acc.: 47.983% | Loss: 1.4536
**Epoch: 006/130 | Valid. Acc.: 51.440% | Loss: 1.3880
Time elapsed: 3.52 min
Epoch: 007/130 | Current Learning Rate: 0.100000
Epoch: 007/130 | Batch 0000/0313 | Loss: 1.2415
Epoch: 007/130 | Batch 0050/0313 | Loss: 1.1105
Epoch: 007/130 | Batch 0100/0313 | Loss: 1.2685
Epoch: 007/130 | Batch 0150/0313 | Loss: 1.1928
Epoch: 007/130 | Batch 0200/0313 | Loss: 1.2844
Epoch: 007/130 | Batch 0250/0313 | Loss: 1.0805
Epoch: 007/130 | Batch 0300/0313 | Loss: 1.2076
**Epoch: 007/130 | Train. Acc.: 58.973% | Loss: 1.1572
**Epoch: 007/130 | Valid. Acc.: 63.650% | Loss: 1.0237
**Validation loss decreased (1.358579 --> 1.023684). Saving model ...
Time elapsed: 4.11 min
Epoch: 008/130 | Current Learning Rate: 0.100000
Epoch: 008/130 | Batch 0000/0313 | Loss: 1.0076
Epoch: 008/130 | Batch 0050/0313 | Loss: 1.1566
Epoch: 008/130 | Batch 0100/0313 | Loss: 1.1016
Epoch: 008/130 | Batch 0150/0313 | Loss: 1.0565
Epoch: 008/130 | Batch 0200/0313 | Loss: 1.1486
Epoch: 008/130 | Batch 0250/0313 | Loss: 0.9209
Epoch: 008/130 | Batch 0300/0313 | Loss: 0.9903
**Epoch: 008/130 | Train. Acc.: 64.593% | Loss: 1.0043
**Epoch: 008/130 | Valid. Acc.: 67.890% | Loss: 0.9047
**Validation loss decreased (1.023684 --> 0.904719). Saving model ...
Time elapsed: 4.70 min
Epoch: 009/130 | Current Learning Rate: 0.100000
Epoch: 009/130 | Batch 0000/0313 | Loss: 0.9466
Epoch: 009/130 | Batch 0050/0313 | Loss: 0.9773
Epoch: 009/130 | Batch 0100/0313 | Loss: 0.9716
Epoch: 009/130 | Batch 0150/0313 | Loss: 0.8875
Epoch: 009/130 | Batch 0200/0313 | Loss: 0.9811
Epoch: 009/130 | Batch 0250/0313 | Loss: 0.7814
Epoch: 009/130 | Batch 0300/0313 | Loss: 0.9853
**Epoch: 009/130 | Train. Acc.: 60.975% | Loss: 1.1154
**Epoch: 009/130 | Valid. Acc.: 66.230% | Loss: 0.9952
Time elapsed: 5.29 min
Epoch: 010/130 | Current Learning Rate: 0.100000
Epoch: 010/130 | Batch 0000/0313 | Loss: 0.8154
Epoch: 010/130 | Batch 0050/0313 | Loss: 0.9939
Epoch: 010/130 | Batch 0100/0313 | Loss: 0.8443
Epoch: 010/130 | Batch 0150/0313 | Loss: 0.7401
Epoch: 010/130 | Batch 0200/0313 | Loss: 0.8647
Epoch: 010/130 | Batch 0250/0313 | Loss: 0.7129
Epoch: 010/130 | Batch 0300/0313 | Loss: 0.7529
**Epoch: 010/130 | Train. Acc.: 59.003% | Loss: 1.1955
**Epoch: 010/130 | Valid. Acc.: 61.460% | Loss: 1.2038
Time elapsed: 5.88 min
Epoch: 011/130 | Current Learning Rate: 0.100000
Epoch: 011/130 | Batch 0000/0313 | Loss: 0.7064
Epoch: 011/130 | Batch 0050/0313 | Loss: 0.7184
Epoch: 011/130 | Batch 0100/0313 | Loss: 0.8611
Epoch: 011/130 | Batch 0150/0313 | Loss: 0.8111
Epoch: 011/130 | Batch 0200/0313 | Loss: 0.8219
Epoch: 011/130 | Batch 0250/0313 | Loss: 0.9331
Epoch: 011/130 | Batch 0300/0313 | Loss: 0.9298
**Epoch: 011/130 | Train. Acc.: 68.787% | Loss: 0.9064
**Epoch: 011/130 | Valid. Acc.: 73.450% | Loss: 0.7830
**Validation loss decreased (0.904719 --> 0.782990). Saving model ...
Time elapsed: 6.47 min
Epoch: 012/130 | Current Learning Rate: 0.100000
Epoch: 012/130 | Batch 0000/0313 | Loss: 0.7998
Epoch: 012/130 | Batch 0050/0313 | Loss: 0.6984
Epoch: 012/130 | Batch 0100/0313 | Loss: 0.7237
Epoch: 012/130 | Batch 0150/0313 | Loss: 0.6844
Epoch: 012/130 | Batch 0200/0313 | Loss: 0.7784
Epoch: 012/130 | Batch 0250/0313 | Loss: 0.6248
Epoch: 012/130 | Batch 0300/0313 | Loss: 0.7561
**Epoch: 012/130 | Train. Acc.: 69.075% | Loss: 0.8882
**Epoch: 012/130 | Valid. Acc.: 73.850% | Loss: 0.7660
**Validation loss decreased (0.782990 --> 0.765979). Saving model ...
Time elapsed: 7.07 min
Epoch: 013/130 | Current Learning Rate: 0.100000
Epoch: 013/130 | Batch 0000/0313 | Loss: 0.7185
Epoch: 013/130 | Batch 0050/0313 | Loss: 0.5906
Epoch: 013/130 | Batch 0100/0313 | Loss: 0.6027
Epoch: 013/130 | Batch 0150/0313 | Loss: 0.8790
Epoch: 013/130 | Batch 0200/0313 | Loss: 0.6250
Epoch: 013/130 | Batch 0250/0313 | Loss: 0.6866
Epoch: 013/130 | Batch 0300/0313 | Loss: 0.6429
**Epoch: 013/130 | Train. Acc.: 70.368% | Loss: 0.8502
**Epoch: 013/130 | Valid. Acc.: 74.530% | Loss: 0.7571
**Validation loss decreased (0.765979 --> 0.757056). Saving model ...
Time elapsed: 7.66 min
Epoch: 014/130 | Current Learning Rate: 0.100000
Epoch: 014/130 | Batch 0000/0313 | Loss: 0.7862
Epoch: 014/130 | Batch 0050/0313 | Loss: 0.6231
Epoch: 014/130 | Batch 0100/0313 | Loss: 0.6689
Epoch: 014/130 | Batch 0150/0313 | Loss: 0.6804
Epoch: 014/130 | Batch 0200/0313 | Loss: 0.6971
Epoch: 014/130 | Batch 0250/0313 | Loss: 0.6678
Epoch: 014/130 | Batch 0300/0313 | Loss: 0.8338
**Epoch: 014/130 | Train. Acc.: 73.388% | Loss: 0.7543
**Epoch: 014/130 | Valid. Acc.: 75.830% | Loss: 0.6980
**Validation loss decreased (0.757056 --> 0.697957). Saving model ...
Time elapsed: 8.25 min
Epoch: 015/130 | Current Learning Rate: 0.100000
Epoch: 015/130 | Batch 0000/0313 | Loss: 0.6701
Epoch: 015/130 | Batch 0050/0313 | Loss: 0.8014
Epoch: 015/130 | Batch 0100/0313 | Loss: 0.5342
Epoch: 015/130 | Batch 0150/0313 | Loss: 0.7982
Epoch: 015/130 | Batch 0200/0313 | Loss: 0.7155
Epoch: 015/130 | Batch 0250/0313 | Loss: 0.6807
Epoch: 015/130 | Batch 0300/0313 | Loss: 0.5914
**Epoch: 015/130 | Train. Acc.: 73.615% | Loss: 0.7649
**Epoch: 015/130 | Valid. Acc.: 78.230% | Loss: 0.6374
**Validation loss decreased (0.697957 --> 0.637428). Saving model ...
Time elapsed: 8.84 min
Epoch: 016/130 | Current Learning Rate: 0.100000
Epoch: 016/130 | Batch 0000/0313 | Loss: 0.6680
Epoch: 016/130 | Batch 0050/0313 | Loss: 0.6127
Epoch: 016/130 | Batch 0100/0313 | Loss: 0.6472
Epoch: 016/130 | Batch 0150/0313 | Loss: 0.6243
Epoch: 016/130 | Batch 0200/0313 | Loss: 0.7239
Epoch: 016/130 | Batch 0250/0313 | Loss: 0.6204
Epoch: 016/130 | Batch 0300/0313 | Loss: 0.6307
**Epoch: 016/130 | Train. Acc.: 76.490% | Loss: 0.6892
**Epoch: 016/130 | Valid. Acc.: 79.460% | Loss: 0.6140
**Validation loss decreased (0.637428 --> 0.614023). Saving model ...
Time elapsed: 9.42 min
Epoch: 017/130 | Current Learning Rate: 0.100000
Epoch: 017/130 | Batch 0000/0313 | Loss: 0.6167
Epoch: 017/130 | Batch 0050/0313 | Loss: 0.7134
Epoch: 017/130 | Batch 0100/0313 | Loss: 0.4698
Epoch: 017/130 | Batch 0150/0313 | Loss: 0.5136
Epoch: 017/130 | Batch 0200/0313 | Loss: 0.6340
Epoch: 017/130 | Batch 0250/0313 | Loss: 0.7673
Epoch: 017/130 | Batch 0300/0313 | Loss: 0.5554
**Epoch: 017/130 | Train. Acc.: 77.070% | Loss: 0.6721
**Epoch: 017/130 | Valid. Acc.: 80.150% | Loss: 0.5958
**Validation loss decreased (0.614023 --> 0.595813). Saving model ...
Time elapsed: 10.01 min
Epoch: 018/130 | Current Learning Rate: 0.100000
Epoch: 018/130 | Batch 0000/0313 | Loss: 0.5695
Epoch: 018/130 | Batch 0050/0313 | Loss: 0.5719
Epoch: 018/130 | Batch 0100/0313 | Loss: 0.5078
Epoch: 018/130 | Batch 0150/0313 | Loss: 0.5135
Epoch: 018/130 | Batch 0200/0313 | Loss: 0.6820
Epoch: 018/130 | Batch 0250/0313 | Loss: 0.5265
Epoch: 018/130 | Batch 0300/0313 | Loss: 0.5875
**Epoch: 018/130 | Train. Acc.: 75.495% | Loss: 0.7082
**Epoch: 018/130 | Valid. Acc.: 77.190% | Loss: 0.6599
Time elapsed: 10.60 min
Epoch: 019/130 | Current Learning Rate: 0.100000
Epoch: 019/130 | Batch 0000/0313 | Loss: 0.6443
Epoch: 019/130 | Batch 0050/0313 | Loss: 0.5247
Epoch: 019/130 | Batch 0100/0313 | Loss: 0.6827
Epoch: 019/130 | Batch 0150/0313 | Loss: 0.4749
Epoch: 019/130 | Batch 0200/0313 | Loss: 0.6542
Epoch: 019/130 | Batch 0250/0313 | Loss: 0.7369
Epoch: 019/130 | Batch 0300/0313 | Loss: 0.4943
**Epoch: 019/130 | Train. Acc.: 73.225% | Loss: 0.8052
**Epoch: 019/130 | Valid. Acc.: 77.300% | Loss: 0.7031
Time elapsed: 11.18 min
Epoch: 020/130 | Current Learning Rate: 0.100000
Epoch: 020/130 | Batch 0000/0313 | Loss: 0.5832
Epoch: 020/130 | Batch 0050/0313 | Loss: 0.4100
Epoch: 020/130 | Batch 0100/0313 | Loss: 0.6947
Epoch: 020/130 | Batch 0150/0313 | Loss: 0.4378
Epoch: 020/130 | Batch 0200/0313 | Loss: 0.6198
Epoch: 020/130 | Batch 0250/0313 | Loss: 0.5665
Epoch: 020/130 | Batch 0300/0313 | Loss: 0.5047
**Epoch: 020/130 | Train. Acc.: 78.373% | Loss: 0.6223
**Epoch: 020/130 | Valid. Acc.: 80.770% | Loss: 0.5674
**Validation loss decreased (0.595813 --> 0.567448). Saving model ...
Time elapsed: 11.78 min
Epoch: 021/130 | Current Learning Rate: 0.100000
Epoch: 021/130 | Batch 0000/0313 | Loss: 0.4873
Epoch: 021/130 | Batch 0050/0313 | Loss: 0.5849
Epoch: 021/130 | Batch 0100/0313 | Loss: 0.5293
Epoch: 021/130 | Batch 0150/0313 | Loss: 0.5743
Epoch: 021/130 | Batch 0200/0313 | Loss: 0.8584
Epoch: 021/130 | Batch 0250/0313 | Loss: 0.7206
Epoch: 021/130 | Batch 0300/0313 | Loss: 0.5487
**Epoch: 021/130 | Train. Acc.: 60.133% | Loss: 1.4589
**Epoch: 021/130 | Valid. Acc.: 66.060% | Loss: 1.2756
Time elapsed: 12.37 min
Epoch: 022/130 | Current Learning Rate: 0.100000
Epoch: 022/130 | Batch 0000/0313 | Loss: 0.5128
Epoch: 022/130 | Batch 0050/0313 | Loss: 0.6430
Epoch: 022/130 | Batch 0100/0313 | Loss: 0.5523
Epoch: 022/130 | Batch 0150/0313 | Loss: 0.6260
Epoch: 022/130 | Batch 0200/0313 | Loss: 0.4041
Epoch: 022/130 | Batch 0250/0313 | Loss: 0.5914
Epoch: 022/130 | Batch 0300/0313 | Loss: 0.4618
**Epoch: 022/130 | Train. Acc.: 74.608% | Loss: 0.7518
**Epoch: 022/130 | Valid. Acc.: 77.480% | Loss: 0.6742
Time elapsed: 12.95 min
Epoch: 023/130 | Current Learning Rate: 0.100000
Epoch: 023/130 | Batch 0000/0313 | Loss: 0.6749
Epoch: 023/130 | Batch 0050/0313 | Loss: 0.6119
Epoch: 023/130 | Batch 0100/0313 | Loss: 0.7639
Epoch: 023/130 | Batch 0150/0313 | Loss: 0.5198
Epoch: 023/130 | Batch 0200/0313 | Loss: 0.4835
Epoch: 023/130 | Batch 0250/0313 | Loss: 0.5213
Epoch: 023/130 | Batch 0300/0313 | Loss: 0.5368
**Epoch: 023/130 | Train. Acc.: 78.580% | Loss: 0.6132
**Epoch: 023/130 | Valid. Acc.: 80.210% | Loss: 0.5817
Time elapsed: 13.54 min
Epoch: 024/130 | Current Learning Rate: 0.100000
Epoch: 024/130 | Batch 0000/0313 | Loss: 0.4329
Epoch: 024/130 | Batch 0050/0313 | Loss: 0.6096
Epoch: 024/130 | Batch 0100/0313 | Loss: 0.6006
Epoch: 024/130 | Batch 0150/0313 | Loss: 0.3783
Epoch: 024/130 | Batch 0200/0313 | Loss: 0.4535
Epoch: 024/130 | Batch 0250/0313 | Loss: 0.5524
Epoch: 024/130 | Batch 0300/0313 | Loss: 0.5890
**Epoch: 024/130 | Train. Acc.: 77.850% | Loss: 0.6370
**Epoch: 024/130 | Valid. Acc.: 80.420% | Loss: 0.5935
Time elapsed: 14.13 min
Epoch: 025/130 | Current Learning Rate: 0.100000
Epoch: 025/130 | Batch 0000/0313 | Loss: 0.6444
Epoch: 025/130 | Batch 0050/0313 | Loss: 0.4096
Epoch: 025/130 | Batch 0100/0313 | Loss: 0.6294
Epoch: 025/130 | Batch 0150/0313 | Loss: 0.5132
Epoch: 025/130 | Batch 0200/0313 | Loss: 0.4169
Epoch: 025/130 | Batch 0250/0313 | Loss: 0.4638
Epoch: 025/130 | Batch 0300/0313 | Loss: 0.5112
**Epoch: 025/130 | Train. Acc.: 77.325% | Loss: 0.6689
**Epoch: 025/130 | Valid. Acc.: 81.370% | Loss: 0.5714
Time elapsed: 14.71 min
Epoch: 026/130 | Current Learning Rate: 0.100000
Epoch: 026/130 | Batch 0000/0313 | Loss: 0.5314
Epoch: 026/130 | Batch 0050/0313 | Loss: 0.6060
Epoch: 026/130 | Batch 0100/0313 | Loss: 0.6530
Epoch: 026/130 | Batch 0150/0313 | Loss: 0.5239
Epoch: 026/130 | Batch 0200/0313 | Loss: 0.3952
Epoch: 026/130 | Batch 0250/0313 | Loss: 0.4670
Epoch: 026/130 | Batch 0300/0313 | Loss: 0.3731
**Epoch: 026/130 | Train. Acc.: 74.623% | Loss: 0.7377
**Epoch: 026/130 | Valid. Acc.: 78.470% | Loss: 0.6352
Time elapsed: 15.30 min
Epoch: 027/130 | Current Learning Rate: 0.100000
Epoch: 027/130 | Batch 0000/0313 | Loss: 0.4735
Epoch: 027/130 | Batch 0050/0313 | Loss: 0.5842
Epoch: 027/130 | Batch 0100/0313 | Loss: 0.5838
Epoch: 027/130 | Batch 0150/0313 | Loss: 0.4381
Epoch: 027/130 | Batch 0200/0313 | Loss: 0.4213
Epoch: 027/130 | Batch 0250/0313 | Loss: 0.7059
Epoch: 027/130 | Batch 0300/0313 | Loss: 0.5971
**Epoch: 027/130 | Train. Acc.: 75.147% | Loss: 0.7507
**Epoch: 027/130 | Valid. Acc.: 77.760% | Loss: 0.6991
Epoch 00027: reducing learning rate of group 0 to 5.0000e-02.
Time elapsed: 15.89 min
Epoch: 028/130 | Current Learning Rate: 0.050000
Epoch: 028/130 | Batch 0000/0313 | Loss: 0.5568
Epoch: 028/130 | Batch 0050/0313 | Loss: 0.3873
Epoch: 028/130 | Batch 0100/0313 | Loss: 0.3615
Epoch: 028/130 | Batch 0150/0313 | Loss: 0.4203
Epoch: 028/130 | Batch 0200/0313 | Loss: 0.3350
Epoch: 028/130 | Batch 0250/0313 | Loss: 0.3516
Epoch: 028/130 | Batch 0300/0313 | Loss: 0.3516
**Epoch: 028/130 | Train. Acc.: 86.620% | Loss: 0.3855
**Epoch: 028/130 | Valid. Acc.: 87.630% | Loss: 0.3795
**Validation loss decreased (0.567448 --> 0.379481). Saving model ...
Time elapsed: 16.47 min
Epoch: 029/130 | Current Learning Rate: 0.050000
Epoch: 029/130 | Batch 0000/0313 | Loss: 0.3273
Epoch: 029/130 | Batch 0050/0313 | Loss: 0.4521
Epoch: 029/130 | Batch 0100/0313 | Loss: 0.3639
Epoch: 029/130 | Batch 0150/0313 | Loss: 0.3600
Epoch: 029/130 | Batch 0200/0313 | Loss: 0.3977
Epoch: 029/130 | Batch 0250/0313 | Loss: 0.3662
Epoch: 029/130 | Batch 0300/0313 | Loss: 0.4454
**Epoch: 029/130 | Train. Acc.: 86.722% | Loss: 0.3853
**Epoch: 029/130 | Valid. Acc.: 86.830% | Loss: 0.3844
Time elapsed: 17.07 min
Epoch: 030/130 | Current Learning Rate: 0.050000
Epoch: 030/130 | Batch 0000/0313 | Loss: 0.3107
Epoch: 030/130 | Batch 0050/0313 | Loss: 0.3443
Epoch: 030/130 | Batch 0100/0313 | Loss: 0.3165
Epoch: 030/130 | Batch 0150/0313 | Loss: 0.4669
Epoch: 030/130 | Batch 0200/0313 | Loss: 0.3794
Epoch: 030/130 | Batch 0250/0313 | Loss: 0.3536
Epoch: 030/130 | Batch 0300/0313 | Loss: 0.1960
**Epoch: 030/130 | Train. Acc.: 86.115% | Loss: 0.3972
**Epoch: 030/130 | Valid. Acc.: 86.760% | Loss: 0.3998
Time elapsed: 17.66 min
Epoch: 031/130 | Current Learning Rate: 0.050000
Epoch: 031/130 | Batch 0000/0313 | Loss: 0.2793
Epoch: 031/130 | Batch 0050/0313 | Loss: 0.2450
Epoch: 031/130 | Batch 0100/0313 | Loss: 0.4467
Epoch: 031/130 | Batch 0150/0313 | Loss: 0.3817
Epoch: 031/130 | Batch 0200/0313 | Loss: 0.3873
Epoch: 031/130 | Batch 0250/0313 | Loss: 0.4475
Epoch: 031/130 | Batch 0300/0313 | Loss: 0.3891
**Epoch: 031/130 | Train. Acc.: 85.125% | Loss: 0.4297
**Epoch: 031/130 | Valid. Acc.: 86.310% | Loss: 0.4203
Time elapsed: 18.24 min
Epoch: 032/130 | Current Learning Rate: 0.050000
Epoch: 032/130 | Batch 0000/0313 | Loss: 0.2758
Epoch: 032/130 | Batch 0050/0313 | Loss: 0.3072
Epoch: 032/130 | Batch 0100/0313 | Loss: 0.2943
Epoch: 032/130 | Batch 0150/0313 | Loss: 0.2665
Epoch: 032/130 | Batch 0200/0313 | Loss: 0.4664
Epoch: 032/130 | Batch 0250/0313 | Loss: 0.5236
Epoch: 032/130 | Batch 0300/0313 | Loss: 0.3927
**Epoch: 032/130 | Train. Acc.: 84.640% | Loss: 0.4399
**Epoch: 032/130 | Valid. Acc.: 84.550% | Loss: 0.4710
Time elapsed: 18.82 min
Epoch: 033/130 | Current Learning Rate: 0.050000
Epoch: 033/130 | Batch 0000/0313 | Loss: 0.3597
Epoch: 033/130 | Batch 0050/0313 | Loss: 0.3826
Epoch: 033/130 | Batch 0100/0313 | Loss: 0.3617
Epoch: 033/130 | Batch 0150/0313 | Loss: 0.3168
Epoch: 033/130 | Batch 0200/0313 | Loss: 0.4091
Epoch: 033/130 | Batch 0250/0313 | Loss: 0.4100
Epoch: 033/130 | Batch 0300/0313 | Loss: 0.4426
**Epoch: 033/130 | Train. Acc.: 86.415% | Loss: 0.3904
**Epoch: 033/130 | Valid. Acc.: 87.230% | Loss: 0.3751
**Validation loss decreased (0.379481 --> 0.375146). Saving model ...
Time elapsed: 19.42 min
Epoch: 034/130 | Current Learning Rate: 0.050000
Epoch: 034/130 | Batch 0000/0313 | Loss: 0.4252
Epoch: 034/130 | Batch 0050/0313 | Loss: 0.3857
Epoch: 034/130 | Batch 0100/0313 | Loss: 0.3014
Epoch: 034/130 | Batch 0150/0313 | Loss: 0.3065
Epoch: 034/130 | Batch 0200/0313 | Loss: 0.3418
Epoch: 034/130 | Batch 0250/0313 | Loss: 0.5596
Epoch: 034/130 | Batch 0300/0313 | Loss: 0.3716
**Epoch: 034/130 | Train. Acc.: 86.248% | Loss: 0.4022
**Epoch: 034/130 | Valid. Acc.: 86.620% | Loss: 0.4028
Time elapsed: 20.01 min
Epoch: 035/130 | Current Learning Rate: 0.050000
Epoch: 035/130 | Batch 0000/0313 | Loss: 0.4093
Epoch: 035/130 | Batch 0050/0313 | Loss: 0.3878
Epoch: 035/130 | Batch 0100/0313 | Loss: 0.4138
Epoch: 035/130 | Batch 0150/0313 | Loss: 0.3818
Epoch: 035/130 | Batch 0200/0313 | Loss: 0.3681
Epoch: 035/130 | Batch 0250/0313 | Loss: 0.3672
Epoch: 035/130 | Batch 0300/0313 | Loss: 0.3922
**Epoch: 035/130 | Train. Acc.: 82.810% | Loss: 0.4971
**Epoch: 035/130 | Valid. Acc.: 83.860% | Loss: 0.4741
Time elapsed: 20.60 min
Epoch: 036/130 | Current Learning Rate: 0.050000
Epoch: 036/130 | Batch 0000/0313 | Loss: 0.5381
Epoch: 036/130 | Batch 0050/0313 | Loss: 0.2851
Epoch: 036/130 | Batch 0100/0313 | Loss: 0.2637
Epoch: 036/130 | Batch 0150/0313 | Loss: 0.4182
Epoch: 036/130 | Batch 0200/0313 | Loss: 0.4432
Epoch: 036/130 | Batch 0250/0313 | Loss: 0.3679
Epoch: 036/130 | Batch 0300/0313 | Loss: 0.4687
**Epoch: 036/130 | Train. Acc.: 83.265% | Loss: 0.4834
**Epoch: 036/130 | Valid. Acc.: 84.120% | Loss: 0.4819
Time elapsed: 21.18 min
Epoch: 037/130 | Current Learning Rate: 0.050000
Epoch: 037/130 | Batch 0000/0313 | Loss: 0.3937
Epoch: 037/130 | Batch 0050/0313 | Loss: 0.3617
Epoch: 037/130 | Batch 0100/0313 | Loss: 0.4470
Epoch: 037/130 | Batch 0150/0313 | Loss: 0.3385
Epoch: 037/130 | Batch 0200/0313 | Loss: 0.3550
Epoch: 037/130 | Batch 0250/0313 | Loss: 0.4808
Epoch: 037/130 | Batch 0300/0313 | Loss: 0.3118
**Epoch: 037/130 | Train. Acc.: 84.977% | Loss: 0.4370
**Epoch: 037/130 | Valid. Acc.: 86.750% | Loss: 0.3944
Time elapsed: 21.77 min
Epoch: 038/130 | Current Learning Rate: 0.050000
Epoch: 038/130 | Batch 0000/0313 | Loss: 0.3555
Epoch: 038/130 | Batch 0050/0313 | Loss: 0.4155
Epoch: 038/130 | Batch 0100/0313 | Loss: 0.4166
Epoch: 038/130 | Batch 0150/0313 | Loss: 0.4375
Epoch: 038/130 | Batch 0200/0313 | Loss: 0.3134
Epoch: 038/130 | Batch 0250/0313 | Loss: 0.3720
Epoch: 038/130 | Batch 0300/0313 | Loss: 0.3414
**Epoch: 038/130 | Train. Acc.: 87.708% | Loss: 0.3564
**Epoch: 038/130 | Valid. Acc.: 88.180% | Loss: 0.3501
**Validation loss decreased (0.375146 --> 0.350101). Saving model ...
Time elapsed: 22.35 min
Epoch: 039/130 | Current Learning Rate: 0.050000
Epoch: 039/130 | Batch 0000/0313 | Loss: 0.2045
Epoch: 039/130 | Batch 0050/0313 | Loss: 0.2367
Epoch: 039/130 | Batch 0100/0313 | Loss: 0.3148
Epoch: 039/130 | Batch 0150/0313 | Loss: 0.4068
Epoch: 039/130 | Batch 0200/0313 | Loss: 0.1924
Epoch: 039/130 | Batch 0250/0313 | Loss: 0.3943
Epoch: 039/130 | Batch 0300/0313 | Loss: 0.4187
**Epoch: 039/130 | Train. Acc.: 85.745% | Loss: 0.4143
**Epoch: 039/130 | Valid. Acc.: 86.990% | Loss: 0.3992
Time elapsed: 22.94 min
Epoch: 040/130 | Current Learning Rate: 0.050000
Epoch: 040/130 | Batch 0000/0313 | Loss: 0.3057
Epoch: 040/130 | Batch 0050/0313 | Loss: 0.3305
Epoch: 040/130 | Batch 0100/0313 | Loss: 0.5177
Epoch: 040/130 | Batch 0150/0313 | Loss: 0.3330
Epoch: 040/130 | Batch 0200/0313 | Loss: 0.3346
Epoch: 040/130 | Batch 0250/0313 | Loss: 0.3894
Epoch: 040/130 | Batch 0300/0313 | Loss: 0.2996
**Epoch: 040/130 | Train. Acc.: 86.087% | Loss: 0.4019
**Epoch: 040/130 | Valid. Acc.: 86.190% | Loss: 0.4239
Time elapsed: 23.53 min
Epoch: 041/130 | Current Learning Rate: 0.050000
Epoch: 041/130 | Batch 0000/0313 | Loss: 0.2750
Epoch: 041/130 | Batch 0050/0313 | Loss: 0.3054
Epoch: 041/130 | Batch 0100/0313 | Loss: 0.5006
Epoch: 041/130 | Batch 0150/0313 | Loss: 0.3132
Epoch: 041/130 | Batch 0200/0313 | Loss: 0.3853
Epoch: 041/130 | Batch 0250/0313 | Loss: 0.3759
Epoch: 041/130 | Batch 0300/0313 | Loss: 0.2346
**Epoch: 041/130 | Train. Acc.: 85.685% | Loss: 0.4048
**Epoch: 041/130 | Valid. Acc.: 85.910% | Loss: 0.4125
Time elapsed: 24.12 min
Epoch: 042/130 | Current Learning Rate: 0.050000
Epoch: 042/130 | Batch 0000/0313 | Loss: 0.2773
Epoch: 042/130 | Batch 0050/0313 | Loss: 0.4234
Epoch: 042/130 | Batch 0100/0313 | Loss: 0.3266
Epoch: 042/130 | Batch 0150/0313 | Loss: 0.3438
Epoch: 042/130 | Batch 0200/0313 | Loss: 0.4218
Epoch: 042/130 | Batch 0250/0313 | Loss: 0.3851
Epoch: 042/130 | Batch 0300/0313 | Loss: 0.3881
**Epoch: 042/130 | Train. Acc.: 85.638% | Loss: 0.4085
**Epoch: 042/130 | Valid. Acc.: 86.810% | Loss: 0.4015
Time elapsed: 24.70 min
Epoch: 043/130 | Current Learning Rate: 0.050000
Epoch: 043/130 | Batch 0000/0313 | Loss: 0.4488
Epoch: 043/130 | Batch 0050/0313 | Loss: 0.1912
Epoch: 043/130 | Batch 0100/0313 | Loss: 0.3581
Epoch: 043/130 | Batch 0150/0313 | Loss: 0.4115
Epoch: 043/130 | Batch 0200/0313 | Loss: 0.3571
Epoch: 043/130 | Batch 0250/0313 | Loss: 0.2718
Epoch: 043/130 | Batch 0300/0313 | Loss: 0.3163
**Epoch: 043/130 | Train. Acc.: 84.740% | Loss: 0.4456
**Epoch: 043/130 | Valid. Acc.: 86.070% | Loss: 0.4223
Time elapsed: 25.29 min
Epoch: 044/130 | Current Learning Rate: 0.050000
Epoch: 044/130 | Batch 0000/0313 | Loss: 0.3922
Epoch: 044/130 | Batch 0050/0313 | Loss: 0.2907
Epoch: 044/130 | Batch 0100/0313 | Loss: 0.4865
Epoch: 044/130 | Batch 0150/0313 | Loss: 0.3202
Epoch: 044/130 | Batch 0200/0313 | Loss: 0.3193
Epoch: 044/130 | Batch 0250/0313 | Loss: 0.3677
Epoch: 044/130 | Batch 0300/0313 | Loss: 0.2485
**Epoch: 044/130 | Train. Acc.: 84.883% | Loss: 0.4445
**Epoch: 044/130 | Valid. Acc.: 86.300% | Loss: 0.4151
Time elapsed: 25.87 min
Epoch: 045/130 | Current Learning Rate: 0.050000
Epoch: 045/130 | Batch 0000/0313 | Loss: 0.4444
Epoch: 045/130 | Batch 0050/0313 | Loss: 0.3283
Epoch: 045/130 | Batch 0100/0313 | Loss: 0.3396
Epoch: 045/130 | Batch 0150/0313 | Loss: 0.3942
Epoch: 045/130 | Batch 0200/0313 | Loss: 0.3325
Epoch: 045/130 | Batch 0250/0313 | Loss: 0.2467
Epoch: 045/130 | Batch 0300/0313 | Loss: 0.2397
**Epoch: 045/130 | Train. Acc.: 85.358% | Loss: 0.4199
**Epoch: 045/130 | Valid. Acc.: 86.440% | Loss: 0.4119
Epoch 00045: reducing learning rate of group 0 to 2.5000e-02.
Time elapsed: 26.46 min
Epoch: 046/130 | Current Learning Rate: 0.025000
Epoch: 046/130 | Batch 0000/0313 | Loss: 0.3183
Epoch: 046/130 | Batch 0050/0313 | Loss: 0.2552
Epoch: 046/130 | Batch 0100/0313 | Loss: 0.1799
Epoch: 046/130 | Batch 0150/0313 | Loss: 0.1372
Epoch: 046/130 | Batch 0200/0313 | Loss: 0.2364
Epoch: 046/130 | Batch 0250/0313 | Loss: 0.2677
Epoch: 046/130 | Batch 0300/0313 | Loss: 0.1610
**Epoch: 046/130 | Train. Acc.: 91.688% | Loss: 0.2400
**Epoch: 046/130 | Valid. Acc.: 89.970% | Loss: 0.3166
**Validation loss decreased (0.350101 --> 0.316588). Saving model ...
Time elapsed: 27.05 min
Epoch: 047/130 | Current Learning Rate: 0.025000
Epoch: 047/130 | Batch 0000/0313 | Loss: 0.2000
Epoch: 047/130 | Batch 0050/0313 | Loss: 0.2026
Epoch: 047/130 | Batch 0100/0313 | Loss: 0.2138
Epoch: 047/130 | Batch 0150/0313 | Loss: 0.2273
Epoch: 047/130 | Batch 0200/0313 | Loss: 0.2805
Epoch: 047/130 | Batch 0250/0313 | Loss: 0.1666
Epoch: 047/130 | Batch 0300/0313 | Loss: 0.1934
**Epoch: 047/130 | Train. Acc.: 91.853% | Loss: 0.2370
**Epoch: 047/130 | Valid. Acc.: 89.910% | Loss: 0.3016
**Validation loss decreased (0.316588 --> 0.301553). Saving model ...
Time elapsed: 27.65 min
Epoch: 048/130 | Current Learning Rate: 0.025000
Epoch: 048/130 | Batch 0000/0313 | Loss: 0.1795
Epoch: 048/130 | Batch 0050/0313 | Loss: 0.1372
Epoch: 048/130 | Batch 0100/0313 | Loss: 0.2878
Epoch: 048/130 | Batch 0150/0313 | Loss: 0.3209
Epoch: 048/130 | Batch 0200/0313 | Loss: 0.2559
Epoch: 048/130 | Batch 0250/0313 | Loss: 0.2533
Epoch: 048/130 | Batch 0300/0313 | Loss: 0.2335
**Epoch: 048/130 | Train. Acc.: 91.608% | Loss: 0.2421
**Epoch: 048/130 | Valid. Acc.: 89.660% | Loss: 0.3168
Time elapsed: 28.24 min
Epoch: 049/130 | Current Learning Rate: 0.025000
Epoch: 049/130 | Batch 0000/0313 | Loss: 0.1942
Epoch: 049/130 | Batch 0050/0313 | Loss: 0.1800
Epoch: 049/130 | Batch 0100/0313 | Loss: 0.3058
Epoch: 049/130 | Batch 0150/0313 | Loss: 0.2236
Epoch: 049/130 | Batch 0200/0313 | Loss: 0.1627
Epoch: 049/130 | Batch 0250/0313 | Loss: 0.2173
Epoch: 049/130 | Batch 0300/0313 | Loss: 0.1669
**Epoch: 049/130 | Train. Acc.: 92.510% | Loss: 0.2213
**Epoch: 049/130 | Valid. Acc.: 90.970% | Loss: 0.2853
**Validation loss decreased (0.301553 --> 0.285325). Saving model ...
Time elapsed: 28.83 min
Epoch: 050/130 | Current Learning Rate: 0.025000
Epoch: 050/130 | Batch 0000/0313 | Loss: 0.2277
Epoch: 050/130 | Batch 0050/0313 | Loss: 0.2141
Epoch: 050/130 | Batch 0100/0313 | Loss: 0.3069
Epoch: 050/130 | Batch 0150/0313 | Loss: 0.2260
Epoch: 050/130 | Batch 0200/0313 | Loss: 0.1306
Epoch: 050/130 | Batch 0250/0313 | Loss: 0.3545
Epoch: 050/130 | Batch 0300/0313 | Loss: 0.2446
**Epoch: 050/130 | Train. Acc.: 92.343% | Loss: 0.2154
**Epoch: 050/130 | Valid. Acc.: 89.910% | Loss: 0.3198
Time elapsed: 29.42 min
Epoch: 051/130 | Current Learning Rate: 0.025000
Epoch: 051/130 | Batch 0000/0313 | Loss: 0.1946
Epoch: 051/130 | Batch 0050/0313 | Loss: 0.1675
Epoch: 051/130 | Batch 0100/0313 | Loss: 0.2414
Epoch: 051/130 | Batch 0150/0313 | Loss: 0.1423
Epoch: 051/130 | Batch 0200/0313 | Loss: 0.1787
Epoch: 051/130 | Batch 0250/0313 | Loss: 0.2858
Epoch: 051/130 | Batch 0300/0313 | Loss: 0.2142
**Epoch: 051/130 | Train. Acc.: 92.438% | Loss: 0.2179
**Epoch: 051/130 | Valid. Acc.: 90.110% | Loss: 0.3190
Time elapsed: 30.00 min
Epoch: 052/130 | Current Learning Rate: 0.025000
Epoch: 052/130 | Batch 0000/0313 | Loss: 0.2576
Epoch: 052/130 | Batch 0050/0313 | Loss: 0.1512
Epoch: 052/130 | Batch 0100/0313 | Loss: 0.2360
Epoch: 052/130 | Batch 0150/0313 | Loss: 0.2544
Epoch: 052/130 | Batch 0200/0313 | Loss: 0.3230
Epoch: 052/130 | Batch 0250/0313 | Loss: 0.2772
Epoch: 052/130 | Batch 0300/0313 | Loss: 0.2209
**Epoch: 052/130 | Train. Acc.: 92.200% | Loss: 0.2262
**Epoch: 052/130 | Valid. Acc.: 89.800% | Loss: 0.3161
Time elapsed: 30.59 min
Epoch: 053/130 | Current Learning Rate: 0.025000
Epoch: 053/130 | Batch 0000/0313 | Loss: 0.1293
Epoch: 053/130 | Batch 0050/0313 | Loss: 0.2482
Epoch: 053/130 | Batch 0100/0313 | Loss: 0.1553
Epoch: 053/130 | Batch 0150/0313 | Loss: 0.1519
Epoch: 053/130 | Batch 0200/0313 | Loss: 0.2720
Epoch: 053/130 | Batch 0250/0313 | Loss: 0.2761
Epoch: 053/130 | Batch 0300/0313 | Loss: 0.1577
**Epoch: 053/130 | Train. Acc.: 93.215% | Loss: 0.1963
**Epoch: 053/130 | Valid. Acc.: 90.640% | Loss: 0.3062
Time elapsed: 31.17 min
Epoch: 054/130 | Current Learning Rate: 0.025000
Epoch: 054/130 | Batch 0000/0313 | Loss: 0.1501
Epoch: 054/130 | Batch 0050/0313 | Loss: 0.1160
Epoch: 054/130 | Batch 0100/0313 | Loss: 0.1508
Epoch: 054/130 | Batch 0150/0313 | Loss: 0.3227
Epoch: 054/130 | Batch 0200/0313 | Loss: 0.2307
Epoch: 054/130 | Batch 0250/0313 | Loss: 0.2930
Epoch: 054/130 | Batch 0300/0313 | Loss: 0.2115
**Epoch: 054/130 | Train. Acc.: 92.865% | Loss: 0.2025
**Epoch: 054/130 | Valid. Acc.: 90.790% | Loss: 0.2815
**Validation loss decreased (0.285325 --> 0.281462). Saving model ...
Time elapsed: 31.76 min
Epoch: 055/130 | Current Learning Rate: 0.025000
Epoch: 055/130 | Batch 0000/0313 | Loss: 0.1502
Epoch: 055/130 | Batch 0050/0313 | Loss: 0.1665
Epoch: 055/130 | Batch 0100/0313 | Loss: 0.2081
Epoch: 055/130 | Batch 0150/0313 | Loss: 0.1521
Epoch: 055/130 | Batch 0200/0313 | Loss: 0.2176
Epoch: 055/130 | Batch 0250/0313 | Loss: 0.2118
Epoch: 055/130 | Batch 0300/0313 | Loss: 0.2136
**Epoch: 055/130 | Train. Acc.: 93.445% | Loss: 0.1888
**Epoch: 055/130 | Valid. Acc.: 90.890% | Loss: 0.2887
Time elapsed: 32.35 min
Epoch: 056/130 | Current Learning Rate: 0.025000
Epoch: 056/130 | Batch 0000/0313 | Loss: 0.1367
Epoch: 056/130 | Batch 0050/0313 | Loss: 0.3043
Epoch: 056/130 | Batch 0100/0313 | Loss: 0.2593
Epoch: 056/130 | Batch 0150/0313 | Loss: 0.1425
Epoch: 056/130 | Batch 0200/0313 | Loss: 0.2051
Epoch: 056/130 | Batch 0250/0313 | Loss: 0.2155
Epoch: 056/130 | Batch 0300/0313 | Loss: 0.3710
**Epoch: 056/130 | Train. Acc.: 92.623% | Loss: 0.2140
**Epoch: 056/130 | Valid. Acc.: 90.170% | Loss: 0.2989
Time elapsed: 32.93 min
Epoch: 057/130 | Current Learning Rate: 0.025000
Epoch: 057/130 | Batch 0000/0313 | Loss: 0.1537
Epoch: 057/130 | Batch 0050/0313 | Loss: 0.2769
Epoch: 057/130 | Batch 0100/0313 | Loss: 0.3172
Epoch: 057/130 | Batch 0150/0313 | Loss: 0.2538
Epoch: 057/130 | Batch 0200/0313 | Loss: 0.1728
Epoch: 057/130 | Batch 0250/0313 | Loss: 0.1773
Epoch: 057/130 | Batch 0300/0313 | Loss: 0.2996
**Epoch: 057/130 | Train. Acc.: 93.123% | Loss: 0.1958
**Epoch: 057/130 | Valid. Acc.: 90.800% | Loss: 0.2865
Time elapsed: 33.51 min
Epoch: 058/130 | Current Learning Rate: 0.025000
Epoch: 058/130 | Batch 0000/0313 | Loss: 0.2938
Epoch: 058/130 | Batch 0050/0313 | Loss: 0.2086
Epoch: 058/130 | Batch 0100/0313 | Loss: 0.1686
Epoch: 058/130 | Batch 0150/0313 | Loss: 0.2462
Epoch: 058/130 | Batch 0200/0313 | Loss: 0.1275
Epoch: 058/130 | Batch 0250/0313 | Loss: 0.2363
Epoch: 058/130 | Batch 0300/0313 | Loss: 0.1791
**Epoch: 058/130 | Train. Acc.: 92.835% | Loss: 0.2047
**Epoch: 058/130 | Valid. Acc.: 90.800% | Loss: 0.2959
Time elapsed: 34.10 min
Epoch: 059/130 | Current Learning Rate: 0.025000
Epoch: 059/130 | Batch 0000/0313 | Loss: 0.1090
Epoch: 059/130 | Batch 0050/0313 | Loss: 0.2194
Epoch: 059/130 | Batch 0100/0313 | Loss: 0.2297
Epoch: 059/130 | Batch 0150/0313 | Loss: 0.4116
Epoch: 059/130 | Batch 0200/0313 | Loss: 0.2797
Epoch: 059/130 | Batch 0250/0313 | Loss: 0.2288
Epoch: 059/130 | Batch 0300/0313 | Loss: 0.1924
**Epoch: 059/130 | Train. Acc.: 91.443% | Loss: 0.2527
**Epoch: 059/130 | Valid. Acc.: 89.340% | Loss: 0.3284
Time elapsed: 34.68 min
Epoch: 060/130 | Current Learning Rate: 0.025000
Epoch: 060/130 | Batch 0000/0313 | Loss: 0.2073
Epoch: 060/130 | Batch 0050/0313 | Loss: 0.1830
Epoch: 060/130 | Batch 0100/0313 | Loss: 0.1994
Epoch: 060/130 | Batch 0150/0313 | Loss: 0.3357
Epoch: 060/130 | Batch 0200/0313 | Loss: 0.3138
Epoch: 060/130 | Batch 0250/0313 | Loss: 0.2721
Epoch: 060/130 | Batch 0300/0313 | Loss: 0.2527
**Epoch: 060/130 | Train. Acc.: 91.320% | Loss: 0.2547
**Epoch: 060/130 | Valid. Acc.: 89.670% | Loss: 0.3199
Time elapsed: 35.27 min
Epoch: 061/130 | Current Learning Rate: 0.025000
Epoch: 061/130 | Batch 0000/0313 | Loss: 0.2039
Epoch: 061/130 | Batch 0050/0313 | Loss: 0.1391
Epoch: 061/130 | Batch 0100/0313 | Loss: 0.1662
Epoch: 061/130 | Batch 0150/0313 | Loss: 0.2434
Epoch: 061/130 | Batch 0200/0313 | Loss: 0.1746
Epoch: 061/130 | Batch 0250/0313 | Loss: 0.1743
Epoch: 061/130 | Batch 0300/0313 | Loss: 0.1958
**Epoch: 061/130 | Train. Acc.: 90.360% | Loss: 0.2887
**Epoch: 061/130 | Valid. Acc.: 89.290% | Loss: 0.3592
Epoch 00061: reducing learning rate of group 0 to 1.2500e-02.
Time elapsed: 35.86 min
Epoch: 062/130 | Current Learning Rate: 0.012500
Epoch: 062/130 | Batch 0000/0313 | Loss: 0.2020
Epoch: 062/130 | Batch 0050/0313 | Loss: 0.1808
Epoch: 062/130 | Batch 0100/0313 | Loss: 0.1482
Epoch: 062/130 | Batch 0150/0313 | Loss: 0.1397
Epoch: 062/130 | Batch 0200/0313 | Loss: 0.1050
Epoch: 062/130 | Batch 0250/0313 | Loss: 0.0932
Epoch: 062/130 | Batch 0300/0313 | Loss: 0.1276
**Epoch: 062/130 | Train. Acc.: 96.088% | Loss: 0.1155
**Epoch: 062/130 | Valid. Acc.: 92.380% | Loss: 0.2492
**Validation loss decreased (0.281462 --> 0.249161). Saving model ...
Time elapsed: 36.45 min
Epoch: 063/130 | Current Learning Rate: 0.012500
Epoch: 063/130 | Batch 0000/0313 | Loss: 0.1192
Epoch: 063/130 | Batch 0050/0313 | Loss: 0.1735
Epoch: 063/130 | Batch 0100/0313 | Loss: 0.1756
Epoch: 063/130 | Batch 0150/0313 | Loss: 0.1349
Epoch: 063/130 | Batch 0200/0313 | Loss: 0.1618
Epoch: 063/130 | Batch 0250/0313 | Loss: 0.0593
Epoch: 063/130 | Batch 0300/0313 | Loss: 0.0822
**Epoch: 063/130 | Train. Acc.: 95.810% | Loss: 0.1186
**Epoch: 063/130 | Valid. Acc.: 92.240% | Loss: 0.2690
Time elapsed: 37.03 min
Epoch: 064/130 | Current Learning Rate: 0.012500
Epoch: 064/130 | Batch 0000/0313 | Loss: 0.0911
Epoch: 064/130 | Batch 0050/0313 | Loss: 0.1012
Epoch: 064/130 | Batch 0100/0313 | Loss: 0.1695
Epoch: 064/130 | Batch 0150/0313 | Loss: 0.1633
Epoch: 064/130 | Batch 0200/0313 | Loss: 0.1137
Epoch: 064/130 | Batch 0250/0313 | Loss: 0.0458
Epoch: 064/130 | Batch 0300/0313 | Loss: 0.0500
**Epoch: 064/130 | Train. Acc.: 96.358% | Loss: 0.1041
**Epoch: 064/130 | Valid. Acc.: 92.480% | Loss: 0.2520
Time elapsed: 37.62 min
Epoch: 065/130 | Current Learning Rate: 0.012500
Epoch: 065/130 | Batch 0000/0313 | Loss: 0.0539
Epoch: 065/130 | Batch 0050/0313 | Loss: 0.0908
Epoch: 065/130 | Batch 0100/0313 | Loss: 0.1070
Epoch: 065/130 | Batch 0150/0313 | Loss: 0.1780
Epoch: 065/130 | Batch 0200/0313 | Loss: 0.0961
Epoch: 065/130 | Batch 0250/0313 | Loss: 0.2213
Epoch: 065/130 | Batch 0300/0313 | Loss: 0.0762
**Epoch: 065/130 | Train. Acc.: 95.855% | Loss: 0.1188
**Epoch: 065/130 | Valid. Acc.: 91.700% | Loss: 0.2750
Time elapsed: 38.19 min
Epoch: 066/130 | Current Learning Rate: 0.012500
Epoch: 066/130 | Batch 0000/0313 | Loss: 0.1379
Epoch: 066/130 | Batch 0050/0313 | Loss: 0.0775
Epoch: 066/130 | Batch 0100/0313 | Loss: 0.0624
Epoch: 066/130 | Batch 0150/0313 | Loss: 0.1267
Epoch: 066/130 | Batch 0200/0313 | Loss: 0.0877
Epoch: 066/130 | Batch 0250/0313 | Loss: 0.1548
Epoch: 066/130 | Batch 0300/0313 | Loss: 0.1339
**Epoch: 066/130 | Train. Acc.: 95.757% | Loss: 0.1234
**Epoch: 066/130 | Valid. Acc.: 91.800% | Loss: 0.2863
Time elapsed: 38.78 min
Epoch: 067/130 | Current Learning Rate: 0.012500
Epoch: 067/130 | Batch 0000/0313 | Loss: 0.0729
Epoch: 067/130 | Batch 0050/0313 | Loss: 0.1895
Epoch: 067/130 | Batch 0100/0313 | Loss: 0.0521
Epoch: 067/130 | Batch 0150/0313 | Loss: 0.1705
Epoch: 067/130 | Batch 0200/0313 | Loss: 0.0773
Epoch: 067/130 | Batch 0250/0313 | Loss: 0.0868
Epoch: 067/130 | Batch 0300/0313 | Loss: 0.0874
**Epoch: 067/130 | Train. Acc.: 96.940% | Loss: 0.0909
**Epoch: 067/130 | Valid. Acc.: 92.760% | Loss: 0.2417
**Validation loss decreased (0.249161 --> 0.241701). Saving model ...
Time elapsed: 39.37 min
Epoch: 068/130 | Current Learning Rate: 0.012500
Epoch: 068/130 | Batch 0000/0313 | Loss: 0.1027
Epoch: 068/130 | Batch 0050/0313 | Loss: 0.0846
Epoch: 068/130 | Batch 0100/0313 | Loss: 0.1144
Epoch: 068/130 | Batch 0150/0313 | Loss: 0.0907
Epoch: 068/130 | Batch 0200/0313 | Loss: 0.0561
Epoch: 068/130 | Batch 0250/0313 | Loss: 0.0638
Epoch: 068/130 | Batch 0300/0313 | Loss: 0.1356
**Epoch: 068/130 | Train. Acc.: 96.188% | Loss: 0.1110
**Epoch: 068/130 | Valid. Acc.: 92.000% | Loss: 0.2803
Time elapsed: 39.95 min
Epoch: 069/130 | Current Learning Rate: 0.012500
Epoch: 069/130 | Batch 0000/0313 | Loss: 0.0743
Epoch: 069/130 | Batch 0050/0313 | Loss: 0.0767
Epoch: 069/130 | Batch 0100/0313 | Loss: 0.0370
Epoch: 069/130 | Batch 0150/0313 | Loss: 0.2234
Epoch: 069/130 | Batch 0200/0313 | Loss: 0.1040
Epoch: 069/130 | Batch 0250/0313 | Loss: 0.0933
Epoch: 069/130 | Batch 0300/0313 | Loss: 0.0473
**Epoch: 069/130 | Train. Acc.: 96.693% | Loss: 0.0971
**Epoch: 069/130 | Valid. Acc.: 92.020% | Loss: 0.2785
Time elapsed: 40.54 min
Epoch: 070/130 | Current Learning Rate: 0.012500
Epoch: 070/130 | Batch 0000/0313 | Loss: 0.0585
Epoch: 070/130 | Batch 0050/0313 | Loss: 0.0511
Epoch: 070/130 | Batch 0100/0313 | Loss: 0.1525
Epoch: 070/130 | Batch 0150/0313 | Loss: 0.1216
Epoch: 070/130 | Batch 0200/0313 | Loss: 0.1017
Epoch: 070/130 | Batch 0250/0313 | Loss: 0.0715
Epoch: 070/130 | Batch 0300/0313 | Loss: 0.0773
**Epoch: 070/130 | Train. Acc.: 97.240% | Loss: 0.0809
**Epoch: 070/130 | Valid. Acc.: 92.940% | Loss: 0.2369
**Validation loss decreased (0.241701 --> 0.236859). Saving model ...
Time elapsed: 41.13 min
Epoch: 071/130 | Current Learning Rate: 0.012500
Epoch: 071/130 | Batch 0000/0313 | Loss: 0.0724
Epoch: 071/130 | Batch 0050/0313 | Loss: 0.1109
Epoch: 071/130 | Batch 0100/0313 | Loss: 0.0938
Epoch: 071/130 | Batch 0150/0313 | Loss: 0.1832
Epoch: 071/130 | Batch 0200/0313 | Loss: 0.1099
Epoch: 071/130 | Batch 0250/0313 | Loss: 0.1712
Epoch: 071/130 | Batch 0300/0313 | Loss: 0.0897
**Epoch: 071/130 | Train. Acc.: 96.667% | Loss: 0.0980
**Epoch: 071/130 | Valid. Acc.: 92.180% | Loss: 0.2728
Time elapsed: 41.71 min
Epoch: 072/130 | Current Learning Rate: 0.012500
Epoch: 072/130 | Batch 0000/0313 | Loss: 0.1572
Epoch: 072/130 | Batch 0050/0313 | Loss: 0.0772
Epoch: 072/130 | Batch 0100/0313 | Loss: 0.1313
Epoch: 072/130 | Batch 0150/0313 | Loss: 0.0400
Epoch: 072/130 | Batch 0200/0313 | Loss: 0.1555
Epoch: 072/130 | Batch 0250/0313 | Loss: 0.0745
Epoch: 072/130 | Batch 0300/0313 | Loss: 0.0539
**Epoch: 072/130 | Train. Acc.: 96.508% | Loss: 0.1028
**Epoch: 072/130 | Valid. Acc.: 92.300% | Loss: 0.2622
Time elapsed: 42.31 min
Epoch: 073/130 | Current Learning Rate: 0.012500
Epoch: 073/130 | Batch 0000/0313 | Loss: 0.1721
Epoch: 073/130 | Batch 0050/0313 | Loss: 0.1373
Epoch: 073/130 | Batch 0100/0313 | Loss: 0.1367
Epoch: 073/130 | Batch 0150/0313 | Loss: 0.1182
Epoch: 073/130 | Batch 0200/0313 | Loss: 0.0714
Epoch: 073/130 | Batch 0250/0313 | Loss: 0.0825
Epoch: 073/130 | Batch 0300/0313 | Loss: 0.0747
**Epoch: 073/130 | Train. Acc.: 96.530% | Loss: 0.1005
**Epoch: 073/130 | Valid. Acc.: 91.650% | Loss: 0.2867
Time elapsed: 42.89 min
Epoch: 074/130 | Current Learning Rate: 0.012500
Epoch: 074/130 | Batch 0000/0313 | Loss: 0.1113
Epoch: 074/130 | Batch 0050/0313 | Loss: 0.1144
Epoch: 074/130 | Batch 0100/0313 | Loss: 0.0494
Epoch: 074/130 | Batch 0150/0313 | Loss: 0.0742
Epoch: 074/130 | Batch 0200/0313 | Loss: 0.1489
Epoch: 074/130 | Batch 0250/0313 | Loss: 0.0700
Epoch: 074/130 | Batch 0300/0313 | Loss: 0.0711
**Epoch: 074/130 | Train. Acc.: 96.665% | Loss: 0.0978
**Epoch: 074/130 | Valid. Acc.: 91.950% | Loss: 0.2736
Time elapsed: 43.48 min
Epoch: 075/130 | Current Learning Rate: 0.012500
Epoch: 075/130 | Batch 0000/0313 | Loss: 0.0614
Epoch: 075/130 | Batch 0050/0313 | Loss: 0.0604
Epoch: 075/130 | Batch 0100/0313 | Loss: 0.0661
Epoch: 075/130 | Batch 0150/0313 | Loss: 0.0713
Epoch: 075/130 | Batch 0200/0313 | Loss: 0.0836
Epoch: 075/130 | Batch 0250/0313 | Loss: 0.1321
Epoch: 075/130 | Batch 0300/0313 | Loss: 0.1178
**Epoch: 075/130 | Train. Acc.: 96.297% | Loss: 0.1060
**Epoch: 075/130 | Valid. Acc.: 91.550% | Loss: 0.2912
Time elapsed: 44.06 min
Epoch: 076/130 | Current Learning Rate: 0.012500
Epoch: 076/130 | Batch 0000/0313 | Loss: 0.0640
Epoch: 076/130 | Batch 0050/0313 | Loss: 0.0775
Epoch: 076/130 | Batch 0100/0313 | Loss: 0.0816
Epoch: 076/130 | Batch 0150/0313 | Loss: 0.1645
Epoch: 076/130 | Batch 0200/0313 | Loss: 0.0803
Epoch: 076/130 | Batch 0250/0313 | Loss: 0.1138
Epoch: 076/130 | Batch 0300/0313 | Loss: 0.1673
**Epoch: 076/130 | Train. Acc.: 96.447% | Loss: 0.1023
**Epoch: 076/130 | Valid. Acc.: 91.370% | Loss: 0.2920
Time elapsed: 44.64 min
Epoch: 077/130 | Current Learning Rate: 0.012500
Epoch: 077/130 | Batch 0000/0313 | Loss: 0.0606
Epoch: 077/130 | Batch 0050/0313 | Loss: 0.0493
Epoch: 077/130 | Batch 0100/0313 | Loss: 0.1391
Epoch: 077/130 | Batch 0150/0313 | Loss: 0.0983
Epoch: 077/130 | Batch 0200/0313 | Loss: 0.0888
Epoch: 077/130 | Batch 0250/0313 | Loss: 0.1443
Epoch: 077/130 | Batch 0300/0313 | Loss: 0.1233
**Epoch: 077/130 | Train. Acc.: 97.035% | Loss: 0.0877
**Epoch: 077/130 | Valid. Acc.: 92.240% | Loss: 0.2689
Epoch 00077: reducing learning rate of group 0 to 6.2500e-03.
Time elapsed: 45.23 min
Epoch: 078/130 | Current Learning Rate: 0.006250
Epoch: 078/130 | Batch 0000/0313 | Loss: 0.1138
Epoch: 078/130 | Batch 0050/0313 | Loss: 0.0207
Epoch: 078/130 | Batch 0100/0313 | Loss: 0.0565
Epoch: 078/130 | Batch 0150/0313 | Loss: 0.0414
Epoch: 078/130 | Batch 0200/0313 | Loss: 0.0981
Epoch: 078/130 | Batch 0250/0313 | Loss: 0.0701
Epoch: 078/130 | Batch 0300/0313 | Loss: 0.0447
**Epoch: 078/130 | Train. Acc.: 98.633% | Loss: 0.0434
**Epoch: 078/130 | Valid. Acc.: 93.550% | Loss: 0.2307
**Validation loss decreased (0.236859 --> 0.230669). Saving model ...
Time elapsed: 45.82 min
Epoch: 079/130 | Current Learning Rate: 0.006250
Epoch: 079/130 | Batch 0000/0313 | Loss: 0.0349
Epoch: 079/130 | Batch 0050/0313 | Loss: 0.0161
Epoch: 079/130 | Batch 0100/0313 | Loss: 0.0555
Epoch: 079/130 | Batch 0150/0313 | Loss: 0.0421
Epoch: 079/130 | Batch 0200/0313 | Loss: 0.0331
Epoch: 079/130 | Batch 0250/0313 | Loss: 0.0241
Epoch: 079/130 | Batch 0300/0313 | Loss: 0.0708
**Epoch: 079/130 | Train. Acc.: 98.498% | Loss: 0.0453
**Epoch: 079/130 | Valid. Acc.: 93.350% | Loss: 0.2500
Time elapsed: 46.41 min
Epoch: 080/130 | Current Learning Rate: 0.006250
Epoch: 080/130 | Batch 0000/0313 | Loss: 0.0149
Epoch: 080/130 | Batch 0050/0313 | Loss: 0.0101
Epoch: 080/130 | Batch 0100/0313 | Loss: 0.0210
Epoch: 080/130 | Batch 0150/0313 | Loss: 0.0626
Epoch: 080/130 | Batch 0200/0313 | Loss: 0.0170
Epoch: 080/130 | Batch 0250/0313 | Loss: 0.0319
Epoch: 080/130 | Batch 0300/0313 | Loss: 0.0374
**Epoch: 080/130 | Train. Acc.: 98.572% | Loss: 0.0431
**Epoch: 080/130 | Valid. Acc.: 93.140% | Loss: 0.2661
Time elapsed: 47.00 min
Epoch: 081/130 | Current Learning Rate: 0.006250
Epoch: 081/130 | Batch 0000/0313 | Loss: 0.0393
Epoch: 081/130 | Batch 0050/0313 | Loss: 0.0341
Epoch: 081/130 | Batch 0100/0313 | Loss: 0.0109
Epoch: 081/130 | Batch 0150/0313 | Loss: 0.0376
Epoch: 081/130 | Batch 0200/0313 | Loss: 0.0283
Epoch: 081/130 | Batch 0250/0313 | Loss: 0.0274
Epoch: 081/130 | Batch 0300/0313 | Loss: 0.0284
**Epoch: 081/130 | Train. Acc.: 98.790% | Loss: 0.0376
**Epoch: 081/130 | Valid. Acc.: 93.130% | Loss: 0.2545
Time elapsed: 47.59 min
Epoch: 082/130 | Current Learning Rate: 0.006250
Epoch: 082/130 | Batch 0000/0313 | Loss: 0.1114
Epoch: 082/130 | Batch 0050/0313 | Loss: 0.0250
Epoch: 082/130 | Batch 0100/0313 | Loss: 0.0303
Epoch: 082/130 | Batch 0150/0313 | Loss: 0.0197
Epoch: 082/130 | Batch 0200/0313 | Loss: 0.1654
Epoch: 082/130 | Batch 0250/0313 | Loss: 0.0489
Epoch: 082/130 | Batch 0300/0313 | Loss: 0.1505
**Epoch: 082/130 | Train. Acc.: 98.685% | Loss: 0.0405
**Epoch: 082/130 | Valid. Acc.: 93.530% | Loss: 0.2472
Time elapsed: 48.17 min
Epoch: 083/130 | Current Learning Rate: 0.006250
Epoch: 083/130 | Batch 0000/0313 | Loss: 0.0330
Epoch: 083/130 | Batch 0050/0313 | Loss: 0.0244
Epoch: 083/130 | Batch 0100/0313 | Loss: 0.0492
Epoch: 083/130 | Batch 0150/0313 | Loss: 0.0664
Epoch: 083/130 | Batch 0200/0313 | Loss: 0.0518
Epoch: 083/130 | Batch 0250/0313 | Loss: 0.0648
Epoch: 083/130 | Batch 0300/0313 | Loss: 0.0345
**Epoch: 083/130 | Train. Acc.: 98.693% | Loss: 0.0395
**Epoch: 083/130 | Valid. Acc.: 93.210% | Loss: 0.2596
Time elapsed: 48.76 min
Epoch: 084/130 | Current Learning Rate: 0.006250
Epoch: 084/130 | Batch 0000/0313 | Loss: 0.0134
Epoch: 084/130 | Batch 0050/0313 | Loss: 0.0791
Epoch: 084/130 | Batch 0100/0313 | Loss: 0.0098
Epoch: 084/130 | Batch 0150/0313 | Loss: 0.0377
Epoch: 084/130 | Batch 0200/0313 | Loss: 0.0364
Epoch: 084/130 | Batch 0250/0313 | Loss: 0.0344
Epoch: 084/130 | Batch 0300/0313 | Loss: 0.0396
**Epoch: 084/130 | Train. Acc.: 98.785% | Loss: 0.0375
**Epoch: 084/130 | Valid. Acc.: 93.290% | Loss: 0.2582
Time elapsed: 49.34 min
Epoch: 085/130 | Current Learning Rate: 0.006250
Epoch: 085/130 | Batch 0000/0313 | Loss: 0.0247
Epoch: 085/130 | Batch 0050/0313 | Loss: 0.0392
Epoch: 085/130 | Batch 0100/0313 | Loss: 0.0531
Epoch: 085/130 | Batch 0150/0313 | Loss: 0.0183
Epoch: 085/130 | Batch 0200/0313 | Loss: 0.0402
Epoch: 085/130 | Batch 0250/0313 | Loss: 0.0734
Epoch: 085/130 | Batch 0300/0313 | Loss: 0.0513
**Epoch: 085/130 | Train. Acc.: 98.810% | Loss: 0.0356
**Epoch: 085/130 | Valid. Acc.: 93.540% | Loss: 0.2427
Epoch 00085: reducing learning rate of group 0 to 3.1250e-03.
Time elapsed: 49.93 min
Epoch: 086/130 | Current Learning Rate: 0.003125
Epoch: 086/130 | Batch 0000/0313 | Loss: 0.0390
Epoch: 086/130 | Batch 0050/0313 | Loss: 0.0559
Epoch: 086/130 | Batch 0100/0313 | Loss: 0.0255
Epoch: 086/130 | Batch 0150/0313 | Loss: 0.0210
Epoch: 086/130 | Batch 0200/0313 | Loss: 0.0329
Epoch: 086/130 | Batch 0250/0313 | Loss: 0.0081
Epoch: 086/130 | Batch 0300/0313 | Loss: 0.0147
**Epoch: 086/130 | Train. Acc.: 99.347% | Loss: 0.0203
**Epoch: 086/130 | Valid. Acc.: 93.910% | Loss: 0.2314
Time elapsed: 50.52 min
Epoch: 087/130 | Current Learning Rate: 0.003125
Epoch: 087/130 | Batch 0000/0313 | Loss: 0.0043
Epoch: 087/130 | Batch 0050/0313 | Loss: 0.0240
Epoch: 087/130 | Batch 0100/0313 | Loss: 0.0211
Epoch: 087/130 | Batch 0150/0313 | Loss: 0.0022
Epoch: 087/130 | Batch 0200/0313 | Loss: 0.0085
Epoch: 087/130 | Batch 0250/0313 | Loss: 0.0060
Epoch: 087/130 | Batch 0300/0313 | Loss: 0.0389
**Epoch: 087/130 | Train. Acc.: 99.450% | Loss: 0.0176
**Epoch: 087/130 | Valid. Acc.: 93.840% | Loss: 0.2390
Time elapsed: 51.10 min
Epoch: 088/130 | Current Learning Rate: 0.003125
Epoch: 088/130 | Batch 0000/0313 | Loss: 0.0123
Epoch: 088/130 | Batch 0050/0313 | Loss: 0.0047
Epoch: 088/130 | Batch 0100/0313 | Loss: 0.0102
Epoch: 088/130 | Batch 0150/0313 | Loss: 0.0063
Epoch: 088/130 | Batch 0200/0313 | Loss: 0.0635
Epoch: 088/130 | Batch 0250/0313 | Loss: 0.0059
Epoch: 088/130 | Batch 0300/0313 | Loss: 0.0413
**Epoch: 088/130 | Train. Acc.: 99.453% | Loss: 0.0171
**Epoch: 088/130 | Valid. Acc.: 93.880% | Loss: 0.2380
Time elapsed: 51.68 min
Epoch: 089/130 | Current Learning Rate: 0.003125
Epoch: 089/130 | Batch 0000/0313 | Loss: 0.0069
Epoch: 089/130 | Batch 0050/0313 | Loss: 0.0166
Epoch: 089/130 | Batch 0100/0313 | Loss: 0.0055
Epoch: 089/130 | Batch 0150/0313 | Loss: 0.0519
Epoch: 089/130 | Batch 0200/0313 | Loss: 0.0076
Epoch: 089/130 | Batch 0250/0313 | Loss: 0.0050
Epoch: 089/130 | Batch 0300/0313 | Loss: 0.0160
**Epoch: 089/130 | Train. Acc.: 99.465% | Loss: 0.0167
**Epoch: 089/130 | Valid. Acc.: 93.820% | Loss: 0.2455
Time elapsed: 52.27 min
Epoch: 090/130 | Current Learning Rate: 0.003125
Epoch: 090/130 | Batch 0000/0313 | Loss: 0.0122
Epoch: 090/130 | Batch 0050/0313 | Loss: 0.0660
Epoch: 090/130 | Batch 0100/0313 | Loss: 0.0227
Epoch: 090/130 | Batch 0150/0313 | Loss: 0.0286
Epoch: 090/130 | Batch 0200/0313 | Loss: 0.0039
Epoch: 090/130 | Batch 0250/0313 | Loss: 0.0132
Epoch: 090/130 | Batch 0300/0313 | Loss: 0.0088
**Epoch: 090/130 | Train. Acc.: 99.502% | Loss: 0.0164
**Epoch: 090/130 | Valid. Acc.: 93.810% | Loss: 0.2438
Time elapsed: 52.86 min
Epoch: 091/130 | Current Learning Rate: 0.003125
Epoch: 091/130 | Batch 0000/0313 | Loss: 0.0353
Epoch: 091/130 | Batch 0050/0313 | Loss: 0.0035
Epoch: 091/130 | Batch 0100/0313 | Loss: 0.0155
Epoch: 091/130 | Batch 0150/0313 | Loss: 0.0241
Epoch: 091/130 | Batch 0200/0313 | Loss: 0.0443
Epoch: 091/130 | Batch 0250/0313 | Loss: 0.0052
Epoch: 091/130 | Batch 0300/0313 | Loss: 0.0076
**Epoch: 091/130 | Train. Acc.: 99.540% | Loss: 0.0155
**Epoch: 091/130 | Valid. Acc.: 93.870% | Loss: 0.2327
Time elapsed: 53.44 min
Epoch: 092/130 | Current Learning Rate: 0.003125
Epoch: 092/130 | Batch 0000/0313 | Loss: 0.0053
Epoch: 092/130 | Batch 0050/0313 | Loss: 0.0113
Epoch: 092/130 | Batch 0100/0313 | Loss: 0.0250
Epoch: 092/130 | Batch 0150/0313 | Loss: 0.0131
Epoch: 092/130 | Batch 0200/0313 | Loss: 0.0052
Epoch: 092/130 | Batch 0250/0313 | Loss: 0.0115
Epoch: 092/130 | Batch 0300/0313 | Loss: 0.0211
**Epoch: 092/130 | Train. Acc.: 99.543% | Loss: 0.0151
**Epoch: 092/130 | Valid. Acc.: 93.960% | Loss: 0.2388
Epoch 00092: reducing learning rate of group 0 to 1.5625e-03.
Time elapsed: 54.02 min
Epoch: 093/130 | Current Learning Rate: 0.001563
Epoch: 093/130 | Batch 0000/0313 | Loss: 0.0099
Epoch: 093/130 | Batch 0050/0313 | Loss: 0.0030
Epoch: 093/130 | Batch 0100/0313 | Loss: 0.0206
Epoch: 093/130 | Batch 0150/0313 | Loss: 0.0105
Epoch: 093/130 | Batch 0200/0313 | Loss: 0.0317
Epoch: 093/130 | Batch 0250/0313 | Loss: 0.0053
Epoch: 093/130 | Batch 0300/0313 | Loss: 0.0048
**Epoch: 093/130 | Train. Acc.: 99.740% | Loss: 0.0101
**Epoch: 093/130 | Valid. Acc.: 94.040% | Loss: 0.2325
Time elapsed: 54.61 min
Epoch: 094/130 | Current Learning Rate: 0.001563
Epoch: 094/130 | Batch 0000/0313 | Loss: 0.0093
Epoch: 094/130 | Batch 0050/0313 | Loss: 0.0137
Epoch: 094/130 | Batch 0100/0313 | Loss: 0.0156
Epoch: 094/130 | Batch 0150/0313 | Loss: 0.0109
Epoch: 094/130 | Batch 0200/0313 | Loss: 0.0059
Epoch: 094/130 | Batch 0250/0313 | Loss: 0.0126
Epoch: 094/130 | Batch 0300/0313 | Loss: 0.0087
**Epoch: 094/130 | Train. Acc.: 99.665% | Loss: 0.0117
**Epoch: 094/130 | Valid. Acc.: 94.240% | Loss: 0.2332
Time elapsed: 55.20 min
Epoch: 095/130 | Current Learning Rate: 0.001563
Epoch: 095/130 | Batch 0000/0313 | Loss: 0.0319
Epoch: 095/130 | Batch 0050/0313 | Loss: 0.0086
Epoch: 095/130 | Batch 0100/0313 | Loss: 0.0514
Epoch: 095/130 | Batch 0150/0313 | Loss: 0.0061
Epoch: 095/130 | Batch 0200/0313 | Loss: 0.0127
Epoch: 095/130 | Batch 0250/0313 | Loss: 0.0031
Epoch: 095/130 | Batch 0300/0313 | Loss: 0.0226
**Epoch: 095/130 | Train. Acc.: 99.700% | Loss: 0.0104
**Epoch: 095/130 | Valid. Acc.: 94.010% | Loss: 0.2347
Time elapsed: 55.78 min
Epoch: 096/130 | Current Learning Rate: 0.001563
Epoch: 096/130 | Batch 0000/0313 | Loss: 0.0170
Epoch: 096/130 | Batch 0050/0313 | Loss: 0.0032
Epoch: 096/130 | Batch 0100/0313 | Loss: 0.0046
Epoch: 096/130 | Batch 0150/0313 | Loss: 0.0051
Epoch: 096/130 | Batch 0200/0313 | Loss: 0.0021
Epoch: 096/130 | Batch 0250/0313 | Loss: 0.0059
Epoch: 096/130 | Batch 0300/0313 | Loss: 0.0166
**Epoch: 096/130 | Train. Acc.: 99.792% | Loss: 0.0089
**Epoch: 096/130 | Valid. Acc.: 94.160% | Loss: 0.2346
Time elapsed: 56.36 min
Epoch: 097/130 | Current Learning Rate: 0.001563
Epoch: 097/130 | Batch 0000/0313 | Loss: 0.0029
Epoch: 097/130 | Batch 0050/0313 | Loss: 0.0097
Epoch: 097/130 | Batch 0100/0313 | Loss: 0.0460
Epoch: 097/130 | Batch 0150/0313 | Loss: 0.0055
Epoch: 097/130 | Batch 0200/0313 | Loss: 0.0094
Epoch: 097/130 | Batch 0250/0313 | Loss: 0.0107
Epoch: 097/130 | Batch 0300/0313 | Loss: 0.0078
**Epoch: 097/130 | Train. Acc.: 99.682% | Loss: 0.0103
**Epoch: 097/130 | Valid. Acc.: 94.190% | Loss: 0.2323
Time elapsed: 56.94 min
Epoch: 098/130 | Current Learning Rate: 0.001563
Epoch: 098/130 | Batch 0000/0313 | Loss: 0.0027
Epoch: 098/130 | Batch 0050/0313 | Loss: 0.0097
Epoch: 098/130 | Batch 0100/0313 | Loss: 0.0190
Epoch: 098/130 | Batch 0150/0313 | Loss: 0.0182
Epoch: 098/130 | Batch 0200/0313 | Loss: 0.0568
Epoch: 098/130 | Batch 0250/0313 | Loss: 0.0087
Epoch: 098/130 | Batch 0300/0313 | Loss: 0.0126
**Epoch: 098/130 | Train. Acc.: 99.733% | Loss: 0.0095
**Epoch: 098/130 | Valid. Acc.: 94.250% | Loss: 0.2296
**Validation loss decreased (0.230669 --> 0.229622). Saving model ...
Time elapsed: 57.53 min
Epoch: 099/130 | Current Learning Rate: 0.001563
Epoch: 099/130 | Batch 0000/0313 | Loss: 0.0066
Epoch: 099/130 | Batch 0050/0313 | Loss: 0.0165
Epoch: 099/130 | Batch 0100/0313 | Loss: 0.0129
Epoch: 099/130 | Batch 0150/0313 | Loss: 0.0039
Epoch: 099/130 | Batch 0200/0313 | Loss: 0.0169
Epoch: 099/130 | Batch 0250/0313 | Loss: 0.0032
Epoch: 099/130 | Batch 0300/0313 | Loss: 0.0130
**Epoch: 099/130 | Train. Acc.: 99.770% | Loss: 0.0087
**Epoch: 099/130 | Valid. Acc.: 94.130% | Loss: 0.2385
Time elapsed: 58.12 min
Epoch: 100/130 | Current Learning Rate: 0.001563
Epoch: 100/130 | Batch 0000/0313 | Loss: 0.0055
Epoch: 100/130 | Batch 0050/0313 | Loss: 0.0079
Epoch: 100/130 | Batch 0100/0313 | Loss: 0.0271
Epoch: 100/130 | Batch 0150/0313 | Loss: 0.0025
Epoch: 100/130 | Batch 0200/0313 | Loss: 0.0317
Epoch: 100/130 | Batch 0250/0313 | Loss: 0.0154
Epoch: 100/130 | Batch 0300/0313 | Loss: 0.0345
**Epoch: 100/130 | Train. Acc.: 99.775% | Loss: 0.0083
**Epoch: 100/130 | Valid. Acc.: 94.060% | Loss: 0.2351
Time elapsed: 58.70 min
Epoch: 101/130 | Current Learning Rate: 0.001563
Epoch: 101/130 | Batch 0000/0313 | Loss: 0.0212
Epoch: 101/130 | Batch 0050/0313 | Loss: 0.0077
Epoch: 101/130 | Batch 0100/0313 | Loss: 0.0050
Epoch: 101/130 | Batch 0150/0313 | Loss: 0.0116
Epoch: 101/130 | Batch 0200/0313 | Loss: 0.0033
Epoch: 101/130 | Batch 0250/0313 | Loss: 0.0050
Epoch: 101/130 | Batch 0300/0313 | Loss: 0.0188
**Epoch: 101/130 | Train. Acc.: 99.810% | Loss: 0.0077
**Epoch: 101/130 | Valid. Acc.: 94.080% | Loss: 0.2354
Time elapsed: 59.29 min
Epoch: 102/130 | Current Learning Rate: 0.001563
Epoch: 102/130 | Batch 0000/0313 | Loss: 0.0055
Epoch: 102/130 | Batch 0050/0313 | Loss: 0.0034
Epoch: 102/130 | Batch 0100/0313 | Loss: 0.0039
Epoch: 102/130 | Batch 0150/0313 | Loss: 0.0020
Epoch: 102/130 | Batch 0200/0313 | Loss: 0.0038
Epoch: 102/130 | Batch 0250/0313 | Loss: 0.0248
Epoch: 102/130 | Batch 0300/0313 | Loss: 0.0083
**Epoch: 102/130 | Train. Acc.: 99.740% | Loss: 0.0081
**Epoch: 102/130 | Valid. Acc.: 94.150% | Loss: 0.2367
Time elapsed: 59.88 min
Epoch: 103/130 | Current Learning Rate: 0.001563
Epoch: 103/130 | Batch 0000/0313 | Loss: 0.0076
Epoch: 103/130 | Batch 0050/0313 | Loss: 0.0020
Epoch: 103/130 | Batch 0100/0313 | Loss: 0.0028
Epoch: 103/130 | Batch 0150/0313 | Loss: 0.0105
Epoch: 103/130 | Batch 0200/0313 | Loss: 0.0028
Epoch: 103/130 | Batch 0250/0313 | Loss: 0.0172
Epoch: 103/130 | Batch 0300/0313 | Loss: 0.0017
**Epoch: 103/130 | Train. Acc.: 99.743% | Loss: 0.0084
**Epoch: 103/130 | Valid. Acc.: 94.220% | Loss: 0.2389
Time elapsed: 60.47 min
Epoch: 104/130 | Current Learning Rate: 0.001563
Epoch: 104/130 | Batch 0000/0313 | Loss: 0.0028
Epoch: 104/130 | Batch 0050/0313 | Loss: 0.0031
Epoch: 104/130 | Batch 0100/0313 | Loss: 0.0202
Epoch: 104/130 | Batch 0150/0313 | Loss: 0.0218
Epoch: 104/130 | Batch 0200/0313 | Loss: 0.0025
Epoch: 104/130 | Batch 0250/0313 | Loss: 0.0302
Epoch: 104/130 | Batch 0300/0313 | Loss: 0.0278
**Epoch: 104/130 | Train. Acc.: 99.818% | Loss: 0.0073
**Epoch: 104/130 | Valid. Acc.: 94.310% | Loss: 0.2326
Time elapsed: 61.05 min
Epoch: 105/130 | Current Learning Rate: 0.001563
Epoch: 105/130 | Batch 0000/0313 | Loss: 0.0102
Epoch: 105/130 | Batch 0050/0313 | Loss: 0.0109
Epoch: 105/130 | Batch 0100/0313 | Loss: 0.0014
Epoch: 105/130 | Batch 0150/0313 | Loss: 0.0122
Epoch: 105/130 | Batch 0200/0313 | Loss: 0.0069
Epoch: 105/130 | Batch 0250/0313 | Loss: 0.0058
Epoch: 105/130 | Batch 0300/0313 | Loss: 0.0051
**Epoch: 105/130 | Train. Acc.: 99.787% | Loss: 0.0075
**Epoch: 105/130 | Valid. Acc.: 94.190% | Loss: 0.2342
Epoch 00105: reducing learning rate of group 0 to 7.8125e-04.
Time elapsed: 61.64 min
Epoch: 106/130 | Current Learning Rate: 0.000781
Epoch: 106/130 | Batch 0000/0313 | Loss: 0.0155
Epoch: 106/130 | Batch 0050/0313 | Loss: 0.0048
Epoch: 106/130 | Batch 0100/0313 | Loss: 0.0080
Epoch: 106/130 | Batch 0150/0313 | Loss: 0.0014
Epoch: 106/130 | Batch 0200/0313 | Loss: 0.0018
Epoch: 106/130 | Batch 0250/0313 | Loss: 0.0158
Epoch: 106/130 | Batch 0300/0313 | Loss: 0.0017
**Epoch: 106/130 | Train. Acc.: 99.865% | Loss: 0.0062
**Epoch: 106/130 | Valid. Acc.: 94.150% | Loss: 0.2341
Time elapsed: 62.23 min
Epoch: 107/130 | Current Learning Rate: 0.000781
Epoch: 107/130 | Batch 0000/0313 | Loss: 0.0109
Epoch: 107/130 | Batch 0050/0313 | Loss: 0.0031
Epoch: 107/130 | Batch 0100/0313 | Loss: 0.0008
Epoch: 107/130 | Batch 0150/0313 | Loss: 0.0027
Epoch: 107/130 | Batch 0200/0313 | Loss: 0.0097
Epoch: 107/130 | Batch 0250/0313 | Loss: 0.0426
Epoch: 107/130 | Batch 0300/0313 | Loss: 0.0041
**Epoch: 107/130 | Train. Acc.: 99.797% | Loss: 0.0073
**Epoch: 107/130 | Valid. Acc.: 94.360% | Loss: 0.2272
**Validation loss decreased (0.229622 --> 0.227248). Saving model ...
Time elapsed: 62.82 min
Epoch: 108/130 | Current Learning Rate: 0.000781
Epoch: 108/130 | Batch 0000/0313 | Loss: 0.0028
Epoch: 108/130 | Batch 0050/0313 | Loss: 0.0059
Epoch: 108/130 | Batch 0100/0313 | Loss: 0.0041
Epoch: 108/130 | Batch 0150/0313 | Loss: 0.0038
Epoch: 108/130 | Batch 0200/0313 | Loss: 0.0036
Epoch: 108/130 | Batch 0250/0313 | Loss: 0.0020
Epoch: 108/130 | Batch 0300/0313 | Loss: 0.0141
**Epoch: 108/130 | Train. Acc.: 99.832% | Loss: 0.0060
**Epoch: 108/130 | Valid. Acc.: 94.210% | Loss: 0.2344
Time elapsed: 63.41 min
Epoch: 109/130 | Current Learning Rate: 0.000781
Epoch: 109/130 | Batch 0000/0313 | Loss: 0.0023
Epoch: 109/130 | Batch 0050/0313 | Loss: 0.0035
Epoch: 109/130 | Batch 0100/0313 | Loss: 0.0215
Epoch: 109/130 | Batch 0150/0313 | Loss: 0.0070
Epoch: 109/130 | Batch 0200/0313 | Loss: 0.0100
Epoch: 109/130 | Batch 0250/0313 | Loss: 0.0144
Epoch: 109/130 | Batch 0300/0313 | Loss: 0.0039
**Epoch: 109/130 | Train. Acc.: 99.892% | Loss: 0.0053
**Epoch: 109/130 | Valid. Acc.: 94.360% | Loss: 0.2306
Time elapsed: 64.00 min
Epoch: 110/130 | Current Learning Rate: 0.000781
Epoch: 110/130 | Batch 0000/0313 | Loss: 0.0011
Epoch: 110/130 | Batch 0050/0313 | Loss: 0.0053
Epoch: 110/130 | Batch 0100/0313 | Loss: 0.0032
Epoch: 110/130 | Batch 0150/0313 | Loss: 0.0056
Epoch: 110/130 | Batch 0200/0313 | Loss: 0.0162
Epoch: 110/130 | Batch 0250/0313 | Loss: 0.0019
Epoch: 110/130 | Batch 0300/0313 | Loss: 0.0023
**Epoch: 110/130 | Train. Acc.: 99.843% | Loss: 0.0062
**Epoch: 110/130 | Valid. Acc.: 94.260% | Loss: 0.2323
Time elapsed: 64.58 min
Epoch: 111/130 | Current Learning Rate: 0.000781
Epoch: 111/130 | Batch 0000/0313 | Loss: 0.0052
Epoch: 111/130 | Batch 0050/0313 | Loss: 0.0021
Epoch: 111/130 | Batch 0100/0313 | Loss: 0.0059
Epoch: 111/130 | Batch 0150/0313 | Loss: 0.0038
Epoch: 111/130 | Batch 0200/0313 | Loss: 0.0120
Epoch: 111/130 | Batch 0250/0313 | Loss: 0.0034
Epoch: 111/130 | Batch 0300/0313 | Loss: 0.0029
**Epoch: 111/130 | Train. Acc.: 99.838% | Loss: 0.0057
**Epoch: 111/130 | Valid. Acc.: 94.300% | Loss: 0.2319
Time elapsed: 65.17 min
Epoch: 112/130 | Current Learning Rate: 0.000781
Epoch: 112/130 | Batch 0000/0313 | Loss: 0.0033
Epoch: 112/130 | Batch 0050/0313 | Loss: 0.0112
Epoch: 112/130 | Batch 0100/0313 | Loss: 0.0028
Epoch: 112/130 | Batch 0150/0313 | Loss: 0.0086
Epoch: 112/130 | Batch 0200/0313 | Loss: 0.0033
Epoch: 112/130 | Batch 0250/0313 | Loss: 0.0029
Epoch: 112/130 | Batch 0300/0313 | Loss: 0.0039
**Epoch: 112/130 | Train. Acc.: 99.860% | Loss: 0.0055
**Epoch: 112/130 | Valid. Acc.: 94.350% | Loss: 0.2344
Time elapsed: 65.75 min
Epoch: 113/130 | Current Learning Rate: 0.000781
Epoch: 113/130 | Batch 0000/0313 | Loss: 0.0022
Epoch: 113/130 | Batch 0050/0313 | Loss: 0.0017
Epoch: 113/130 | Batch 0100/0313 | Loss: 0.0064
Epoch: 113/130 | Batch 0150/0313 | Loss: 0.0243
Epoch: 113/130 | Batch 0200/0313 | Loss: 0.0052
Epoch: 113/130 | Batch 0250/0313 | Loss: 0.0065
Epoch: 113/130 | Batch 0300/0313 | Loss: 0.0112
**Epoch: 113/130 | Train. Acc.: 99.875% | Loss: 0.0052
**Epoch: 113/130 | Valid. Acc.: 94.330% | Loss: 0.2331
Time elapsed: 66.34 min
Epoch: 114/130 | Current Learning Rate: 0.000781
Epoch: 114/130 | Batch 0000/0313 | Loss: 0.0018
Epoch: 114/130 | Batch 0050/0313 | Loss: 0.0246
Epoch: 114/130 | Batch 0100/0313 | Loss: 0.0013
Epoch: 114/130 | Batch 0150/0313 | Loss: 0.0019
Epoch: 114/130 | Batch 0200/0313 | Loss: 0.0156
Epoch: 114/130 | Batch 0250/0313 | Loss: 0.0047
Epoch: 114/130 | Batch 0300/0313 | Loss: 0.0171
**Epoch: 114/130 | Train. Acc.: 99.860% | Loss: 0.0054
**Epoch: 114/130 | Valid. Acc.: 94.340% | Loss: 0.2331
Epoch 00114: reducing learning rate of group 0 to 3.9063e-04.
Time elapsed: 66.92 min
Epoch: 115/130 | Current Learning Rate: 0.000391
Epoch: 115/130 | Batch 0000/0313 | Loss: 0.0120
Epoch: 115/130 | Batch 0050/0313 | Loss: 0.0110
Epoch: 115/130 | Batch 0100/0313 | Loss: 0.0079
Epoch: 115/130 | Batch 0150/0313 | Loss: 0.0053
Epoch: 115/130 | Batch 0200/0313 | Loss: 0.0017
Epoch: 115/130 | Batch 0250/0313 | Loss: 0.0040
Epoch: 115/130 | Batch 0300/0313 | Loss: 0.0068
**Epoch: 115/130 | Train. Acc.: 99.887% | Loss: 0.0049
**Epoch: 115/130 | Valid. Acc.: 94.320% | Loss: 0.2351
Time elapsed: 67.51 min
Epoch: 116/130 | Current Learning Rate: 0.000391
Epoch: 116/130 | Batch 0000/0313 | Loss: 0.0208
Epoch: 116/130 | Batch 0050/0313 | Loss: 0.0109
Epoch: 116/130 | Batch 0100/0313 | Loss: 0.0018
Epoch: 116/130 | Batch 0150/0313 | Loss: 0.0051
Epoch: 116/130 | Batch 0200/0313 | Loss: 0.0108
Epoch: 116/130 | Batch 0250/0313 | Loss: 0.0019
Epoch: 116/130 | Batch 0300/0313 | Loss: 0.0163
**Epoch: 116/130 | Train. Acc.: 99.873% | Loss: 0.0049
**Epoch: 116/130 | Valid. Acc.: 94.330% | Loss: 0.2348
Time elapsed: 68.10 min
Epoch: 117/130 | Current Learning Rate: 0.000391
Epoch: 117/130 | Batch 0000/0313 | Loss: 0.0179
Epoch: 117/130 | Batch 0050/0313 | Loss: 0.0033
Epoch: 117/130 | Batch 0100/0313 | Loss: 0.0056
Epoch: 117/130 | Batch 0150/0313 | Loss: 0.0023
Epoch: 117/130 | Batch 0200/0313 | Loss: 0.0046
Epoch: 117/130 | Batch 0250/0313 | Loss: 0.0089
Epoch: 117/130 | Batch 0300/0313 | Loss: 0.0107
**Epoch: 117/130 | Train. Acc.: 99.850% | Loss: 0.0056
**Epoch: 117/130 | Valid. Acc.: 94.310% | Loss: 0.2320
Time elapsed: 68.68 min
Epoch: 118/130 | Current Learning Rate: 0.000391
Epoch: 118/130 | Batch 0000/0313 | Loss: 0.0186
Epoch: 118/130 | Batch 0050/0313 | Loss: 0.0049
Epoch: 118/130 | Batch 0100/0313 | Loss: 0.0153
Epoch: 118/130 | Batch 0150/0313 | Loss: 0.0121
Epoch: 118/130 | Batch 0200/0313 | Loss: 0.0034
Epoch: 118/130 | Batch 0250/0313 | Loss: 0.0145
Epoch: 118/130 | Batch 0300/0313 | Loss: 0.0569
**Epoch: 118/130 | Train. Acc.: 99.867% | Loss: 0.0049
**Epoch: 118/130 | Valid. Acc.: 94.360% | Loss: 0.2318
Time elapsed: 69.27 min
Epoch: 119/130 | Current Learning Rate: 0.000391
Epoch: 119/130 | Batch 0000/0313 | Loss: 0.0109
Epoch: 119/130 | Batch 0050/0313 | Loss: 0.0094
Epoch: 119/130 | Batch 0100/0313 | Loss: 0.0014
Epoch: 119/130 | Batch 0150/0313 | Loss: 0.0187
Epoch: 119/130 | Batch 0200/0313 | Loss: 0.0045
Epoch: 119/130 | Batch 0250/0313 | Loss: 0.0075
Epoch: 119/130 | Batch 0300/0313 | Loss: 0.0034
**Epoch: 119/130 | Train. Acc.: 99.903% | Loss: 0.0043
**Epoch: 119/130 | Valid. Acc.: 94.360% | Loss: 0.2309
Time elapsed: 69.86 min
Epoch: 120/130 | Current Learning Rate: 0.000391
Epoch: 120/130 | Batch 0000/0313 | Loss: 0.0019
Epoch: 120/130 | Batch 0050/0313 | Loss: 0.0160
Epoch: 120/130 | Batch 0100/0313 | Loss: 0.0233
Epoch: 120/130 | Batch 0150/0313 | Loss: 0.0072
Epoch: 120/130 | Batch 0200/0313 | Loss: 0.0019
Epoch: 120/130 | Batch 0250/0313 | Loss: 0.0117
Epoch: 120/130 | Batch 0300/0313 | Loss: 0.0009
**Epoch: 120/130 | Train. Acc.: 99.882% | Loss: 0.0048
**Epoch: 120/130 | Valid. Acc.: 94.310% | Loss: 0.2304
Time elapsed: 70.45 min
Epoch: 121/130 | Current Learning Rate: 0.000391
Epoch: 121/130 | Batch 0000/0313 | Loss: 0.0285
Epoch: 121/130 | Batch 0050/0313 | Loss: 0.0015
Epoch: 121/130 | Batch 0100/0313 | Loss: 0.0102
Epoch: 121/130 | Batch 0150/0313 | Loss: 0.0028
Epoch: 121/130 | Batch 0200/0313 | Loss: 0.0097
Epoch: 121/130 | Batch 0250/0313 | Loss: 0.0544
Epoch: 121/130 | Batch 0300/0313 | Loss: 0.0055
**Epoch: 121/130 | Train. Acc.: 99.855% | Loss: 0.0053
**Epoch: 121/130 | Valid. Acc.: 94.240% | Loss: 0.2327
Epoch 00121: reducing learning rate of group 0 to 1.9531e-04.
Time elapsed: 71.03 min
Epoch: 122/130 | Current Learning Rate: 0.000195
Epoch: 122/130 | Batch 0000/0313 | Loss: 0.0186
Epoch: 122/130 | Batch 0050/0313 | Loss: 0.0045
Epoch: 122/130 | Batch 0100/0313 | Loss: 0.0211
Epoch: 122/130 | Batch 0150/0313 | Loss: 0.0029
Epoch: 122/130 | Batch 0200/0313 | Loss: 0.0035
Epoch: 122/130 | Batch 0250/0313 | Loss: 0.0021
Epoch: 122/130 | Batch 0300/0313 | Loss: 0.0030
**Epoch: 122/130 | Train. Acc.: 99.875% | Loss: 0.0055
**Epoch: 122/130 | Valid. Acc.: 94.350% | Loss: 0.2332
Time elapsed: 71.62 min
Epoch: 123/130 | Current Learning Rate: 0.000195
Epoch: 123/130 | Batch 0000/0313 | Loss: 0.0019
Epoch: 123/130 | Batch 0050/0313 | Loss: 0.0046
Epoch: 123/130 | Batch 0100/0313 | Loss: 0.0030
Epoch: 123/130 | Batch 0150/0313 | Loss: 0.0097
Epoch: 123/130 | Batch 0200/0313 | Loss: 0.0038
Epoch: 123/130 | Batch 0250/0313 | Loss: 0.0028
Epoch: 123/130 | Batch 0300/0313 | Loss: 0.0059
**Epoch: 123/130 | Train. Acc.: 99.877% | Loss: 0.0051
**Epoch: 123/130 | Valid. Acc.: 94.410% | Loss: 0.2316
Time elapsed: 72.21 min
Epoch: 124/130 | Current Learning Rate: 0.000195
Epoch: 124/130 | Batch 0000/0313 | Loss: 0.0066
Epoch: 124/130 | Batch 0050/0313 | Loss: 0.0014
Epoch: 124/130 | Batch 0100/0313 | Loss: 0.0032
Epoch: 124/130 | Batch 0150/0313 | Loss: 0.0045
Epoch: 124/130 | Batch 0200/0313 | Loss: 0.0140
Epoch: 124/130 | Batch 0250/0313 | Loss: 0.0143
Epoch: 124/130 | Batch 0300/0313 | Loss: 0.0303
**Epoch: 124/130 | Train. Acc.: 99.850% | Loss: 0.0052
**Epoch: 124/130 | Valid. Acc.: 94.330% | Loss: 0.2325
Time elapsed: 72.80 min
Epoch: 125/130 | Current Learning Rate: 0.000195
Epoch: 125/130 | Batch 0000/0313 | Loss: 0.0013
Epoch: 125/130 | Batch 0050/0313 | Loss: 0.0011
Epoch: 125/130 | Batch 0100/0313 | Loss: 0.0120
Epoch: 125/130 | Batch 0150/0313 | Loss: 0.0014
Epoch: 125/130 | Batch 0200/0313 | Loss: 0.0021
Epoch: 125/130 | Batch 0250/0313 | Loss: 0.0095
Epoch: 125/130 | Batch 0300/0313 | Loss: 0.0203
**Epoch: 125/130 | Train. Acc.: 99.887% | Loss: 0.0045
**Epoch: 125/130 | Valid. Acc.: 94.350% | Loss: 0.2323
Time elapsed: 73.39 min
Epoch: 126/130 | Current Learning Rate: 0.000195
Epoch: 126/130 | Batch 0000/0313 | Loss: 0.0015
Epoch: 126/130 | Batch 0050/0313 | Loss: 0.0025
Epoch: 126/130 | Batch 0100/0313 | Loss: 0.0059
Epoch: 126/130 | Batch 0150/0313 | Loss: 0.0021
Epoch: 126/130 | Batch 0200/0313 | Loss: 0.0069
Epoch: 126/130 | Batch 0250/0313 | Loss: 0.0023
Epoch: 126/130 | Batch 0300/0313 | Loss: 0.0012
**Epoch: 126/130 | Train. Acc.: 99.903% | Loss: 0.0048
**Epoch: 126/130 | Valid. Acc.: 94.330% | Loss: 0.2321
Time elapsed: 73.98 min
Epoch: 127/130 | Current Learning Rate: 0.000195
Epoch: 127/130 | Batch 0000/0313 | Loss: 0.0030
Epoch: 127/130 | Batch 0050/0313 | Loss: 0.0016
Epoch: 127/130 | Batch 0100/0313 | Loss: 0.0034
Epoch: 127/130 | Batch 0150/0313 | Loss: 0.0030
Epoch: 127/130 | Batch 0200/0313 | Loss: 0.0014
Epoch: 127/130 | Batch 0250/0313 | Loss: 0.0054
Epoch: 127/130 | Batch 0300/0313 | Loss: 0.0018
**Epoch: 127/130 | Train. Acc.: 99.885% | Loss: 0.0047
**Epoch: 127/130 | Valid. Acc.: 94.360% | Loss: 0.2338
Time elapsed: 74.56 min
Epoch: 128/130 | Current Learning Rate: 0.000195
Epoch: 128/130 | Batch 0000/0313 | Loss: 0.0016
Epoch: 128/130 | Batch 0050/0313 | Loss: 0.0030
Epoch: 128/130 | Batch 0100/0313 | Loss: 0.0030
Epoch: 128/130 | Batch 0150/0313 | Loss: 0.0073
Epoch: 128/130 | Batch 0200/0313 | Loss: 0.0046
Epoch: 128/130 | Batch 0250/0313 | Loss: 0.0075
Epoch: 128/130 | Batch 0300/0313 | Loss: 0.0015
**Epoch: 128/130 | Train. Acc.: 99.907% | Loss: 0.0046
**Epoch: 128/130 | Valid. Acc.: 94.390% | Loss: 0.2322
Epoch 00128: reducing learning rate of group 0 to 9.7656e-05.
Time elapsed: 75.15 min
Epoch: 129/130 | Current Learning Rate: 0.000098
Epoch: 129/130 | Batch 0000/0313 | Loss: 0.0053
Epoch: 129/130 | Batch 0050/0313 | Loss: 0.0041
Epoch: 129/130 | Batch 0100/0313 | Loss: 0.0016
Epoch: 129/130 | Batch 0150/0313 | Loss: 0.0050
Epoch: 129/130 | Batch 0200/0313 | Loss: 0.0022
Epoch: 129/130 | Batch 0250/0313 | Loss: 0.0024
Epoch: 129/130 | Batch 0300/0313 | Loss: 0.0014
**Epoch: 129/130 | Train. Acc.: 99.905% | Loss: 0.0046
**Epoch: 129/130 | Valid. Acc.: 94.420% | Loss: 0.2317
Time elapsed: 75.72 min
Epoch: 130/130 | Current Learning Rate: 0.000098
Epoch: 130/130 | Batch 0000/0313 | Loss: 0.0051
Epoch: 130/130 | Batch 0050/0313 | Loss: 0.0100
Epoch: 130/130 | Batch 0100/0313 | Loss: 0.0015
Epoch: 130/130 | Batch 0150/0313 | Loss: 0.0072
Epoch: 130/130 | Batch 0200/0313 | Loss: 0.0073
Epoch: 130/130 | Batch 0250/0313 | Loss: 0.0328
Epoch: 130/130 | Batch 0300/0313 | Loss: 0.0318
**Epoch: 130/130 | Train. Acc.: 99.895% | Loss: 0.0044
**Epoch: 130/130 | Valid. Acc.: 94.380% | Loss: 0.2305
Time elapsed: 76.31 min
Total Training Time: 76.31 min
Model: ResNet34
Test Loss: 0.2395
Test Accuracy (Overall): 94.22%

Test Accuracy of Airplane: 94% (943/1000)
Test Accuracy of      Car: 97% (979/1000)
Test Accuracy of     Bird: 92% (928/1000)
Test Accuracy of      Cat: 87% (874/1000)
Test Accuracy of     Deer: 95% (957/1000)
Test Accuracy of      Dog: 89% (892/1000)
Test Accuracy of     Frog: 96% (963/1000)
Test Accuracy of    Horse: 96% (964/1000)
Test Accuracy of     Ship: 96% (963/1000)
Test Accuracy of    Truck: 95% (959/1000)
Training ResNet50 for 130 epochs with initial learning rate 0.1...
Epoch: 001/130 | Current Learning Rate: 0.100000
Epoch: 001/130 | Batch 0000/0313 | Loss: 2.3804
Epoch: 001/130 | Batch 0050/0313 | Loss: 2.7309
Epoch: 001/130 | Batch 0100/0313 | Loss: 2.5513
Epoch: 001/130 | Batch 0150/0313 | Loss: 2.4388
Epoch: 001/130 | Batch 0200/0313 | Loss: 2.2835
Epoch: 001/130 | Batch 0250/0313 | Loss: 2.3319
Epoch: 001/130 | Batch 0300/0313 | Loss: 2.0624
**Epoch: 001/130 | Train. Acc.: 23.340% | Loss: 2.0242
**Epoch: 001/130 | Valid. Acc.: 23.990% | Loss: 1.9889
**Validation loss decreased (inf --> 1.988859). Saving model ...
Time elapsed: 0.81 min
Epoch: 002/130 | Current Learning Rate: 0.100000
Epoch: 002/130 | Batch 0000/0313 | Loss: 2.0985
Epoch: 002/130 | Batch 0050/0313 | Loss: 2.0194
Epoch: 002/130 | Batch 0100/0313 | Loss: 1.8831
Epoch: 002/130 | Batch 0150/0313 | Loss: 2.0428
Epoch: 002/130 | Batch 0200/0313 | Loss: 1.9077
Epoch: 002/130 | Batch 0250/0313 | Loss: 2.0085
Epoch: 002/130 | Batch 0300/0313 | Loss: 1.9930
**Epoch: 002/130 | Train. Acc.: 31.662% | Loss: 1.8702
**Epoch: 002/130 | Valid. Acc.: 34.730% | Loss: 1.8225
**Validation loss decreased (1.988859 --> 1.822511). Saving model ...
Time elapsed: 1.62 min
Epoch: 003/130 | Current Learning Rate: 0.100000
Epoch: 003/130 | Batch 0000/0313 | Loss: 1.8482
Epoch: 003/130 | Batch 0050/0313 | Loss: 1.8066
Epoch: 003/130 | Batch 0100/0313 | Loss: 1.9623
Epoch: 003/130 | Batch 0150/0313 | Loss: 1.8280
Epoch: 003/130 | Batch 0200/0313 | Loss: 1.7442
Epoch: 003/130 | Batch 0250/0313 | Loss: 1.7112
Epoch: 003/130 | Batch 0300/0313 | Loss: 1.6943
**Epoch: 003/130 | Train. Acc.: 37.130% | Loss: 1.7630
**Epoch: 003/130 | Valid. Acc.: 40.390% | Loss: 1.7633
**Validation loss decreased (1.822511 --> 1.763330). Saving model ...
Time elapsed: 2.43 min
Epoch: 004/130 | Current Learning Rate: 0.100000
Epoch: 004/130 | Batch 0000/0313 | Loss: 1.8507
Epoch: 004/130 | Batch 0050/0313 | Loss: 1.7217
Epoch: 004/130 | Batch 0100/0313 | Loss: 1.6337
Epoch: 004/130 | Batch 0150/0313 | Loss: 1.9186
Epoch: 004/130 | Batch 0200/0313 | Loss: 1.6840
Epoch: 004/130 | Batch 0250/0313 | Loss: 1.7097
Epoch: 004/130 | Batch 0300/0313 | Loss: 1.6368
**Epoch: 004/130 | Train. Acc.: 39.605% | Loss: 1.6889
**Epoch: 004/130 | Valid. Acc.: 44.130% | Loss: 1.6154
**Validation loss decreased (1.763330 --> 1.615438). Saving model ...
Time elapsed: 3.25 min
Epoch: 005/130 | Current Learning Rate: 0.100000
Epoch: 005/130 | Batch 0000/0313 | Loss: 1.5782
Epoch: 005/130 | Batch 0050/0313 | Loss: 1.4537
Epoch: 005/130 | Batch 0100/0313 | Loss: 1.6849
Epoch: 005/130 | Batch 0150/0313 | Loss: 1.6741
Epoch: 005/130 | Batch 0200/0313 | Loss: 1.6240
Epoch: 005/130 | Batch 0250/0313 | Loss: 1.4532
Epoch: 005/130 | Batch 0300/0313 | Loss: 1.3959
**Epoch: 005/130 | Train. Acc.: 44.992% | Loss: 1.5052
**Epoch: 005/130 | Valid. Acc.: 49.080% | Loss: 1.4141
**Validation loss decreased (1.615438 --> 1.414138). Saving model ...
Time elapsed: 4.06 min
Epoch: 006/130 | Current Learning Rate: 0.100000
Epoch: 006/130 | Batch 0000/0313 | Loss: 1.5560
Epoch: 006/130 | Batch 0050/0313 | Loss: 1.6354
Epoch: 006/130 | Batch 0100/0313 | Loss: 1.4992
Epoch: 006/130 | Batch 0150/0313 | Loss: 1.3912
Epoch: 006/130 | Batch 0200/0313 | Loss: 1.3209
Epoch: 006/130 | Batch 0250/0313 | Loss: 1.2678
Epoch: 006/130 | Batch 0300/0313 | Loss: 1.3894
**Epoch: 006/130 | Train. Acc.: 47.053% | Loss: 1.4736
**Epoch: 006/130 | Valid. Acc.: 52.850% | Loss: 1.3187
**Validation loss decreased (1.414138 --> 1.318669). Saving model ...
Time elapsed: 4.87 min
Epoch: 007/130 | Current Learning Rate: 0.100000
Epoch: 007/130 | Batch 0000/0313 | Loss: 1.3008
Epoch: 007/130 | Batch 0050/0313 | Loss: 1.2196
Epoch: 007/130 | Batch 0100/0313 | Loss: 1.2718
Epoch: 007/130 | Batch 0150/0313 | Loss: 1.1445
Epoch: 007/130 | Batch 0200/0313 | Loss: 1.3923
Epoch: 007/130 | Batch 0250/0313 | Loss: 1.2302
Epoch: 007/130 | Batch 0300/0313 | Loss: 1.2065
**Epoch: 007/130 | Train. Acc.: 54.143% | Loss: 1.2748
**Epoch: 007/130 | Valid. Acc.: 59.530% | Loss: 1.1360
**Validation loss decreased (1.318669 --> 1.135967). Saving model ...
Time elapsed: 5.69 min
Epoch: 008/130 | Current Learning Rate: 0.100000
Epoch: 008/130 | Batch 0000/0313 | Loss: 1.2925
Epoch: 008/130 | Batch 0050/0313 | Loss: 1.2277
Epoch: 008/130 | Batch 0100/0313 | Loss: 1.3609
Epoch: 008/130 | Batch 0150/0313 | Loss: 1.1554
Epoch: 008/130 | Batch 0200/0313 | Loss: 1.2890
Epoch: 008/130 | Batch 0250/0313 | Loss: 0.9652
Epoch: 008/130 | Batch 0300/0313 | Loss: 1.2934
**Epoch: 008/130 | Train. Acc.: 54.810% | Loss: 1.2763
**Epoch: 008/130 | Valid. Acc.: 59.720% | Loss: 1.1491
Time elapsed: 6.50 min
Epoch: 009/130 | Current Learning Rate: 0.100000
Epoch: 009/130 | Batch 0000/0313 | Loss: 1.3141
Epoch: 009/130 | Batch 0050/0313 | Loss: 1.2285
Epoch: 009/130 | Batch 0100/0313 | Loss: 1.0512
Epoch: 009/130 | Batch 0150/0313 | Loss: 1.2149
Epoch: 009/130 | Batch 0200/0313 | Loss: 1.0413
Epoch: 009/130 | Batch 0250/0313 | Loss: 1.0297
Epoch: 009/130 | Batch 0300/0313 | Loss: 1.1266
**Epoch: 009/130 | Train. Acc.: 58.095% | Loss: 1.2065
**Epoch: 009/130 | Valid. Acc.: 62.680% | Loss: 1.0880
**Validation loss decreased (1.135967 --> 1.088003). Saving model ...
Time elapsed: 7.31 min
Epoch: 010/130 | Current Learning Rate: 0.100000
Epoch: 010/130 | Batch 0000/0313 | Loss: 1.0812
Epoch: 010/130 | Batch 0050/0313 | Loss: 1.0415
Epoch: 010/130 | Batch 0100/0313 | Loss: 1.0483
Epoch: 010/130 | Batch 0150/0313 | Loss: 1.0741
Epoch: 010/130 | Batch 0200/0313 | Loss: 1.1042
Epoch: 010/130 | Batch 0250/0313 | Loss: 1.0572
Epoch: 010/130 | Batch 0300/0313 | Loss: 1.1195
**Epoch: 010/130 | Train. Acc.: 61.667% | Loss: 1.0962
**Epoch: 010/130 | Valid. Acc.: 65.110% | Loss: 1.0192
**Validation loss decreased (1.088003 --> 1.019199). Saving model ...
Time elapsed: 8.11 min
Epoch: 011/130 | Current Learning Rate: 0.100000
Epoch: 011/130 | Batch 0000/0313 | Loss: 1.0556
Epoch: 011/130 | Batch 0050/0313 | Loss: 0.7166
Epoch: 011/130 | Batch 0100/0313 | Loss: 0.9808
Epoch: 011/130 | Batch 0150/0313 | Loss: 1.1216
Epoch: 011/130 | Batch 0200/0313 | Loss: 1.0663
Epoch: 011/130 | Batch 0250/0313 | Loss: 0.9841
Epoch: 011/130 | Batch 0300/0313 | Loss: 0.9738
**Epoch: 011/130 | Train. Acc.: 63.157% | Loss: 1.0369
**Epoch: 011/130 | Valid. Acc.: 67.410% | Loss: 0.9309
**Validation loss decreased (1.019199 --> 0.930905). Saving model ...
Time elapsed: 8.93 min
Epoch: 012/130 | Current Learning Rate: 0.100000
Epoch: 012/130 | Batch 0000/0313 | Loss: 0.8967
Epoch: 012/130 | Batch 0050/0313 | Loss: 0.9843
Epoch: 012/130 | Batch 0100/0313 | Loss: 1.0514
Epoch: 012/130 | Batch 0150/0313 | Loss: 0.8612
Epoch: 012/130 | Batch 0200/0313 | Loss: 0.8603
Epoch: 012/130 | Batch 0250/0313 | Loss: 0.9223
Epoch: 012/130 | Batch 0300/0313 | Loss: 0.8284
**Epoch: 012/130 | Train. Acc.: 60.980% | Loss: 1.0987
**Epoch: 012/130 | Valid. Acc.: 62.310% | Loss: 1.0911
Time elapsed: 9.74 min
Epoch: 013/130 | Current Learning Rate: 0.100000
Epoch: 013/130 | Batch 0000/0313 | Loss: 0.8371
Epoch: 013/130 | Batch 0050/0313 | Loss: 0.8981
Epoch: 013/130 | Batch 0100/0313 | Loss: 0.7616
Epoch: 013/130 | Batch 0150/0313 | Loss: 0.8230
Epoch: 013/130 | Batch 0200/0313 | Loss: 0.8655
Epoch: 013/130 | Batch 0250/0313 | Loss: 0.7184
Epoch: 013/130 | Batch 0300/0313 | Loss: 0.7366
**Epoch: 013/130 | Train. Acc.: 63.763% | Loss: 1.0193
**Epoch: 013/130 | Valid. Acc.: 66.700% | Loss: 0.9598
Time elapsed: 10.55 min
Epoch: 014/130 | Current Learning Rate: 0.100000
Epoch: 014/130 | Batch 0000/0313 | Loss: 0.9056
Epoch: 014/130 | Batch 0050/0313 | Loss: 0.7455
Epoch: 014/130 | Batch 0100/0313 | Loss: 0.9246
Epoch: 014/130 | Batch 0150/0313 | Loss: 0.6391
Epoch: 014/130 | Batch 0200/0313 | Loss: 0.8539
Epoch: 014/130 | Batch 0250/0313 | Loss: 0.8993
Epoch: 014/130 | Batch 0300/0313 | Loss: 0.6205
**Epoch: 014/130 | Train. Acc.: 68.642% | Loss: 0.8990
**Epoch: 014/130 | Valid. Acc.: 74.270% | Loss: 0.7527
**Validation loss decreased (0.930905 --> 0.752732). Saving model ...
Time elapsed: 11.36 min
Epoch: 015/130 | Current Learning Rate: 0.100000
Epoch: 015/130 | Batch 0000/0313 | Loss: 0.7163
Epoch: 015/130 | Batch 0050/0313 | Loss: 0.7632
Epoch: 015/130 | Batch 0100/0313 | Loss: 0.7703
Epoch: 015/130 | Batch 0150/0313 | Loss: 0.8242
Epoch: 015/130 | Batch 0200/0313 | Loss: 0.7977
Epoch: 015/130 | Batch 0250/0313 | Loss: 0.6749
Epoch: 015/130 | Batch 0300/0313 | Loss: 0.9276
**Epoch: 015/130 | Train. Acc.: 66.610% | Loss: 0.9871
**Epoch: 015/130 | Valid. Acc.: 69.420% | Loss: 0.9209
Time elapsed: 12.17 min
Epoch: 016/130 | Current Learning Rate: 0.100000
Epoch: 016/130 | Batch 0000/0313 | Loss: 0.7163
Epoch: 016/130 | Batch 0050/0313 | Loss: 0.6686
Epoch: 016/130 | Batch 0100/0313 | Loss: 0.8152
Epoch: 016/130 | Batch 0150/0313 | Loss: 0.7272
Epoch: 016/130 | Batch 0200/0313 | Loss: 0.6705
Epoch: 016/130 | Batch 0250/0313 | Loss: 0.8616
Epoch: 016/130 | Batch 0300/0313 | Loss: 0.7766
**Epoch: 016/130 | Train. Acc.: 69.532% | Loss: 0.8812
**Epoch: 016/130 | Valid. Acc.: 73.730% | Loss: 0.7600
Time elapsed: 12.99 min
Epoch: 017/130 | Current Learning Rate: 0.100000
Epoch: 017/130 | Batch 0000/0313 | Loss: 0.9400
Epoch: 017/130 | Batch 0050/0313 | Loss: 0.8277
Epoch: 017/130 | Batch 0100/0313 | Loss: 0.6952
Epoch: 017/130 | Batch 0150/0313 | Loss: 0.7038
Epoch: 017/130 | Batch 0200/0313 | Loss: 0.6142
Epoch: 017/130 | Batch 0250/0313 | Loss: 0.6899
Epoch: 017/130 | Batch 0300/0313 | Loss: 0.8075
**Epoch: 017/130 | Train. Acc.: 69.117% | Loss: 0.8696
**Epoch: 017/130 | Valid. Acc.: 72.280% | Loss: 0.7953
Time elapsed: 13.80 min
Epoch: 018/130 | Current Learning Rate: 0.100000
Epoch: 018/130 | Batch 0000/0313 | Loss: 0.7761
Epoch: 018/130 | Batch 0050/0313 | Loss: 1.0206
Epoch: 018/130 | Batch 0100/0313 | Loss: 0.6998
Epoch: 018/130 | Batch 0150/0313 | Loss: 0.7195
Epoch: 018/130 | Batch 0200/0313 | Loss: 0.6021
Epoch: 018/130 | Batch 0250/0313 | Loss: 0.7919
Epoch: 018/130 | Batch 0300/0313 | Loss: 0.8641
**Epoch: 018/130 | Train. Acc.: 66.385% | Loss: 0.9846
**Epoch: 018/130 | Valid. Acc.: 69.580% | Loss: 0.9369
Time elapsed: 14.62 min
Epoch: 019/130 | Current Learning Rate: 0.100000
Epoch: 019/130 | Batch 0000/0313 | Loss: 0.7359
Epoch: 019/130 | Batch 0050/0313 | Loss: 0.7723
Epoch: 019/130 | Batch 0100/0313 | Loss: 0.6430
Epoch: 019/130 | Batch 0150/0313 | Loss: 0.5972
Epoch: 019/130 | Batch 0200/0313 | Loss: 0.6657
Epoch: 019/130 | Batch 0250/0313 | Loss: 0.6210
Epoch: 019/130 | Batch 0300/0313 | Loss: 0.6540
**Epoch: 019/130 | Train. Acc.: 69.562% | Loss: 0.8669
**Epoch: 019/130 | Valid. Acc.: 73.940% | Loss: 0.7642
Time elapsed: 15.43 min
Epoch: 020/130 | Current Learning Rate: 0.100000
Epoch: 020/130 | Batch 0000/0313 | Loss: 0.6378
Epoch: 020/130 | Batch 0050/0313 | Loss: 0.8265
Epoch: 020/130 | Batch 0100/0313 | Loss: 0.5670
Epoch: 020/130 | Batch 0150/0313 | Loss: 0.6560
Epoch: 020/130 | Batch 0200/0313 | Loss: 0.6304
Epoch: 020/130 | Batch 0250/0313 | Loss: 0.5612
Epoch: 020/130 | Batch 0300/0313 | Loss: 0.6840
**Epoch: 020/130 | Train. Acc.: 73.623% | Loss: 0.7432
**Epoch: 020/130 | Valid. Acc.: 76.190% | Loss: 0.6712
**Validation loss decreased (0.752732 --> 0.671220). Saving model ...
Time elapsed: 16.25 min
Epoch: 021/130 | Current Learning Rate: 0.100000
Epoch: 021/130 | Batch 0000/0313 | Loss: 0.5339
Epoch: 021/130 | Batch 0050/0313 | Loss: 0.6486
Epoch: 021/130 | Batch 0100/0313 | Loss: 0.6756
Epoch: 021/130 | Batch 0150/0313 | Loss: 0.7736
Epoch: 021/130 | Batch 0200/0313 | Loss: 0.6236
Epoch: 021/130 | Batch 0250/0313 | Loss: 0.7794
Epoch: 021/130 | Batch 0300/0313 | Loss: 0.6862
**Epoch: 021/130 | Train. Acc.: 68.172% | Loss: 0.9413
**Epoch: 021/130 | Valid. Acc.: 70.700% | Loss: 0.9295
Time elapsed: 17.06 min
Epoch: 022/130 | Current Learning Rate: 0.100000
Epoch: 022/130 | Batch 0000/0313 | Loss: 0.6918
Epoch: 022/130 | Batch 0050/0313 | Loss: 0.6975
Epoch: 022/130 | Batch 0100/0313 | Loss: 0.6071
Epoch: 022/130 | Batch 0150/0313 | Loss: 0.5869
Epoch: 022/130 | Batch 0200/0313 | Loss: 0.6659
Epoch: 022/130 | Batch 0250/0313 | Loss: 0.6831
Epoch: 022/130 | Batch 0300/0313 | Loss: 0.7734
**Epoch: 022/130 | Train. Acc.: 74.695% | Loss: 0.7117
**Epoch: 022/130 | Valid. Acc.: 76.780% | Loss: 0.6788
Time elapsed: 17.87 min
Epoch: 023/130 | Current Learning Rate: 0.100000
Epoch: 023/130 | Batch 0000/0313 | Loss: 0.8625
Epoch: 023/130 | Batch 0050/0313 | Loss: 0.5697
Epoch: 023/130 | Batch 0100/0313 | Loss: 0.7156
Epoch: 023/130 | Batch 0150/0313 | Loss: 0.4441
Epoch: 023/130 | Batch 0200/0313 | Loss: 0.8098
Epoch: 023/130 | Batch 0250/0313 | Loss: 0.6040
Epoch: 023/130 | Batch 0300/0313 | Loss: 0.6018
**Epoch: 023/130 | Train. Acc.: 71.242% | Loss: 0.8354
**Epoch: 023/130 | Valid. Acc.: 73.430% | Loss: 0.7990
Time elapsed: 18.69 min
Epoch: 024/130 | Current Learning Rate: 0.100000
Epoch: 024/130 | Batch 0000/0313 | Loss: 0.6807
Epoch: 024/130 | Batch 0050/0313 | Loss: 0.7481
Epoch: 024/130 | Batch 0100/0313 | Loss: 0.6453
Epoch: 024/130 | Batch 0150/0313 | Loss: 0.8974
Epoch: 024/130 | Batch 0200/0313 | Loss: 0.5043
Epoch: 024/130 | Batch 0250/0313 | Loss: 0.6155
Epoch: 024/130 | Batch 0300/0313 | Loss: 0.6160
**Epoch: 024/130 | Train. Acc.: 69.213% | Loss: 0.9477
**Epoch: 024/130 | Valid. Acc.: 72.690% | Loss: 0.9049
Time elapsed: 19.50 min
Epoch: 025/130 | Current Learning Rate: 0.100000
Epoch: 025/130 | Batch 0000/0313 | Loss: 0.7217
Epoch: 025/130 | Batch 0050/0313 | Loss: 0.4913
Epoch: 025/130 | Batch 0100/0313 | Loss: 0.6433
Epoch: 025/130 | Batch 0150/0313 | Loss: 0.4781
Epoch: 025/130 | Batch 0200/0313 | Loss: 0.4891
Epoch: 025/130 | Batch 0250/0313 | Loss: 0.6895
Epoch: 025/130 | Batch 0300/0313 | Loss: 0.5909
**Epoch: 025/130 | Train. Acc.: 74.208% | Loss: 0.7404
**Epoch: 025/130 | Valid. Acc.: 77.270% | Loss: 0.6810
Time elapsed: 20.31 min
Epoch: 026/130 | Current Learning Rate: 0.100000
Epoch: 026/130 | Batch 0000/0313 | Loss: 0.7717
Epoch: 026/130 | Batch 0050/0313 | Loss: 0.5618
Epoch: 026/130 | Batch 0100/0313 | Loss: 0.7429
Epoch: 026/130 | Batch 0150/0313 | Loss: 0.6044
Epoch: 026/130 | Batch 0200/0313 | Loss: 0.6318
Epoch: 026/130 | Batch 0250/0313 | Loss: 0.7273
Epoch: 026/130 | Batch 0300/0313 | Loss: 0.7117
**Epoch: 026/130 | Train. Acc.: 75.505% | Loss: 0.7139
**Epoch: 026/130 | Valid. Acc.: 77.680% | Loss: 0.6716
Time elapsed: 21.11 min
Epoch: 027/130 | Current Learning Rate: 0.100000
Epoch: 027/130 | Batch 0000/0313 | Loss: 0.6955
Epoch: 027/130 | Batch 0050/0313 | Loss: 0.6140
Epoch: 027/130 | Batch 0100/0313 | Loss: 0.5760
Epoch: 027/130 | Batch 0150/0313 | Loss: 0.7524
Epoch: 027/130 | Batch 0200/0313 | Loss: 0.8317
Epoch: 027/130 | Batch 0250/0313 | Loss: 0.6126
Epoch: 027/130 | Batch 0300/0313 | Loss: 0.5368
**Epoch: 027/130 | Train. Acc.: 73.850% | Loss: 0.7631
**Epoch: 027/130 | Valid. Acc.: 75.570% | Loss: 0.7320
Epoch 00027: reducing learning rate of group 0 to 5.0000e-02.
Time elapsed: 21.93 min
Epoch: 028/130 | Current Learning Rate: 0.050000
Epoch: 028/130 | Batch 0000/0313 | Loss: 0.6326
Epoch: 028/130 | Batch 0050/0313 | Loss: 0.5270
Epoch: 028/130 | Batch 0100/0313 | Loss: 0.6584
Epoch: 028/130 | Batch 0150/0313 | Loss: 0.3372
Epoch: 028/130 | Batch 0200/0313 | Loss: 0.4956
Epoch: 028/130 | Batch 0250/0313 | Loss: 0.4663
Epoch: 028/130 | Batch 0300/0313 | Loss: 0.4038
**Epoch: 028/130 | Train. Acc.: 82.203% | Loss: 0.5072
**Epoch: 028/130 | Valid. Acc.: 83.790% | Loss: 0.4601
**Validation loss decreased (0.671220 --> 0.460054). Saving model ...
Time elapsed: 22.74 min
Epoch: 029/130 | Current Learning Rate: 0.050000
Epoch: 029/130 | Batch 0000/0313 | Loss: 0.5072
Epoch: 029/130 | Batch 0050/0313 | Loss: 0.3482
Epoch: 029/130 | Batch 0100/0313 | Loss: 0.3497
Epoch: 029/130 | Batch 0150/0313 | Loss: 0.5173
Epoch: 029/130 | Batch 0200/0313 | Loss: 0.4759
Epoch: 029/130 | Batch 0250/0313 | Loss: 0.6112
Epoch: 029/130 | Batch 0300/0313 | Loss: 0.4793
**Epoch: 029/130 | Train. Acc.: 82.675% | Loss: 0.5018
**Epoch: 029/130 | Valid. Acc.: 83.560% | Loss: 0.4866
Time elapsed: 23.55 min
Epoch: 030/130 | Current Learning Rate: 0.050000
Epoch: 030/130 | Batch 0000/0313 | Loss: 0.3803
Epoch: 030/130 | Batch 0050/0313 | Loss: 0.4905
Epoch: 030/130 | Batch 0100/0313 | Loss: 0.5331
Epoch: 030/130 | Batch 0150/0313 | Loss: 0.5117
Epoch: 030/130 | Batch 0200/0313 | Loss: 0.4537
Epoch: 030/130 | Batch 0250/0313 | Loss: 0.4572
Epoch: 030/130 | Batch 0300/0313 | Loss: 0.3523
**Epoch: 030/130 | Train. Acc.: 79.755% | Loss: 0.5738
**Epoch: 030/130 | Valid. Acc.: 82.900% | Loss: 0.4952
Time elapsed: 24.36 min
Epoch: 031/130 | Current Learning Rate: 0.050000
Epoch: 031/130 | Batch 0000/0313 | Loss: 0.4941
Epoch: 031/130 | Batch 0050/0313 | Loss: 0.4973
Epoch: 031/130 | Batch 0100/0313 | Loss: 0.4146
Epoch: 031/130 | Batch 0150/0313 | Loss: 0.5050
Epoch: 031/130 | Batch 0200/0313 | Loss: 0.4486
Epoch: 031/130 | Batch 0250/0313 | Loss: 0.4023
Epoch: 031/130 | Batch 0300/0313 | Loss: 0.5855
**Epoch: 031/130 | Train. Acc.: 83.130% | Loss: 0.4868
**Epoch: 031/130 | Valid. Acc.: 85.000% | Loss: 0.4427
**Validation loss decreased (0.460054 --> 0.442654). Saving model ...
Time elapsed: 25.18 min
Epoch: 032/130 | Current Learning Rate: 0.050000
Epoch: 032/130 | Batch 0000/0313 | Loss: 0.4035
Epoch: 032/130 | Batch 0050/0313 | Loss: 0.3767
Epoch: 032/130 | Batch 0100/0313 | Loss: 0.4181
Epoch: 032/130 | Batch 0150/0313 | Loss: 0.5639
Epoch: 032/130 | Batch 0200/0313 | Loss: 0.3878
Epoch: 032/130 | Batch 0250/0313 | Loss: 0.5035
Epoch: 032/130 | Batch 0300/0313 | Loss: 0.4393
**Epoch: 032/130 | Train. Acc.: 84.108% | Loss: 0.4592
**Epoch: 032/130 | Valid. Acc.: 84.780% | Loss: 0.4481
Time elapsed: 25.99 min
Epoch: 033/130 | Current Learning Rate: 0.050000
Epoch: 033/130 | Batch 0000/0313 | Loss: 0.3943
Epoch: 033/130 | Batch 0050/0313 | Loss: 0.3669
Epoch: 033/130 | Batch 0100/0313 | Loss: 0.5231
Epoch: 033/130 | Batch 0150/0313 | Loss: 0.2604
Epoch: 033/130 | Batch 0200/0313 | Loss: 0.3569
Epoch: 033/130 | Batch 0250/0313 | Loss: 0.4745
Epoch: 033/130 | Batch 0300/0313 | Loss: 0.5481
**Epoch: 033/130 | Train. Acc.: 82.890% | Loss: 0.4848
**Epoch: 033/130 | Valid. Acc.: 83.920% | Loss: 0.4723
Time elapsed: 26.80 min
Epoch: 034/130 | Current Learning Rate: 0.050000
Epoch: 034/130 | Batch 0000/0313 | Loss: 0.4356
Epoch: 034/130 | Batch 0050/0313 | Loss: 0.3841
Epoch: 034/130 | Batch 0100/0313 | Loss: 0.5152
Epoch: 034/130 | Batch 0150/0313 | Loss: 0.4920
Epoch: 034/130 | Batch 0200/0313 | Loss: 0.4606
Epoch: 034/130 | Batch 0250/0313 | Loss: 0.4631
Epoch: 034/130 | Batch 0300/0313 | Loss: 0.6052
**Epoch: 034/130 | Train. Acc.: 83.083% | Loss: 0.4865
**Epoch: 034/130 | Valid. Acc.: 84.360% | Loss: 0.4526
Time elapsed: 27.61 min
Epoch: 035/130 | Current Learning Rate: 0.050000
Epoch: 035/130 | Batch 0000/0313 | Loss: 0.3402
Epoch: 035/130 | Batch 0050/0313 | Loss: 0.5071
Epoch: 035/130 | Batch 0100/0313 | Loss: 0.4065
Epoch: 035/130 | Batch 0150/0313 | Loss: 0.3849
Epoch: 035/130 | Batch 0200/0313 | Loss: 0.5779
Epoch: 035/130 | Batch 0250/0313 | Loss: 0.4315
Epoch: 035/130 | Batch 0300/0313 | Loss: 0.5256
**Epoch: 035/130 | Train. Acc.: 78.545% | Loss: 0.6428
**Epoch: 035/130 | Valid. Acc.: 79.930% | Loss: 0.6195
Time elapsed: 28.42 min
Epoch: 036/130 | Current Learning Rate: 0.050000
Epoch: 036/130 | Batch 0000/0313 | Loss: 0.4374
Epoch: 036/130 | Batch 0050/0313 | Loss: 0.4882
Epoch: 036/130 | Batch 0100/0313 | Loss: 0.5556
Epoch: 036/130 | Batch 0150/0313 | Loss: 0.3493
Epoch: 036/130 | Batch 0200/0313 | Loss: 0.4339
Epoch: 036/130 | Batch 0250/0313 | Loss: 0.5595
Epoch: 036/130 | Batch 0300/0313 | Loss: 0.2665
**Epoch: 036/130 | Train. Acc.: 81.733% | Loss: 0.5334
**Epoch: 036/130 | Valid. Acc.: 82.530% | Loss: 0.5491
Time elapsed: 29.24 min
Epoch: 037/130 | Current Learning Rate: 0.050000
Epoch: 037/130 | Batch 0000/0313 | Loss: 0.4273
Epoch: 037/130 | Batch 0050/0313 | Loss: 0.4212
Epoch: 037/130 | Batch 0100/0313 | Loss: 0.2716
Epoch: 037/130 | Batch 0150/0313 | Loss: 0.3790
Epoch: 037/130 | Batch 0200/0313 | Loss: 0.4277
Epoch: 037/130 | Batch 0250/0313 | Loss: 0.5099
Epoch: 037/130 | Batch 0300/0313 | Loss: 0.4466
**Epoch: 037/130 | Train. Acc.: 82.847% | Loss: 0.4939
**Epoch: 037/130 | Valid. Acc.: 84.400% | Loss: 0.4486
Time elapsed: 30.05 min
Epoch: 038/130 | Current Learning Rate: 0.050000
Epoch: 038/130 | Batch 0000/0313 | Loss: 0.4712
Epoch: 038/130 | Batch 0050/0313 | Loss: 0.4705
Epoch: 038/130 | Batch 0100/0313 | Loss: 0.5535
Epoch: 038/130 | Batch 0150/0313 | Loss: 0.3387
Epoch: 038/130 | Batch 0200/0313 | Loss: 0.4598
Epoch: 038/130 | Batch 0250/0313 | Loss: 0.4612
Epoch: 038/130 | Batch 0300/0313 | Loss: 0.4736
**Epoch: 038/130 | Train. Acc.: 81.005% | Loss: 0.5280
**Epoch: 038/130 | Valid. Acc.: 83.790% | Loss: 0.4747
Epoch 00038: reducing learning rate of group 0 to 2.5000e-02.
Time elapsed: 30.87 min
Epoch: 039/130 | Current Learning Rate: 0.025000
Epoch: 039/130 | Batch 0000/0313 | Loss: 0.4706
Epoch: 039/130 | Batch 0050/0313 | Loss: 0.2304
Epoch: 039/130 | Batch 0100/0313 | Loss: 0.2740
Epoch: 039/130 | Batch 0150/0313 | Loss: 0.4906
Epoch: 039/130 | Batch 0200/0313 | Loss: 0.2984
Epoch: 039/130 | Batch 0250/0313 | Loss: 0.3818
Epoch: 039/130 | Batch 0300/0313 | Loss: 0.4415
**Epoch: 039/130 | Train. Acc.: 87.888% | Loss: 0.3436
**Epoch: 039/130 | Valid. Acc.: 87.690% | Loss: 0.3598
**Validation loss decreased (0.442654 --> 0.359767). Saving model ...
Time elapsed: 31.68 min
Epoch: 040/130 | Current Learning Rate: 0.025000
Epoch: 040/130 | Batch 0000/0313 | Loss: 0.3486
Epoch: 040/130 | Batch 0050/0313 | Loss: 0.1899
Epoch: 040/130 | Batch 0100/0313 | Loss: 0.3042
Epoch: 040/130 | Batch 0150/0313 | Loss: 0.2432
Epoch: 040/130 | Batch 0200/0313 | Loss: 0.3984
Epoch: 040/130 | Batch 0250/0313 | Loss: 0.3242
Epoch: 040/130 | Batch 0300/0313 | Loss: 0.3644
**Epoch: 040/130 | Train. Acc.: 89.142% | Loss: 0.3141
**Epoch: 040/130 | Valid. Acc.: 88.350% | Loss: 0.3384
**Validation loss decreased (0.359767 --> 0.338402). Saving model ...
Time elapsed: 32.49 min
Epoch: 041/130 | Current Learning Rate: 0.025000
Epoch: 041/130 | Batch 0000/0313 | Loss: 0.3776
Epoch: 041/130 | Batch 0050/0313 | Loss: 0.3051
Epoch: 041/130 | Batch 0100/0313 | Loss: 0.2388
Epoch: 041/130 | Batch 0150/0313 | Loss: 0.3742
Epoch: 041/130 | Batch 0200/0313 | Loss: 0.4742
Epoch: 041/130 | Batch 0250/0313 | Loss: 0.4450
Epoch: 041/130 | Batch 0300/0313 | Loss: 0.2289
**Epoch: 041/130 | Train. Acc.: 88.957% | Loss: 0.3183
**Epoch: 041/130 | Valid. Acc.: 88.460% | Loss: 0.3437
Time elapsed: 33.31 min
Epoch: 042/130 | Current Learning Rate: 0.025000
Epoch: 042/130 | Batch 0000/0313 | Loss: 0.2266
Epoch: 042/130 | Batch 0050/0313 | Loss: 0.2999
Epoch: 042/130 | Batch 0100/0313 | Loss: 0.3621
Epoch: 042/130 | Batch 0150/0313 | Loss: 0.2489
Epoch: 042/130 | Batch 0200/0313 | Loss: 0.3841
Epoch: 042/130 | Batch 0250/0313 | Loss: 0.2727
Epoch: 042/130 | Batch 0300/0313 | Loss: 0.4434
**Epoch: 042/130 | Train. Acc.: 89.343% | Loss: 0.3052
**Epoch: 042/130 | Valid. Acc.: 89.120% | Loss: 0.3280
**Validation loss decreased (0.338402 --> 0.327950). Saving model ...
Time elapsed: 34.13 min
Epoch: 043/130 | Current Learning Rate: 0.025000
Epoch: 043/130 | Batch 0000/0313 | Loss: 0.3595
Epoch: 043/130 | Batch 0050/0313 | Loss: 0.2740
Epoch: 043/130 | Batch 0100/0313 | Loss: 0.2223
Epoch: 043/130 | Batch 0150/0313 | Loss: 0.2727
Epoch: 043/130 | Batch 0200/0313 | Loss: 0.2550
Epoch: 043/130 | Batch 0250/0313 | Loss: 0.3809
Epoch: 043/130 | Batch 0300/0313 | Loss: 0.2209
**Epoch: 043/130 | Train. Acc.: 88.157% | Loss: 0.3399
**Epoch: 043/130 | Valid. Acc.: 88.200% | Loss: 0.3580
Time elapsed: 34.94 min
Epoch: 044/130 | Current Learning Rate: 0.025000
Epoch: 044/130 | Batch 0000/0313 | Loss: 0.3423
Epoch: 044/130 | Batch 0050/0313 | Loss: 0.2091
Epoch: 044/130 | Batch 0100/0313 | Loss: 0.4695
Epoch: 044/130 | Batch 0150/0313 | Loss: 0.3096
Epoch: 044/130 | Batch 0200/0313 | Loss: 0.2243
Epoch: 044/130 | Batch 0250/0313 | Loss: 0.2903
Epoch: 044/130 | Batch 0300/0313 | Loss: 0.3740
**Epoch: 044/130 | Train. Acc.: 86.323% | Loss: 0.3818
**Epoch: 044/130 | Valid. Acc.: 86.480% | Loss: 0.4122
Time elapsed: 35.74 min
Epoch: 045/130 | Current Learning Rate: 0.025000
Epoch: 045/130 | Batch 0000/0313 | Loss: 0.2585
Epoch: 045/130 | Batch 0050/0313 | Loss: 0.3469
Epoch: 045/130 | Batch 0100/0313 | Loss: 0.3161
Epoch: 045/130 | Batch 0150/0313 | Loss: 0.3594
Epoch: 045/130 | Batch 0200/0313 | Loss: 0.3038
Epoch: 045/130 | Batch 0250/0313 | Loss: 0.2296
Epoch: 045/130 | Batch 0300/0313 | Loss: 0.2961
**Epoch: 045/130 | Train. Acc.: 88.560% | Loss: 0.3284
**Epoch: 045/130 | Valid. Acc.: 88.250% | Loss: 0.3563
Time elapsed: 36.56 min
Epoch: 046/130 | Current Learning Rate: 0.025000
Epoch: 046/130 | Batch 0000/0313 | Loss: 0.2900
Epoch: 046/130 | Batch 0050/0313 | Loss: 0.3045
Epoch: 046/130 | Batch 0100/0313 | Loss: 0.4476
Epoch: 046/130 | Batch 0150/0313 | Loss: 0.2595
Epoch: 046/130 | Batch 0200/0313 | Loss: 0.3121
Epoch: 046/130 | Batch 0250/0313 | Loss: 0.2707
Epoch: 046/130 | Batch 0300/0313 | Loss: 0.1936
**Epoch: 046/130 | Train. Acc.: 88.992% | Loss: 0.3111
**Epoch: 046/130 | Valid. Acc.: 87.670% | Loss: 0.3697
Time elapsed: 37.37 min
Epoch: 047/130 | Current Learning Rate: 0.025000
Epoch: 047/130 | Batch 0000/0313 | Loss: 0.2444
Epoch: 047/130 | Batch 0050/0313 | Loss: 0.2955
Epoch: 047/130 | Batch 0100/0313 | Loss: 0.2921
Epoch: 047/130 | Batch 0150/0313 | Loss: 0.2666
Epoch: 047/130 | Batch 0200/0313 | Loss: 0.3978
Epoch: 047/130 | Batch 0250/0313 | Loss: 0.2990
Epoch: 047/130 | Batch 0300/0313 | Loss: 0.2683
**Epoch: 047/130 | Train. Acc.: 86.645% | Loss: 0.3813
**Epoch: 047/130 | Valid. Acc.: 86.230% | Loss: 0.4348
Time elapsed: 38.18 min
Epoch: 048/130 | Current Learning Rate: 0.025000
Epoch: 048/130 | Batch 0000/0313 | Loss: 0.3111
Epoch: 048/130 | Batch 0050/0313 | Loss: 0.3975
Epoch: 048/130 | Batch 0100/0313 | Loss: 0.2901
Epoch: 048/130 | Batch 0150/0313 | Loss: 0.2407
Epoch: 048/130 | Batch 0200/0313 | Loss: 0.2655
Epoch: 048/130 | Batch 0250/0313 | Loss: 0.2527
Epoch: 048/130 | Batch 0300/0313 | Loss: 0.2302
**Epoch: 048/130 | Train. Acc.: 87.505% | Loss: 0.3574
**Epoch: 048/130 | Valid. Acc.: 87.540% | Loss: 0.3783
Time elapsed: 39.00 min
Epoch: 049/130 | Current Learning Rate: 0.025000
Epoch: 049/130 | Batch 0000/0313 | Loss: 0.2930
Epoch: 049/130 | Batch 0050/0313 | Loss: 0.2659
Epoch: 049/130 | Batch 0100/0313 | Loss: 0.3739
Epoch: 049/130 | Batch 0150/0313 | Loss: 0.3655
Epoch: 049/130 | Batch 0200/0313 | Loss: 0.3514
Epoch: 049/130 | Batch 0250/0313 | Loss: 0.3374
Epoch: 049/130 | Batch 0300/0313 | Loss: 0.2909
**Epoch: 049/130 | Train. Acc.: 87.700% | Loss: 0.3527
**Epoch: 049/130 | Valid. Acc.: 87.830% | Loss: 0.3744
Epoch 00049: reducing learning rate of group 0 to 1.2500e-02.
Time elapsed: 39.80 min
Epoch: 050/130 | Current Learning Rate: 0.012500
Epoch: 050/130 | Batch 0000/0313 | Loss: 0.2663
Epoch: 050/130 | Batch 0050/0313 | Loss: 0.2481
Epoch: 050/130 | Batch 0100/0313 | Loss: 0.2904
Epoch: 050/130 | Batch 0150/0313 | Loss: 0.1902
Epoch: 050/130 | Batch 0200/0313 | Loss: 0.2156
Epoch: 050/130 | Batch 0250/0313 | Loss: 0.2489
Epoch: 050/130 | Batch 0300/0313 | Loss: 0.1978
**Epoch: 050/130 | Train. Acc.: 93.015% | Loss: 0.1967
**Epoch: 050/130 | Valid. Acc.: 91.430% | Loss: 0.2661
**Validation loss decreased (0.327950 --> 0.266073). Saving model ...
Time elapsed: 40.61 min
Epoch: 051/130 | Current Learning Rate: 0.012500
Epoch: 051/130 | Batch 0000/0313 | Loss: 0.2102
Epoch: 051/130 | Batch 0050/0313 | Loss: 0.1305
Epoch: 051/130 | Batch 0100/0313 | Loss: 0.1384
Epoch: 051/130 | Batch 0150/0313 | Loss: 0.2526
Epoch: 051/130 | Batch 0200/0313 | Loss: 0.1460
Epoch: 051/130 | Batch 0250/0313 | Loss: 0.2539
Epoch: 051/130 | Batch 0300/0313 | Loss: 0.1969
**Epoch: 051/130 | Train. Acc.: 93.315% | Loss: 0.1898
**Epoch: 051/130 | Valid. Acc.: 90.770% | Loss: 0.2811
Time elapsed: 41.42 min
Epoch: 052/130 | Current Learning Rate: 0.012500
Epoch: 052/130 | Batch 0000/0313 | Loss: 0.1706
Epoch: 052/130 | Batch 0050/0313 | Loss: 0.1346
Epoch: 052/130 | Batch 0100/0313 | Loss: 0.2153
Epoch: 052/130 | Batch 0150/0313 | Loss: 0.2015
Epoch: 052/130 | Batch 0200/0313 | Loss: 0.2963
Epoch: 052/130 | Batch 0250/0313 | Loss: 0.2868
Epoch: 052/130 | Batch 0300/0313 | Loss: 0.2290
**Epoch: 052/130 | Train. Acc.: 93.080% | Loss: 0.1964
**Epoch: 052/130 | Valid. Acc.: 90.910% | Loss: 0.2794
Time elapsed: 42.23 min
Epoch: 053/130 | Current Learning Rate: 0.012500
Epoch: 053/130 | Batch 0000/0313 | Loss: 0.2118
Epoch: 053/130 | Batch 0050/0313 | Loss: 0.2545
Epoch: 053/130 | Batch 0100/0313 | Loss: 0.1185
Epoch: 053/130 | Batch 0150/0313 | Loss: 0.1458
Epoch: 053/130 | Batch 0200/0313 | Loss: 0.2567
Epoch: 053/130 | Batch 0250/0313 | Loss: 0.1763
Epoch: 053/130 | Batch 0300/0313 | Loss: 0.2836
**Epoch: 053/130 | Train. Acc.: 93.315% | Loss: 0.1900
**Epoch: 053/130 | Valid. Acc.: 90.660% | Loss: 0.2927
Time elapsed: 43.04 min
Epoch: 054/130 | Current Learning Rate: 0.012500
Epoch: 054/130 | Batch 0000/0313 | Loss: 0.2160
Epoch: 054/130 | Batch 0050/0313 | Loss: 0.1681
Epoch: 054/130 | Batch 0100/0313 | Loss: 0.1163
Epoch: 054/130 | Batch 0150/0313 | Loss: 0.2333
Epoch: 054/130 | Batch 0200/0313 | Loss: 0.0922
Epoch: 054/130 | Batch 0250/0313 | Loss: 0.2298
Epoch: 054/130 | Batch 0300/0313 | Loss: 0.1464
**Epoch: 054/130 | Train. Acc.: 93.815% | Loss: 0.1780
**Epoch: 054/130 | Valid. Acc.: 91.000% | Loss: 0.2759
Time elapsed: 43.85 min
Epoch: 055/130 | Current Learning Rate: 0.012500
Epoch: 055/130 | Batch 0000/0313 | Loss: 0.1443
Epoch: 055/130 | Batch 0050/0313 | Loss: 0.1651
Epoch: 055/130 | Batch 0100/0313 | Loss: 0.1388
Epoch: 055/130 | Batch 0150/0313 | Loss: 0.2527
Epoch: 055/130 | Batch 0200/0313 | Loss: 0.1780
Epoch: 055/130 | Batch 0250/0313 | Loss: 0.1822
Epoch: 055/130 | Batch 0300/0313 | Loss: 0.1731
**Epoch: 055/130 | Train. Acc.: 93.430% | Loss: 0.1856
**Epoch: 055/130 | Valid. Acc.: 90.500% | Loss: 0.2915
Time elapsed: 44.66 min
Epoch: 056/130 | Current Learning Rate: 0.012500
Epoch: 056/130 | Batch 0000/0313 | Loss: 0.1442
Epoch: 056/130 | Batch 0050/0313 | Loss: 0.1899
Epoch: 056/130 | Batch 0100/0313 | Loss: 0.2242
Epoch: 056/130 | Batch 0150/0313 | Loss: 0.1527
Epoch: 056/130 | Batch 0200/0313 | Loss: 0.1798
Epoch: 056/130 | Batch 0250/0313 | Loss: 0.1290
Epoch: 056/130 | Batch 0300/0313 | Loss: 0.1584
**Epoch: 056/130 | Train. Acc.: 93.235% | Loss: 0.1954
**Epoch: 056/130 | Valid. Acc.: 90.560% | Loss: 0.3111
Time elapsed: 45.46 min
Epoch: 057/130 | Current Learning Rate: 0.012500
Epoch: 057/130 | Batch 0000/0313 | Loss: 0.1761
Epoch: 057/130 | Batch 0050/0313 | Loss: 0.1817
Epoch: 057/130 | Batch 0100/0313 | Loss: 0.3079
Epoch: 057/130 | Batch 0150/0313 | Loss: 0.2189
Epoch: 057/130 | Batch 0200/0313 | Loss: 0.1827
Epoch: 057/130 | Batch 0250/0313 | Loss: 0.1984
Epoch: 057/130 | Batch 0300/0313 | Loss: 0.1456
**Epoch: 057/130 | Train. Acc.: 93.195% | Loss: 0.1955
**Epoch: 057/130 | Valid. Acc.: 90.580% | Loss: 0.2986
Epoch 00057: reducing learning rate of group 0 to 6.2500e-03.
Time elapsed: 46.27 min
Epoch: 058/130 | Current Learning Rate: 0.006250
Epoch: 058/130 | Batch 0000/0313 | Loss: 0.1084
Epoch: 058/130 | Batch 0050/0313 | Loss: 0.1377
Epoch: 058/130 | Batch 0100/0313 | Loss: 0.1280
Epoch: 058/130 | Batch 0150/0313 | Loss: 0.1251
Epoch: 058/130 | Batch 0200/0313 | Loss: 0.1177
Epoch: 058/130 | Batch 0250/0313 | Loss: 0.1718
Epoch: 058/130 | Batch 0300/0313 | Loss: 0.1286
**Epoch: 058/130 | Train. Acc.: 96.178% | Loss: 0.1126
**Epoch: 058/130 | Valid. Acc.: 92.130% | Loss: 0.2444
**Validation loss decreased (0.266073 --> 0.244379). Saving model ...
Time elapsed: 47.08 min
Epoch: 059/130 | Current Learning Rate: 0.006250
Epoch: 059/130 | Batch 0000/0313 | Loss: 0.1584
Epoch: 059/130 | Batch 0050/0313 | Loss: 0.1649
Epoch: 059/130 | Batch 0100/0313 | Loss: 0.0686
Epoch: 059/130 | Batch 0150/0313 | Loss: 0.1261
Epoch: 059/130 | Batch 0200/0313 | Loss: 0.1273
Epoch: 059/130 | Batch 0250/0313 | Loss: 0.1245
Epoch: 059/130 | Batch 0300/0313 | Loss: 0.1023
**Epoch: 059/130 | Train. Acc.: 96.303% | Loss: 0.1085
**Epoch: 059/130 | Valid. Acc.: 92.240% | Loss: 0.2558
Time elapsed: 47.89 min
Epoch: 060/130 | Current Learning Rate: 0.006250
Epoch: 060/130 | Batch 0000/0313 | Loss: 0.1135
Epoch: 060/130 | Batch 0050/0313 | Loss: 0.1153
Epoch: 060/130 | Batch 0100/0313 | Loss: 0.1451
Epoch: 060/130 | Batch 0150/0313 | Loss: 0.0771
Epoch: 060/130 | Batch 0200/0313 | Loss: 0.1459
Epoch: 060/130 | Batch 0250/0313 | Loss: 0.1608
Epoch: 060/130 | Batch 0300/0313 | Loss: 0.0755
**Epoch: 060/130 | Train. Acc.: 96.422% | Loss: 0.1045
**Epoch: 060/130 | Valid. Acc.: 92.040% | Loss: 0.2642
Time elapsed: 48.71 min
Epoch: 061/130 | Current Learning Rate: 0.006250
Epoch: 061/130 | Batch 0000/0313 | Loss: 0.1120
Epoch: 061/130 | Batch 0050/0313 | Loss: 0.1157
Epoch: 061/130 | Batch 0100/0313 | Loss: 0.1182
Epoch: 061/130 | Batch 0150/0313 | Loss: 0.0900
Epoch: 061/130 | Batch 0200/0313 | Loss: 0.1601
Epoch: 061/130 | Batch 0250/0313 | Loss: 0.0762
Epoch: 061/130 | Batch 0300/0313 | Loss: 0.1315
**Epoch: 061/130 | Train. Acc.: 96.125% | Loss: 0.1094
**Epoch: 061/130 | Valid. Acc.: 92.000% | Loss: 0.2661
Time elapsed: 49.51 min
Epoch: 062/130 | Current Learning Rate: 0.006250
Epoch: 062/130 | Batch 0000/0313 | Loss: 0.1356
Epoch: 062/130 | Batch 0050/0313 | Loss: 0.0653
Epoch: 062/130 | Batch 0100/0313 | Loss: 0.0951
Epoch: 062/130 | Batch 0150/0313 | Loss: 0.0806
Epoch: 062/130 | Batch 0200/0313 | Loss: 0.0811
Epoch: 062/130 | Batch 0250/0313 | Loss: 0.1127
Epoch: 062/130 | Batch 0300/0313 | Loss: 0.0886
**Epoch: 062/130 | Train. Acc.: 96.418% | Loss: 0.1055
**Epoch: 062/130 | Valid. Acc.: 92.220% | Loss: 0.2593
Time elapsed: 50.33 min
Epoch: 063/130 | Current Learning Rate: 0.006250
Epoch: 063/130 | Batch 0000/0313 | Loss: 0.0760
Epoch: 063/130 | Batch 0050/0313 | Loss: 0.1359
Epoch: 063/130 | Batch 0100/0313 | Loss: 0.1686
Epoch: 063/130 | Batch 0150/0313 | Loss: 0.0458
Epoch: 063/130 | Batch 0200/0313 | Loss: 0.0815
Epoch: 063/130 | Batch 0250/0313 | Loss: 0.1334
Epoch: 063/130 | Batch 0300/0313 | Loss: 0.1018
**Epoch: 063/130 | Train. Acc.: 96.528% | Loss: 0.1027
**Epoch: 063/130 | Valid. Acc.: 92.280% | Loss: 0.2540
Time elapsed: 51.14 min
Epoch: 064/130 | Current Learning Rate: 0.006250
Epoch: 064/130 | Batch 0000/0313 | Loss: 0.1064
Epoch: 064/130 | Batch 0050/0313 | Loss: 0.1122
Epoch: 064/130 | Batch 0100/0313 | Loss: 0.1186
Epoch: 064/130 | Batch 0150/0313 | Loss: 0.0840
Epoch: 064/130 | Batch 0200/0313 | Loss: 0.0862
Epoch: 064/130 | Batch 0250/0313 | Loss: 0.0422
Epoch: 064/130 | Batch 0300/0313 | Loss: 0.0492
**Epoch: 064/130 | Train. Acc.: 96.778% | Loss: 0.0953
**Epoch: 064/130 | Valid. Acc.: 92.370% | Loss: 0.2548
Time elapsed: 51.95 min
Epoch: 065/130 | Current Learning Rate: 0.006250
Epoch: 065/130 | Batch 0000/0313 | Loss: 0.1868
Epoch: 065/130 | Batch 0050/0313 | Loss: 0.0675
Epoch: 065/130 | Batch 0100/0313 | Loss: 0.1107
Epoch: 065/130 | Batch 0150/0313 | Loss: 0.1048
Epoch: 065/130 | Batch 0200/0313 | Loss: 0.1147
Epoch: 065/130 | Batch 0250/0313 | Loss: 0.1238
Epoch: 065/130 | Batch 0300/0313 | Loss: 0.0674
**Epoch: 065/130 | Train. Acc.: 96.540% | Loss: 0.0980
**Epoch: 065/130 | Valid. Acc.: 92.420% | Loss: 0.2609
Epoch 00065: reducing learning rate of group 0 to 3.1250e-03.
Time elapsed: 52.76 min
Epoch: 066/130 | Current Learning Rate: 0.003125
Epoch: 066/130 | Batch 0000/0313 | Loss: 0.0841
Epoch: 066/130 | Batch 0050/0313 | Loss: 0.0984
Epoch: 066/130 | Batch 0100/0313 | Loss: 0.0500
Epoch: 066/130 | Batch 0150/0313 | Loss: 0.0240
Epoch: 066/130 | Batch 0200/0313 | Loss: 0.0361
Epoch: 066/130 | Batch 0250/0313 | Loss: 0.0886
Epoch: 066/130 | Batch 0300/0313 | Loss: 0.0577
**Epoch: 066/130 | Train. Acc.: 97.940% | Loss: 0.0651
**Epoch: 066/130 | Valid. Acc.: 92.970% | Loss: 0.2329
**Validation loss decreased (0.244379 --> 0.232946). Saving model ...
Time elapsed: 53.58 min
Epoch: 067/130 | Current Learning Rate: 0.003125
Epoch: 067/130 | Batch 0000/0313 | Loss: 0.0605
Epoch: 067/130 | Batch 0050/0313 | Loss: 0.1053
Epoch: 067/130 | Batch 0100/0313 | Loss: 0.0222
Epoch: 067/130 | Batch 0150/0313 | Loss: 0.0848
Epoch: 067/130 | Batch 0200/0313 | Loss: 0.0428
Epoch: 067/130 | Batch 0250/0313 | Loss: 0.0972
Epoch: 067/130 | Batch 0300/0313 | Loss: 0.0881
**Epoch: 067/130 | Train. Acc.: 98.172% | Loss: 0.0569
**Epoch: 067/130 | Valid. Acc.: 93.030% | Loss: 0.2340
Time elapsed: 54.40 min
Epoch: 068/130 | Current Learning Rate: 0.003125
Epoch: 068/130 | Batch 0000/0313 | Loss: 0.0928
Epoch: 068/130 | Batch 0050/0313 | Loss: 0.0912
Epoch: 068/130 | Batch 0100/0313 | Loss: 0.0415
Epoch: 068/130 | Batch 0150/0313 | Loss: 0.1008
Epoch: 068/130 | Batch 0200/0313 | Loss: 0.1038
Epoch: 068/130 | Batch 0250/0313 | Loss: 0.0481
Epoch: 068/130 | Batch 0300/0313 | Loss: 0.1015
**Epoch: 068/130 | Train. Acc.: 98.108% | Loss: 0.0558
**Epoch: 068/130 | Valid. Acc.: 93.060% | Loss: 0.2431
Time elapsed: 55.21 min
Epoch: 069/130 | Current Learning Rate: 0.003125
Epoch: 069/130 | Batch 0000/0313 | Loss: 0.0490
Epoch: 069/130 | Batch 0050/0313 | Loss: 0.1313
Epoch: 069/130 | Batch 0100/0313 | Loss: 0.0389
Epoch: 069/130 | Batch 0150/0313 | Loss: 0.0433
Epoch: 069/130 | Batch 0200/0313 | Loss: 0.1074
Epoch: 069/130 | Batch 0250/0313 | Loss: 0.0524
Epoch: 069/130 | Batch 0300/0313 | Loss: 0.1332
**Epoch: 069/130 | Train. Acc.: 98.355% | Loss: 0.0507
**Epoch: 069/130 | Valid. Acc.: 92.790% | Loss: 0.2485
Time elapsed: 56.02 min
Epoch: 070/130 | Current Learning Rate: 0.003125
Epoch: 070/130 | Batch 0000/0313 | Loss: 0.0503
Epoch: 070/130 | Batch 0050/0313 | Loss: 0.0213
Epoch: 070/130 | Batch 0100/0313 | Loss: 0.1353
Epoch: 070/130 | Batch 0150/0313 | Loss: 0.0929
Epoch: 070/130 | Batch 0200/0313 | Loss: 0.0981
Epoch: 070/130 | Batch 0250/0313 | Loss: 0.0974
Epoch: 070/130 | Batch 0300/0313 | Loss: 0.1342
**Epoch: 070/130 | Train. Acc.: 98.350% | Loss: 0.0517
**Epoch: 070/130 | Valid. Acc.: 92.970% | Loss: 0.2545
Time elapsed: 56.83 min
Epoch: 071/130 | Current Learning Rate: 0.003125
Epoch: 071/130 | Batch 0000/0313 | Loss: 0.0539
Epoch: 071/130 | Batch 0050/0313 | Loss: 0.0499
Epoch: 071/130 | Batch 0100/0313 | Loss: 0.0457
Epoch: 071/130 | Batch 0150/0313 | Loss: 0.0542
Epoch: 071/130 | Batch 0200/0313 | Loss: 0.0421
Epoch: 071/130 | Batch 0250/0313 | Loss: 0.0371
Epoch: 071/130 | Batch 0300/0313 | Loss: 0.1136
**Epoch: 071/130 | Train. Acc.: 98.317% | Loss: 0.0512
**Epoch: 071/130 | Valid. Acc.: 92.750% | Loss: 0.2581
Time elapsed: 57.64 min
Epoch: 072/130 | Current Learning Rate: 0.003125
Epoch: 072/130 | Batch 0000/0313 | Loss: 0.0436
Epoch: 072/130 | Batch 0050/0313 | Loss: 0.0899
Epoch: 072/130 | Batch 0100/0313 | Loss: 0.0303
Epoch: 072/130 | Batch 0150/0313 | Loss: 0.0445
Epoch: 072/130 | Batch 0200/0313 | Loss: 0.1145
Epoch: 072/130 | Batch 0250/0313 | Loss: 0.0517
Epoch: 072/130 | Batch 0300/0313 | Loss: 0.0394
**Epoch: 072/130 | Train. Acc.: 98.450% | Loss: 0.0482
**Epoch: 072/130 | Valid. Acc.: 93.120% | Loss: 0.2482
Time elapsed: 58.45 min
Epoch: 073/130 | Current Learning Rate: 0.003125
Epoch: 073/130 | Batch 0000/0313 | Loss: 0.0625
Epoch: 073/130 | Batch 0050/0313 | Loss: 0.0960
Epoch: 073/130 | Batch 0100/0313 | Loss: 0.0761
Epoch: 073/130 | Batch 0150/0313 | Loss: 0.0521
Epoch: 073/130 | Batch 0200/0313 | Loss: 0.0278
Epoch: 073/130 | Batch 0250/0313 | Loss: 0.0539
Epoch: 073/130 | Batch 0300/0313 | Loss: 0.0546
**Epoch: 073/130 | Train. Acc.: 98.575% | Loss: 0.0462
**Epoch: 073/130 | Valid. Acc.: 93.190% | Loss: 0.2497
Epoch 00073: reducing learning rate of group 0 to 1.5625e-03.
Time elapsed: 59.26 min
Epoch: 074/130 | Current Learning Rate: 0.001563
Epoch: 074/130 | Batch 0000/0313 | Loss: 0.0535
Epoch: 074/130 | Batch 0050/0313 | Loss: 0.0439
Epoch: 074/130 | Batch 0100/0313 | Loss: 0.0446
Epoch: 074/130 | Batch 0150/0313 | Loss: 0.0241
Epoch: 074/130 | Batch 0200/0313 | Loss: 0.0440
Epoch: 074/130 | Batch 0250/0313 | Loss: 0.0472
Epoch: 074/130 | Batch 0300/0313 | Loss: 0.0301
**Epoch: 074/130 | Train. Acc.: 98.933% | Loss: 0.0358
**Epoch: 074/130 | Valid. Acc.: 93.430% | Loss: 0.2379
Time elapsed: 60.08 min
Epoch: 075/130 | Current Learning Rate: 0.001563
Epoch: 075/130 | Batch 0000/0313 | Loss: 0.0306
Epoch: 075/130 | Batch 0050/0313 | Loss: 0.0651
Epoch: 075/130 | Batch 0100/0313 | Loss: 0.0533
Epoch: 075/130 | Batch 0150/0313 | Loss: 0.0284
Epoch: 075/130 | Batch 0200/0313 | Loss: 0.0557
Epoch: 075/130 | Batch 0250/0313 | Loss: 0.0158
Epoch: 075/130 | Batch 0300/0313 | Loss: 0.0424
**Epoch: 075/130 | Train. Acc.: 98.920% | Loss: 0.0341
**Epoch: 075/130 | Valid. Acc.: 93.700% | Loss: 0.2421
Time elapsed: 60.89 min
Epoch: 076/130 | Current Learning Rate: 0.001563
Epoch: 076/130 | Batch 0000/0313 | Loss: 0.1001
Epoch: 076/130 | Batch 0050/0313 | Loss: 0.0374
Epoch: 076/130 | Batch 0100/0313 | Loss: 0.0286
Epoch: 076/130 | Batch 0150/0313 | Loss: 0.0457
Epoch: 076/130 | Batch 0200/0313 | Loss: 0.0467
Epoch: 076/130 | Batch 0250/0313 | Loss: 0.0933
Epoch: 076/130 | Batch 0300/0313 | Loss: 0.0308
**Epoch: 076/130 | Train. Acc.: 98.940% | Loss: 0.0333
**Epoch: 076/130 | Valid. Acc.: 93.580% | Loss: 0.2421
Time elapsed: 61.70 min
Epoch: 077/130 | Current Learning Rate: 0.001563
Epoch: 077/130 | Batch 0000/0313 | Loss: 0.0531
Epoch: 077/130 | Batch 0050/0313 | Loss: 0.0513
Epoch: 077/130 | Batch 0100/0313 | Loss: 0.0267
Epoch: 077/130 | Batch 0150/0313 | Loss: 0.0459
Epoch: 077/130 | Batch 0200/0313 | Loss: 0.0662
Epoch: 077/130 | Batch 0250/0313 | Loss: 0.0258
Epoch: 077/130 | Batch 0300/0313 | Loss: 0.0467
**Epoch: 077/130 | Train. Acc.: 99.025% | Loss: 0.0312
**Epoch: 077/130 | Valid. Acc.: 93.490% | Loss: 0.2474
Time elapsed: 62.52 min
Epoch: 078/130 | Current Learning Rate: 0.001563
Epoch: 078/130 | Batch 0000/0313 | Loss: 0.0143
Epoch: 078/130 | Batch 0050/0313 | Loss: 0.0223
Epoch: 078/130 | Batch 0100/0313 | Loss: 0.0392
Epoch: 078/130 | Batch 0150/0313 | Loss: 0.0401
Epoch: 078/130 | Batch 0200/0313 | Loss: 0.0612
Epoch: 078/130 | Batch 0250/0313 | Loss: 0.0419
Epoch: 078/130 | Batch 0300/0313 | Loss: 0.0660
**Epoch: 078/130 | Train. Acc.: 99.035% | Loss: 0.0302
**Epoch: 078/130 | Valid. Acc.: 93.200% | Loss: 0.2580
Time elapsed: 63.33 min
Epoch: 079/130 | Current Learning Rate: 0.001563
Epoch: 079/130 | Batch 0000/0313 | Loss: 0.0627
Epoch: 079/130 | Batch 0050/0313 | Loss: 0.0315
Epoch: 079/130 | Batch 0100/0313 | Loss: 0.0556
Epoch: 079/130 | Batch 0150/0313 | Loss: 0.0302
Epoch: 079/130 | Batch 0200/0313 | Loss: 0.0865
Epoch: 079/130 | Batch 0250/0313 | Loss: 0.0152
Epoch: 079/130 | Batch 0300/0313 | Loss: 0.0299
**Epoch: 079/130 | Train. Acc.: 99.108% | Loss: 0.0301
**Epoch: 079/130 | Valid. Acc.: 93.390% | Loss: 0.2478
Time elapsed: 64.14 min
Epoch: 080/130 | Current Learning Rate: 0.001563
Epoch: 080/130 | Batch 0000/0313 | Loss: 0.0630
Epoch: 080/130 | Batch 0050/0313 | Loss: 0.0173
Epoch: 080/130 | Batch 0100/0313 | Loss: 0.0129
Epoch: 080/130 | Batch 0150/0313 | Loss: 0.0230
Epoch: 080/130 | Batch 0200/0313 | Loss: 0.0159
Epoch: 080/130 | Batch 0250/0313 | Loss: 0.0138
Epoch: 080/130 | Batch 0300/0313 | Loss: 0.0702
**Epoch: 080/130 | Train. Acc.: 99.080% | Loss: 0.0293
**Epoch: 080/130 | Valid. Acc.: 93.310% | Loss: 0.2561
Epoch 00080: reducing learning rate of group 0 to 7.8125e-04.
Time elapsed: 64.95 min
Epoch: 081/130 | Current Learning Rate: 0.000781
Epoch: 081/130 | Batch 0000/0313 | Loss: 0.0200
Epoch: 081/130 | Batch 0050/0313 | Loss: 0.0190
Epoch: 081/130 | Batch 0100/0313 | Loss: 0.0757
Epoch: 081/130 | Batch 0150/0313 | Loss: 0.0354
Epoch: 081/130 | Batch 0200/0313 | Loss: 0.0795
Epoch: 081/130 | Batch 0250/0313 | Loss: 0.0580
Epoch: 081/130 | Batch 0300/0313 | Loss: 0.0543
**Epoch: 081/130 | Train. Acc.: 99.350% | Loss: 0.0232
**Epoch: 081/130 | Valid. Acc.: 93.370% | Loss: 0.2510
Time elapsed: 65.76 min
Epoch: 082/130 | Current Learning Rate: 0.000781
Epoch: 082/130 | Batch 0000/0313 | Loss: 0.0195
Epoch: 082/130 | Batch 0050/0313 | Loss: 0.0594
Epoch: 082/130 | Batch 0100/0313 | Loss: 0.0081
Epoch: 082/130 | Batch 0150/0313 | Loss: 0.0633
Epoch: 082/130 | Batch 0200/0313 | Loss: 0.0513
Epoch: 082/130 | Batch 0250/0313 | Loss: 0.0209
Epoch: 082/130 | Batch 0300/0313 | Loss: 0.0893
**Epoch: 082/130 | Train. Acc.: 99.362% | Loss: 0.0229
**Epoch: 082/130 | Valid. Acc.: 93.530% | Loss: 0.2455
Time elapsed: 66.57 min
Epoch: 083/130 | Current Learning Rate: 0.000781
Epoch: 083/130 | Batch 0000/0313 | Loss: 0.0671
Epoch: 083/130 | Batch 0050/0313 | Loss: 0.0323
Epoch: 083/130 | Batch 0100/0313 | Loss: 0.0627
Epoch: 083/130 | Batch 0150/0313 | Loss: 0.1132
Epoch: 083/130 | Batch 0200/0313 | Loss: 0.0278
Epoch: 083/130 | Batch 0250/0313 | Loss: 0.0296
Epoch: 083/130 | Batch 0300/0313 | Loss: 0.0345
**Epoch: 083/130 | Train. Acc.: 99.272% | Loss: 0.0245
**Epoch: 083/130 | Valid. Acc.: 93.620% | Loss: 0.2450
Time elapsed: 67.38 min
Epoch: 084/130 | Current Learning Rate: 0.000781
Epoch: 084/130 | Batch 0000/0313 | Loss: 0.0364
Epoch: 084/130 | Batch 0050/0313 | Loss: 0.0545
Epoch: 084/130 | Batch 0100/0313 | Loss: 0.0179
Epoch: 084/130 | Batch 0150/0313 | Loss: 0.0646
Epoch: 084/130 | Batch 0200/0313 | Loss: 0.0241
Epoch: 084/130 | Batch 0250/0313 | Loss: 0.0425
Epoch: 084/130 | Batch 0300/0313 | Loss: 0.0210
**Epoch: 084/130 | Train. Acc.: 99.325% | Loss: 0.0228
**Epoch: 084/130 | Valid. Acc.: 93.650% | Loss: 0.2462
Time elapsed: 68.19 min
Epoch: 085/130 | Current Learning Rate: 0.000781
Epoch: 085/130 | Batch 0000/0313 | Loss: 0.0296
Epoch: 085/130 | Batch 0050/0313 | Loss: 0.0162
Epoch: 085/130 | Batch 0100/0313 | Loss: 0.0256
Epoch: 085/130 | Batch 0150/0313 | Loss: 0.0472
Epoch: 085/130 | Batch 0200/0313 | Loss: 0.0086
Epoch: 085/130 | Batch 0250/0313 | Loss: 0.0121
Epoch: 085/130 | Batch 0300/0313 | Loss: 0.0376
**Epoch: 085/130 | Train. Acc.: 99.343% | Loss: 0.0221
**Epoch: 085/130 | Valid. Acc.: 93.570% | Loss: 0.2454
Time elapsed: 69.00 min
Epoch: 086/130 | Current Learning Rate: 0.000781
Epoch: 086/130 | Batch 0000/0313 | Loss: 0.0247
Epoch: 086/130 | Batch 0050/0313 | Loss: 0.0245
Epoch: 086/130 | Batch 0100/0313 | Loss: 0.0538
Epoch: 086/130 | Batch 0150/0313 | Loss: 0.0183
Epoch: 086/130 | Batch 0200/0313 | Loss: 0.0101
Epoch: 086/130 | Batch 0250/0313 | Loss: 0.0289
Epoch: 086/130 | Batch 0300/0313 | Loss: 0.0253
**Epoch: 086/130 | Train. Acc.: 99.400% | Loss: 0.0219
**Epoch: 086/130 | Valid. Acc.: 93.490% | Loss: 0.2516
Time elapsed: 69.81 min
Epoch: 087/130 | Current Learning Rate: 0.000781
Epoch: 087/130 | Batch 0000/0313 | Loss: 0.0242
Epoch: 087/130 | Batch 0050/0313 | Loss: 0.0273
Epoch: 087/130 | Batch 0100/0313 | Loss: 0.0184
Epoch: 087/130 | Batch 0150/0313 | Loss: 0.0526
Epoch: 087/130 | Batch 0200/0313 | Loss: 0.0422
Epoch: 087/130 | Batch 0250/0313 | Loss: 0.0562
Epoch: 087/130 | Batch 0300/0313 | Loss: 0.0467
**Epoch: 087/130 | Train. Acc.: 99.410% | Loss: 0.0211
**Epoch: 087/130 | Valid. Acc.: 93.580% | Loss: 0.2506
Epoch 00087: reducing learning rate of group 0 to 3.9063e-04.
Time elapsed: 70.62 min
Epoch: 088/130 | Current Learning Rate: 0.000391
Epoch: 088/130 | Batch 0000/0313 | Loss: 0.0124
Epoch: 088/130 | Batch 0050/0313 | Loss: 0.0339
Epoch: 088/130 | Batch 0100/0313 | Loss: 0.0445
Epoch: 088/130 | Batch 0150/0313 | Loss: 0.0280
Epoch: 088/130 | Batch 0200/0313 | Loss: 0.0183
Epoch: 088/130 | Batch 0250/0313 | Loss: 0.0695
Epoch: 088/130 | Batch 0300/0313 | Loss: 0.0301
**Epoch: 088/130 | Train. Acc.: 99.403% | Loss: 0.0201
**Epoch: 088/130 | Valid. Acc.: 93.560% | Loss: 0.2454
Time elapsed: 71.43 min
Epoch: 089/130 | Current Learning Rate: 0.000391
Epoch: 089/130 | Batch 0000/0313 | Loss: 0.0192
Epoch: 089/130 | Batch 0050/0313 | Loss: 0.0100
Epoch: 089/130 | Batch 0100/0313 | Loss: 0.0423
Epoch: 089/130 | Batch 0150/0313 | Loss: 0.0357
Epoch: 089/130 | Batch 0200/0313 | Loss: 0.0172
Epoch: 089/130 | Batch 0250/0313 | Loss: 0.0246
Epoch: 089/130 | Batch 0300/0313 | Loss: 0.0218
**Epoch: 089/130 | Train. Acc.: 99.382% | Loss: 0.0207
**Epoch: 089/130 | Valid. Acc.: 93.660% | Loss: 0.2477
Time elapsed: 72.24 min
Epoch: 090/130 | Current Learning Rate: 0.000391
Epoch: 090/130 | Batch 0000/0313 | Loss: 0.0233
Epoch: 090/130 | Batch 0050/0313 | Loss: 0.0258
Epoch: 090/130 | Batch 0100/0313 | Loss: 0.0188
Epoch: 090/130 | Batch 0150/0313 | Loss: 0.0206
Epoch: 090/130 | Batch 0200/0313 | Loss: 0.0472
Epoch: 090/130 | Batch 0250/0313 | Loss: 0.0143
Epoch: 090/130 | Batch 0300/0313 | Loss: 0.0094
**Epoch: 090/130 | Train. Acc.: 99.487% | Loss: 0.0192
**Epoch: 090/130 | Valid. Acc.: 93.600% | Loss: 0.2483
Time elapsed: 73.06 min
Epoch: 091/130 | Current Learning Rate: 0.000391
Epoch: 091/130 | Batch 0000/0313 | Loss: 0.0117
Epoch: 091/130 | Batch 0050/0313 | Loss: 0.0049
Epoch: 091/130 | Batch 0100/0313 | Loss: 0.0539
Epoch: 091/130 | Batch 0150/0313 | Loss: 0.0290
Epoch: 091/130 | Batch 0200/0313 | Loss: 0.0175
Epoch: 091/130 | Batch 0250/0313 | Loss: 0.0123
Epoch: 091/130 | Batch 0300/0313 | Loss: 0.0653
**Epoch: 091/130 | Train. Acc.: 99.492% | Loss: 0.0181
**Epoch: 091/130 | Valid. Acc.: 93.680% | Loss: 0.2502
Time elapsed: 73.86 min
Epoch: 092/130 | Current Learning Rate: 0.000391
Epoch: 092/130 | Batch 0000/0313 | Loss: 0.0250
Epoch: 092/130 | Batch 0050/0313 | Loss: 0.0059
Epoch: 092/130 | Batch 0100/0313 | Loss: 0.0198
Epoch: 092/130 | Batch 0150/0313 | Loss: 0.0195
Epoch: 092/130 | Batch 0200/0313 | Loss: 0.0706
Epoch: 092/130 | Batch 0250/0313 | Loss: 0.0354
Epoch: 092/130 | Batch 0300/0313 | Loss: 0.0186
**Epoch: 092/130 | Train. Acc.: 99.455% | Loss: 0.0190
**Epoch: 092/130 | Valid. Acc.: 93.590% | Loss: 0.2545
Time elapsed: 74.67 min
Epoch: 093/130 | Current Learning Rate: 0.000391
Epoch: 093/130 | Batch 0000/0313 | Loss: 0.0166
Epoch: 093/130 | Batch 0050/0313 | Loss: 0.0282
Epoch: 093/130 | Batch 0100/0313 | Loss: 0.0193
Epoch: 093/130 | Batch 0150/0313 | Loss: 0.0233
Epoch: 093/130 | Batch 0200/0313 | Loss: 0.0201
Epoch: 093/130 | Batch 0250/0313 | Loss: 0.0259
Epoch: 093/130 | Batch 0300/0313 | Loss: 0.0041
**Epoch: 093/130 | Train. Acc.: 99.410% | Loss: 0.0195
**Epoch: 093/130 | Valid. Acc.: 93.620% | Loss: 0.2499
Time elapsed: 75.48 min
Epoch: 094/130 | Current Learning Rate: 0.000391
Epoch: 094/130 | Batch 0000/0313 | Loss: 0.0237
Epoch: 094/130 | Batch 0050/0313 | Loss: 0.0139
Epoch: 094/130 | Batch 0100/0313 | Loss: 0.0519
Epoch: 094/130 | Batch 0150/0313 | Loss: 0.0107
Epoch: 094/130 | Batch 0200/0313 | Loss: 0.0125
Epoch: 094/130 | Batch 0250/0313 | Loss: 0.0219
Epoch: 094/130 | Batch 0300/0313 | Loss: 0.0062
**Epoch: 094/130 | Train. Acc.: 99.485% | Loss: 0.0183
**Epoch: 094/130 | Valid. Acc.: 93.770% | Loss: 0.2490
Epoch 00094: reducing learning rate of group 0 to 1.9531e-04.
Time elapsed: 76.29 min
Epoch: 095/130 | Current Learning Rate: 0.000195
Epoch: 095/130 | Batch 0000/0313 | Loss: 0.0128
Epoch: 095/130 | Batch 0050/0313 | Loss: 0.0705
Epoch: 095/130 | Batch 0100/0313 | Loss: 0.0159
Epoch: 095/130 | Batch 0150/0313 | Loss: 0.0300
Epoch: 095/130 | Batch 0200/0313 | Loss: 0.0132
Epoch: 095/130 | Batch 0250/0313 | Loss: 0.0131
Epoch: 095/130 | Batch 0300/0313 | Loss: 0.0378
**Epoch: 095/130 | Train. Acc.: 99.502% | Loss: 0.0174
**Epoch: 095/130 | Valid. Acc.: 93.710% | Loss: 0.2485
Time elapsed: 77.11 min
Epoch: 096/130 | Current Learning Rate: 0.000195
Epoch: 096/130 | Batch 0000/0313 | Loss: 0.0372
Epoch: 096/130 | Batch 0050/0313 | Loss: 0.0459
Epoch: 096/130 | Batch 0100/0313 | Loss: 0.0071
Epoch: 096/130 | Batch 0150/0313 | Loss: 0.0046
Epoch: 096/130 | Batch 0200/0313 | Loss: 0.0372
Epoch: 096/130 | Batch 0250/0313 | Loss: 0.0133
Epoch: 096/130 | Batch 0300/0313 | Loss: 0.0075
**Epoch: 096/130 | Train. Acc.: 99.530% | Loss: 0.0179
**Epoch: 096/130 | Valid. Acc.: 93.550% | Loss: 0.2488
Time elapsed: 77.92 min
Epoch: 097/130 | Current Learning Rate: 0.000195
Epoch: 097/130 | Batch 0000/0313 | Loss: 0.0248
Epoch: 097/130 | Batch 0050/0313 | Loss: 0.0319
Epoch: 097/130 | Batch 0100/0313 | Loss: 0.0104
Epoch: 097/130 | Batch 0150/0313 | Loss: 0.0344
Epoch: 097/130 | Batch 0200/0313 | Loss: 0.0127
Epoch: 097/130 | Batch 0250/0313 | Loss: 0.0196
Epoch: 097/130 | Batch 0300/0313 | Loss: 0.0194
**Epoch: 097/130 | Train. Acc.: 99.448% | Loss: 0.0194
**Epoch: 097/130 | Valid. Acc.: 93.730% | Loss: 0.2485
Time elapsed: 78.73 min
Epoch: 098/130 | Current Learning Rate: 0.000195
Epoch: 098/130 | Batch 0000/0313 | Loss: 0.0256
Epoch: 098/130 | Batch 0050/0313 | Loss: 0.0135
Epoch: 098/130 | Batch 0100/0313 | Loss: 0.0317
Epoch: 098/130 | Batch 0150/0313 | Loss: 0.0108
Epoch: 098/130 | Batch 0200/0313 | Loss: 0.0286
Epoch: 098/130 | Batch 0250/0313 | Loss: 0.0175
Epoch: 098/130 | Batch 0300/0313 | Loss: 0.0235
**Epoch: 098/130 | Train. Acc.: 99.530% | Loss: 0.0173
**Epoch: 098/130 | Valid. Acc.: 93.640% | Loss: 0.2503
Time elapsed: 79.55 min
Epoch: 099/130 | Current Learning Rate: 0.000195
Epoch: 099/130 | Batch 0000/0313 | Loss: 0.0209
Epoch: 099/130 | Batch 0050/0313 | Loss: 0.0405
Epoch: 099/130 | Batch 0100/0313 | Loss: 0.0146
Epoch: 099/130 | Batch 0150/0313 | Loss: 0.0384
Epoch: 099/130 | Batch 0200/0313 | Loss: 0.0056
Epoch: 099/130 | Batch 0250/0313 | Loss: 0.0041
Epoch: 099/130 | Batch 0300/0313 | Loss: 0.0401
**Epoch: 099/130 | Train. Acc.: 99.523% | Loss: 0.0174
**Epoch: 099/130 | Valid. Acc.: 93.710% | Loss: 0.2503
Time elapsed: 80.36 min
Epoch: 100/130 | Current Learning Rate: 0.000195
Epoch: 100/130 | Batch 0000/0313 | Loss: 0.0190
Epoch: 100/130 | Batch 0050/0313 | Loss: 0.0094
Epoch: 100/130 | Batch 0100/0313 | Loss: 0.0177
Epoch: 100/130 | Batch 0150/0313 | Loss: 0.0126
Epoch: 100/130 | Batch 0200/0313 | Loss: 0.0306
Epoch: 100/130 | Batch 0250/0313 | Loss: 0.0115
Epoch: 100/130 | Batch 0300/0313 | Loss: 0.0200
**Epoch: 100/130 | Train. Acc.: 99.517% | Loss: 0.0171
**Epoch: 100/130 | Valid. Acc.: 93.630% | Loss: 0.2500
Time elapsed: 81.17 min
Epoch: 101/130 | Current Learning Rate: 0.000195
Epoch: 101/130 | Batch 0000/0313 | Loss: 0.0200
Epoch: 101/130 | Batch 0050/0313 | Loss: 0.0276
Epoch: 101/130 | Batch 0100/0313 | Loss: 0.0157
Epoch: 101/130 | Batch 0150/0313 | Loss: 0.0546
Epoch: 101/130 | Batch 0200/0313 | Loss: 0.0711
Epoch: 101/130 | Batch 0250/0313 | Loss: 0.0133
Epoch: 101/130 | Batch 0300/0313 | Loss: 0.0155
**Epoch: 101/130 | Train. Acc.: 99.532% | Loss: 0.0169
**Epoch: 101/130 | Valid. Acc.: 93.700% | Loss: 0.2496
Epoch 00101: reducing learning rate of group 0 to 9.7656e-05.
Time elapsed: 81.98 min
Epoch: 102/130 | Current Learning Rate: 0.000098
Epoch: 102/130 | Batch 0000/0313 | Loss: 0.0120
Epoch: 102/130 | Batch 0050/0313 | Loss: 0.0126
Epoch: 102/130 | Batch 0100/0313 | Loss: 0.0115
Epoch: 102/130 | Batch 0150/0313 | Loss: 0.0411
Epoch: 102/130 | Batch 0200/0313 | Loss: 0.0116
Epoch: 102/130 | Batch 0250/0313 | Loss: 0.0130
Epoch: 102/130 | Batch 0300/0313 | Loss: 0.0224
**Epoch: 102/130 | Train. Acc.: 99.528% | Loss: 0.0174
**Epoch: 102/130 | Valid. Acc.: 93.580% | Loss: 0.2511
Time elapsed: 82.79 min
Epoch: 103/130 | Current Learning Rate: 0.000098
Epoch: 103/130 | Batch 0000/0313 | Loss: 0.0234
Epoch: 103/130 | Batch 0050/0313 | Loss: 0.0127
Epoch: 103/130 | Batch 0100/0313 | Loss: 0.0454
Epoch: 103/130 | Batch 0150/0313 | Loss: 0.0060
Epoch: 103/130 | Batch 0200/0313 | Loss: 0.0142
Epoch: 103/130 | Batch 0250/0313 | Loss: 0.0510
Epoch: 103/130 | Batch 0300/0313 | Loss: 0.0150
**Epoch: 103/130 | Train. Acc.: 99.560% | Loss: 0.0162
**Epoch: 103/130 | Valid. Acc.: 93.650% | Loss: 0.2482
Time elapsed: 83.61 min
Epoch: 104/130 | Current Learning Rate: 0.000098
Epoch: 104/130 | Batch 0000/0313 | Loss: 0.0348
Epoch: 104/130 | Batch 0050/0313 | Loss: 0.0147
Epoch: 104/130 | Batch 0100/0313 | Loss: 0.0172
Epoch: 104/130 | Batch 0150/0313 | Loss: 0.0296
Epoch: 104/130 | Batch 0200/0313 | Loss: 0.0087
Epoch: 104/130 | Batch 0250/0313 | Loss: 0.0341
Epoch: 104/130 | Batch 0300/0313 | Loss: 0.0329
**Epoch: 104/130 | Train. Acc.: 99.445% | Loss: 0.0184
**Epoch: 104/130 | Valid. Acc.: 93.610% | Loss: 0.2505
Time elapsed: 84.41 min
Epoch: 105/130 | Current Learning Rate: 0.000098
Epoch: 105/130 | Batch 0000/0313 | Loss: 0.0289
Epoch: 105/130 | Batch 0050/0313 | Loss: 0.0106
Epoch: 105/130 | Batch 0100/0313 | Loss: 0.0277
Epoch: 105/130 | Batch 0150/0313 | Loss: 0.0403
Epoch: 105/130 | Batch 0200/0313 | Loss: 0.0305
Epoch: 105/130 | Batch 0250/0313 | Loss: 0.0104
Epoch: 105/130 | Batch 0300/0313 | Loss: 0.0146
**Epoch: 105/130 | Train. Acc.: 99.565% | Loss: 0.0165
**Epoch: 105/130 | Valid. Acc.: 93.600% | Loss: 0.2483
Time elapsed: 85.22 min
Epoch: 106/130 | Current Learning Rate: 0.000098
Epoch: 106/130 | Batch 0000/0313 | Loss: 0.0389
Epoch: 106/130 | Batch 0050/0313 | Loss: 0.0132
Epoch: 106/130 | Batch 0100/0313 | Loss: 0.0097
Epoch: 106/130 | Batch 0150/0313 | Loss: 0.0078
Epoch: 106/130 | Batch 0200/0313 | Loss: 0.0270
Epoch: 106/130 | Batch 0250/0313 | Loss: 0.0068
Epoch: 106/130 | Batch 0300/0313 | Loss: 0.0107
**Epoch: 106/130 | Train. Acc.: 99.578% | Loss: 0.0155
**Epoch: 106/130 | Valid. Acc.: 93.680% | Loss: 0.2476
Time elapsed: 86.04 min
Epoch: 107/130 | Current Learning Rate: 0.000098
Epoch: 107/130 | Batch 0000/0313 | Loss: 0.0389
Epoch: 107/130 | Batch 0050/0313 | Loss: 0.0303
Epoch: 107/130 | Batch 0100/0313 | Loss: 0.0141
Epoch: 107/130 | Batch 0150/0313 | Loss: 0.0244
Epoch: 107/130 | Batch 0200/0313 | Loss: 0.0068
Epoch: 107/130 | Batch 0250/0313 | Loss: 0.0090
Epoch: 107/130 | Batch 0300/0313 | Loss: 0.0072
**Epoch: 107/130 | Train. Acc.: 99.598% | Loss: 0.0162
**Epoch: 107/130 | Valid. Acc.: 93.560% | Loss: 0.2497
Time elapsed: 86.86 min
Epoch: 108/130 | Current Learning Rate: 0.000098
Epoch: 108/130 | Batch 0000/0313 | Loss: 0.0315
Epoch: 108/130 | Batch 0050/0313 | Loss: 0.0130
Epoch: 108/130 | Batch 0100/0313 | Loss: 0.0084
Epoch: 108/130 | Batch 0150/0313 | Loss: 0.0244
Epoch: 108/130 | Batch 0200/0313 | Loss: 0.0603
Epoch: 108/130 | Batch 0250/0313 | Loss: 0.0146
Epoch: 108/130 | Batch 0300/0313 | Loss: 0.0322
**Epoch: 108/130 | Train. Acc.: 99.545% | Loss: 0.0168
**Epoch: 108/130 | Valid. Acc.: 93.550% | Loss: 0.2499
Epoch 00108: reducing learning rate of group 0 to 4.8828e-05.
Time elapsed: 87.67 min
Epoch: 109/130 | Current Learning Rate: 0.000049
Epoch: 109/130 | Batch 0000/0313 | Loss: 0.0537
Epoch: 109/130 | Batch 0050/0313 | Loss: 0.0233
Epoch: 109/130 | Batch 0100/0313 | Loss: 0.0141
Epoch: 109/130 | Batch 0150/0313 | Loss: 0.0181
Epoch: 109/130 | Batch 0200/0313 | Loss: 0.0088
Epoch: 109/130 | Batch 0250/0313 | Loss: 0.0034
Epoch: 109/130 | Batch 0300/0313 | Loss: 0.0149
**Epoch: 109/130 | Train. Acc.: 99.508% | Loss: 0.0169
**Epoch: 109/130 | Valid. Acc.: 93.660% | Loss: 0.2470
Time elapsed: 88.48 min
Epoch: 110/130 | Current Learning Rate: 0.000049
Epoch: 110/130 | Batch 0000/0313 | Loss: 0.0419
Epoch: 110/130 | Batch 0050/0313 | Loss: 0.0301
Epoch: 110/130 | Batch 0100/0313 | Loss: 0.0086
Epoch: 110/130 | Batch 0150/0313 | Loss: 0.0446
Epoch: 110/130 | Batch 0200/0313 | Loss: 0.0260
Epoch: 110/130 | Batch 0250/0313 | Loss: 0.0171
Epoch: 110/130 | Batch 0300/0313 | Loss: 0.0562
**Epoch: 110/130 | Train. Acc.: 99.543% | Loss: 0.0166
**Epoch: 110/130 | Valid. Acc.: 93.610% | Loss: 0.2495
Time elapsed: 89.29 min
Epoch: 111/130 | Current Learning Rate: 0.000049
Epoch: 111/130 | Batch 0000/0313 | Loss: 0.0076
Epoch: 111/130 | Batch 0050/0313 | Loss: 0.0260
Epoch: 111/130 | Batch 0100/0313 | Loss: 0.0284
Epoch: 111/130 | Batch 0150/0313 | Loss: 0.0090
Epoch: 111/130 | Batch 0200/0313 | Loss: 0.0141
Epoch: 111/130 | Batch 0250/0313 | Loss: 0.0256
Epoch: 111/130 | Batch 0300/0313 | Loss: 0.0054
**Epoch: 111/130 | Train. Acc.: 99.537% | Loss: 0.0166
**Epoch: 111/130 | Valid. Acc.: 93.660% | Loss: 0.2473
Time elapsed: 90.11 min
Epoch: 112/130 | Current Learning Rate: 0.000049
Epoch: 112/130 | Batch 0000/0313 | Loss: 0.0061
Epoch: 112/130 | Batch 0050/0313 | Loss: 0.0234
Epoch: 112/130 | Batch 0100/0313 | Loss: 0.0265
Epoch: 112/130 | Batch 0150/0313 | Loss: 0.0097
Epoch: 112/130 | Batch 0200/0313 | Loss: 0.0057
Epoch: 112/130 | Batch 0250/0313 | Loss: 0.0289
Epoch: 112/130 | Batch 0300/0313 | Loss: 0.0318
**Epoch: 112/130 | Train. Acc.: 99.515% | Loss: 0.0163
**Epoch: 112/130 | Valid. Acc.: 93.680% | Loss: 0.2480
Time elapsed: 90.92 min
Epoch: 113/130 | Current Learning Rate: 0.000049
Epoch: 113/130 | Batch 0000/0313 | Loss: 0.0114
Epoch: 113/130 | Batch 0050/0313 | Loss: 0.0094
Epoch: 113/130 | Batch 0100/0313 | Loss: 0.0096
Epoch: 113/130 | Batch 0150/0313 | Loss: 0.0157
Epoch: 113/130 | Batch 0200/0313 | Loss: 0.0047
Epoch: 113/130 | Batch 0250/0313 | Loss: 0.0341
Epoch: 113/130 | Batch 0300/0313 | Loss: 0.0574
**Epoch: 113/130 | Train. Acc.: 99.550% | Loss: 0.0163
**Epoch: 113/130 | Valid. Acc.: 93.700% | Loss: 0.2485
Time elapsed: 91.73 min
Epoch: 114/130 | Current Learning Rate: 0.000049
Epoch: 114/130 | Batch 0000/0313 | Loss: 0.0182
Epoch: 114/130 | Batch 0050/0313 | Loss: 0.0066
Epoch: 114/130 | Batch 0100/0313 | Loss: 0.0083
Epoch: 114/130 | Batch 0150/0313 | Loss: 0.0155
Epoch: 114/130 | Batch 0200/0313 | Loss: 0.0105
Epoch: 114/130 | Batch 0250/0313 | Loss: 0.0271
Epoch: 114/130 | Batch 0300/0313 | Loss: 0.0166
**Epoch: 114/130 | Train. Acc.: 99.550% | Loss: 0.0156
**Epoch: 114/130 | Valid. Acc.: 93.680% | Loss: 0.2512
Time elapsed: 92.54 min
Epoch: 115/130 | Current Learning Rate: 0.000049
Epoch: 115/130 | Batch 0000/0313 | Loss: 0.0075
Epoch: 115/130 | Batch 0050/0313 | Loss: 0.0153
Epoch: 115/130 | Batch 0100/0313 | Loss: 0.0149
Epoch: 115/130 | Batch 0150/0313 | Loss: 0.0112
Epoch: 115/130 | Batch 0200/0313 | Loss: 0.0183
Epoch: 115/130 | Batch 0250/0313 | Loss: 0.0202
Epoch: 115/130 | Batch 0300/0313 | Loss: 0.0074
**Epoch: 115/130 | Train. Acc.: 99.535% | Loss: 0.0160
**Epoch: 115/130 | Valid. Acc.: 93.680% | Loss: 0.2471
Epoch 00115: reducing learning rate of group 0 to 2.4414e-05.
Time elapsed: 93.35 min
Epoch: 116/130 | Current Learning Rate: 0.000024
Epoch: 116/130 | Batch 0000/0313 | Loss: 0.0093
Epoch: 116/130 | Batch 0050/0313 | Loss: 0.0051
Epoch: 116/130 | Batch 0100/0313 | Loss: 0.0124
Epoch: 116/130 | Batch 0150/0313 | Loss: 0.0130
Epoch: 116/130 | Batch 0200/0313 | Loss: 0.0261
Epoch: 116/130 | Batch 0250/0313 | Loss: 0.0081
Epoch: 116/130 | Batch 0300/0313 | Loss: 0.0268
**Epoch: 116/130 | Train. Acc.: 99.582% | Loss: 0.0152
**Epoch: 116/130 | Valid. Acc.: 93.630% | Loss: 0.2469
Time elapsed: 94.16 min
Epoch: 117/130 | Current Learning Rate: 0.000024
Epoch: 117/130 | Batch 0000/0313 | Loss: 0.0144
Epoch: 117/130 | Batch 0050/0313 | Loss: 0.0064
Epoch: 117/130 | Batch 0100/0313 | Loss: 0.0172
Epoch: 117/130 | Batch 0150/0313 | Loss: 0.0376
Epoch: 117/130 | Batch 0200/0313 | Loss: 0.0337
Epoch: 117/130 | Batch 0250/0313 | Loss: 0.0124
Epoch: 117/130 | Batch 0300/0313 | Loss: 0.0129
**Epoch: 117/130 | Train. Acc.: 99.508% | Loss: 0.0169
**Epoch: 117/130 | Valid. Acc.: 93.710% | Loss: 0.2483
Time elapsed: 94.97 min
Epoch: 118/130 | Current Learning Rate: 0.000024
Epoch: 118/130 | Batch 0000/0313 | Loss: 0.0227
Epoch: 118/130 | Batch 0050/0313 | Loss: 0.0199
Epoch: 118/130 | Batch 0100/0313 | Loss: 0.0415
Epoch: 118/130 | Batch 0150/0313 | Loss: 0.0121
Epoch: 118/130 | Batch 0200/0313 | Loss: 0.0159
Epoch: 118/130 | Batch 0250/0313 | Loss: 0.0335
Epoch: 118/130 | Batch 0300/0313 | Loss: 0.0080
**Epoch: 118/130 | Train. Acc.: 99.565% | Loss: 0.0152
**Epoch: 118/130 | Valid. Acc.: 93.690% | Loss: 0.2496
Time elapsed: 95.78 min
Epoch: 119/130 | Current Learning Rate: 0.000024
Epoch: 119/130 | Batch 0000/0313 | Loss: 0.0105
Epoch: 119/130 | Batch 0050/0313 | Loss: 0.0136
Epoch: 119/130 | Batch 0100/0313 | Loss: 0.0080
Epoch: 119/130 | Batch 0150/0313 | Loss: 0.0394
Epoch: 119/130 | Batch 0200/0313 | Loss: 0.0193
Epoch: 119/130 | Batch 0250/0313 | Loss: 0.0528
Epoch: 119/130 | Batch 0300/0313 | Loss: 0.0290
**Epoch: 119/130 | Train. Acc.: 99.578% | Loss: 0.0161
**Epoch: 119/130 | Valid. Acc.: 93.720% | Loss: 0.2477
Time elapsed: 96.59 min
Epoch: 120/130 | Current Learning Rate: 0.000024
Epoch: 120/130 | Batch 0000/0313 | Loss: 0.0164
Epoch: 120/130 | Batch 0050/0313 | Loss: 0.0291
Epoch: 120/130 | Batch 0100/0313 | Loss: 0.0331
Epoch: 120/130 | Batch 0150/0313 | Loss: 0.0045
Epoch: 120/130 | Batch 0200/0313 | Loss: 0.0073
Epoch: 120/130 | Batch 0250/0313 | Loss: 0.0116
Epoch: 120/130 | Batch 0300/0313 | Loss: 0.0292
**Epoch: 120/130 | Train. Acc.: 99.575% | Loss: 0.0155
**Epoch: 120/130 | Valid. Acc.: 93.680% | Loss: 0.2493
Time elapsed: 97.41 min
Epoch: 121/130 | Current Learning Rate: 0.000024
Epoch: 121/130 | Batch 0000/0313 | Loss: 0.0530
Epoch: 121/130 | Batch 0050/0313 | Loss: 0.0087
Epoch: 121/130 | Batch 0100/0313 | Loss: 0.0032
Epoch: 121/130 | Batch 0150/0313 | Loss: 0.0169
Epoch: 121/130 | Batch 0200/0313 | Loss: 0.0096
Epoch: 121/130 | Batch 0250/0313 | Loss: 0.0249
Epoch: 121/130 | Batch 0300/0313 | Loss: 0.0078
**Epoch: 121/130 | Train. Acc.: 99.523% | Loss: 0.0164
**Epoch: 121/130 | Valid. Acc.: 93.660% | Loss: 0.2513
Time elapsed: 98.21 min
Epoch: 122/130 | Current Learning Rate: 0.000024
Epoch: 122/130 | Batch 0000/0313 | Loss: 0.0231
Epoch: 122/130 | Batch 0050/0313 | Loss: 0.0368
Epoch: 122/130 | Batch 0100/0313 | Loss: 0.0128
Epoch: 122/130 | Batch 0150/0313 | Loss: 0.0104
Epoch: 122/130 | Batch 0200/0313 | Loss: 0.0206
Epoch: 122/130 | Batch 0250/0313 | Loss: 0.0670
Epoch: 122/130 | Batch 0300/0313 | Loss: 0.0330
**Epoch: 122/130 | Train. Acc.: 99.570% | Loss: 0.0154
**Epoch: 122/130 | Valid. Acc.: 93.760% | Loss: 0.2458
Epoch 00122: reducing learning rate of group 0 to 1.2207e-05.
Time elapsed: 99.02 min
Epoch: 123/130 | Current Learning Rate: 0.000012
Epoch: 123/130 | Batch 0000/0313 | Loss: 0.0359
Epoch: 123/130 | Batch 0050/0313 | Loss: 0.0255
Epoch: 123/130 | Batch 0100/0313 | Loss: 0.0148
Epoch: 123/130 | Batch 0150/0313 | Loss: 0.0167
Epoch: 123/130 | Batch 0200/0313 | Loss: 0.0129
Epoch: 123/130 | Batch 0250/0313 | Loss: 0.0396
Epoch: 123/130 | Batch 0300/0313 | Loss: 0.0151
**Epoch: 123/130 | Train. Acc.: 99.608% | Loss: 0.0153
**Epoch: 123/130 | Valid. Acc.: 93.660% | Loss: 0.2488
Time elapsed: 99.83 min
Epoch: 124/130 | Current Learning Rate: 0.000012
Epoch: 124/130 | Batch 0000/0313 | Loss: 0.0109
Epoch: 124/130 | Batch 0050/0313 | Loss: 0.0204
Epoch: 124/130 | Batch 0100/0313 | Loss: 0.0650
Epoch: 124/130 | Batch 0150/0313 | Loss: 0.0107
Epoch: 124/130 | Batch 0200/0313 | Loss: 0.0043
Epoch: 124/130 | Batch 0250/0313 | Loss: 0.0166
Epoch: 124/130 | Batch 0300/0313 | Loss: 0.0204
**Epoch: 124/130 | Train. Acc.: 99.547% | Loss: 0.0163
**Epoch: 124/130 | Valid. Acc.: 93.680% | Loss: 0.2493
Time elapsed: 100.64 min
Epoch: 125/130 | Current Learning Rate: 0.000012
Epoch: 125/130 | Batch 0000/0313 | Loss: 0.0280
Epoch: 125/130 | Batch 0050/0313 | Loss: 0.0079
Epoch: 125/130 | Batch 0100/0313 | Loss: 0.0208
Epoch: 125/130 | Batch 0150/0313 | Loss: 0.0062
Epoch: 125/130 | Batch 0200/0313 | Loss: 0.0180
Epoch: 125/130 | Batch 0250/0313 | Loss: 0.0086
Epoch: 125/130 | Batch 0300/0313 | Loss: 0.0522
**Epoch: 125/130 | Train. Acc.: 99.590% | Loss: 0.0152
**Epoch: 125/130 | Valid. Acc.: 93.630% | Loss: 0.2513
Time elapsed: 101.45 min
Epoch: 126/130 | Current Learning Rate: 0.000012
Epoch: 126/130 | Batch 0000/0313 | Loss: 0.0118
Epoch: 126/130 | Batch 0050/0313 | Loss: 0.0178
Epoch: 126/130 | Batch 0100/0313 | Loss: 0.0095
Epoch: 126/130 | Batch 0150/0313 | Loss: 0.0062
Epoch: 126/130 | Batch 0200/0313 | Loss: 0.0480
Epoch: 126/130 | Batch 0250/0313 | Loss: 0.0455
Epoch: 126/130 | Batch 0300/0313 | Loss: 0.0205
**Epoch: 126/130 | Train. Acc.: 99.578% | Loss: 0.0163
**Epoch: 126/130 | Valid. Acc.: 93.670% | Loss: 0.2483
Time elapsed: 102.26 min
Epoch: 127/130 | Current Learning Rate: 0.000012
Epoch: 127/130 | Batch 0000/0313 | Loss: 0.0061
Epoch: 127/130 | Batch 0050/0313 | Loss: 0.0243
Epoch: 127/130 | Batch 0100/0313 | Loss: 0.0123
Epoch: 127/130 | Batch 0150/0313 | Loss: 0.0091
Epoch: 127/130 | Batch 0200/0313 | Loss: 0.0104
Epoch: 127/130 | Batch 0250/0313 | Loss: 0.0225
Epoch: 127/130 | Batch 0300/0313 | Loss: 0.0148
**Epoch: 127/130 | Train. Acc.: 99.558% | Loss: 0.0153
**Epoch: 127/130 | Valid. Acc.: 93.790% | Loss: 0.2478
Time elapsed: 103.07 min
Epoch: 128/130 | Current Learning Rate: 0.000012
Epoch: 128/130 | Batch 0000/0313 | Loss: 0.0193
Epoch: 128/130 | Batch 0050/0313 | Loss: 0.0448
Epoch: 128/130 | Batch 0100/0313 | Loss: 0.0185
Epoch: 128/130 | Batch 0150/0313 | Loss: 0.0100
Epoch: 128/130 | Batch 0200/0313 | Loss: 0.0075
Epoch: 128/130 | Batch 0250/0313 | Loss: 0.0264
Epoch: 128/130 | Batch 0300/0313 | Loss: 0.0053
**Epoch: 128/130 | Train. Acc.: 99.653% | Loss: 0.0146
**Epoch: 128/130 | Valid. Acc.: 93.660% | Loss: 0.2500
Time elapsed: 103.87 min
Epoch: 129/130 | Current Learning Rate: 0.000012
Epoch: 129/130 | Batch 0000/0313 | Loss: 0.0078
Epoch: 129/130 | Batch 0050/0313 | Loss: 0.0506
Epoch: 129/130 | Batch 0100/0313 | Loss: 0.0198
Epoch: 129/130 | Batch 0150/0313 | Loss: 0.0335
Epoch: 129/130 | Batch 0200/0313 | Loss: 0.0164
Epoch: 129/130 | Batch 0250/0313 | Loss: 0.0163
Epoch: 129/130 | Batch 0300/0313 | Loss: 0.0046
**Epoch: 129/130 | Train. Acc.: 99.547% | Loss: 0.0155
**Epoch: 129/130 | Valid. Acc.: 93.730% | Loss: 0.2477
Epoch 00129: reducing learning rate of group 0 to 6.1035e-06.
Time elapsed: 104.69 min
Epoch: 130/130 | Current Learning Rate: 0.000006
Epoch: 130/130 | Batch 0000/0313 | Loss: 0.0127
Epoch: 130/130 | Batch 0050/0313 | Loss: 0.0638
Epoch: 130/130 | Batch 0100/0313 | Loss: 0.0224
Epoch: 130/130 | Batch 0150/0313 | Loss: 0.0130
Epoch: 130/130 | Batch 0200/0313 | Loss: 0.0333
Epoch: 130/130 | Batch 0250/0313 | Loss: 0.0213
Epoch: 130/130 | Batch 0300/0313 | Loss: 0.0149
**Epoch: 130/130 | Train. Acc.: 99.565% | Loss: 0.0160
**Epoch: 130/130 | Valid. Acc.: 93.650% | Loss: 0.2501
Time elapsed: 105.50 min
Total Training Time: 105.50 min
Model: ResNet50
Test Loss: 0.2502
Test Accuracy (Overall): 92.66%

Test Accuracy of Airplane: 92% (926/1000)
Test Accuracy of      Car: 98% (980/1000)
Test Accuracy of     Bird: 91% (911/1000)
Test Accuracy of      Cat: 83% (832/1000)
Test Accuracy of     Deer: 93% (933/1000)
Test Accuracy of      Dog: 88% (883/1000)
Test Accuracy of     Frog: 95% (954/1000)
Test Accuracy of    Horse: 94% (942/1000)
Test Accuracy of     Ship: 95% (956/1000)
Test Accuracy of    Truck: 94% (949/1000)
Training ResNet101 for 130 epochs with initial learning rate 0.1...
Epoch: 001/130 | Current Learning Rate: 0.100000
Epoch: 001/130 | Batch 0000/0313 | Loss: 2.4942
Epoch: 001/130 | Batch 0050/0313 | Loss: 2.2498
Epoch: 001/130 | Batch 0100/0313 | Loss: 2.2111
Epoch: 001/130 | Batch 0150/0313 | Loss: 2.2205
Epoch: 001/130 | Batch 0200/0313 | Loss: 2.1535
Epoch: 001/130 | Batch 0250/0313 | Loss: 2.0792
Epoch: 001/130 | Batch 0300/0313 | Loss: 2.0953
**Epoch: 001/130 | Train. Acc.: 19.183% | Loss: 2.0317
**Epoch: 001/130 | Valid. Acc.: 19.010% | Loss: 1.9908
**Validation loss decreased (inf --> 1.990797). Saving model ...
Time elapsed: 1.25 min
Epoch: 002/130 | Current Learning Rate: 0.100000
Epoch: 002/130 | Batch 0000/0313 | Loss: 1.9307
Epoch: 002/130 | Batch 0050/0313 | Loss: 2.1157
Epoch: 002/130 | Batch 0100/0313 | Loss: 1.9527
Epoch: 002/130 | Batch 0150/0313 | Loss: 2.0002
Epoch: 002/130 | Batch 0200/0313 | Loss: 1.9076
Epoch: 002/130 | Batch 0250/0313 | Loss: 1.9723
Epoch: 002/130 | Batch 0300/0313 | Loss: 1.8955
**Epoch: 002/130 | Train. Acc.: 28.095% | Loss: 1.8931
**Epoch: 002/130 | Valid. Acc.: 28.850% | Loss: 1.8685
**Validation loss decreased (1.990797 --> 1.868531). Saving model ...
Time elapsed: 2.50 min
Epoch: 003/130 | Current Learning Rate: 0.100000
Epoch: 003/130 | Batch 0000/0313 | Loss: 1.9195
Epoch: 003/130 | Batch 0050/0313 | Loss: 1.9272
Epoch: 003/130 | Batch 0100/0313 | Loss: 1.8807
Epoch: 003/130 | Batch 0150/0313 | Loss: 1.8048
Epoch: 003/130 | Batch 0200/0313 | Loss: 1.6318
Epoch: 003/130 | Batch 0250/0313 | Loss: 1.7645
Epoch: 003/130 | Batch 0300/0313 | Loss: 1.8678
**Epoch: 003/130 | Train. Acc.: 33.400% | Loss: 1.7894
**Epoch: 003/130 | Valid. Acc.: 34.700% | Loss: 1.7168
**Validation loss decreased (1.868531 --> 1.716795). Saving model ...
Time elapsed: 3.75 min
Epoch: 004/130 | Current Learning Rate: 0.100000
Epoch: 004/130 | Batch 0000/0313 | Loss: 1.8523
Epoch: 004/130 | Batch 0050/0313 | Loss: 1.7669
Epoch: 004/130 | Batch 0100/0313 | Loss: 1.8357
Epoch: 004/130 | Batch 0150/0313 | Loss: 1.8597
Epoch: 004/130 | Batch 0200/0313 | Loss: 1.8257
Epoch: 004/130 | Batch 0250/0313 | Loss: 1.5973
Epoch: 004/130 | Batch 0300/0313 | Loss: 1.5481
**Epoch: 004/130 | Train. Acc.: 37.790% | Loss: 1.7092
**Epoch: 004/130 | Valid. Acc.: 41.910% | Loss: 1.5871
**Validation loss decreased (1.716795 --> 1.587118). Saving model ...
Time elapsed: 5.00 min
Epoch: 005/130 | Current Learning Rate: 0.100000
Epoch: 005/130 | Batch 0000/0313 | Loss: 1.7177
Epoch: 005/130 | Batch 0050/0313 | Loss: 1.5438
Epoch: 005/130 | Batch 0100/0313 | Loss: 1.4726
Epoch: 005/130 | Batch 0150/0313 | Loss: 1.5060
Epoch: 005/130 | Batch 0200/0313 | Loss: 1.5813
Epoch: 005/130 | Batch 0250/0313 | Loss: 1.3500
Epoch: 005/130 | Batch 0300/0313 | Loss: 1.5941
**Epoch: 005/130 | Train. Acc.: 44.205% | Loss: 1.5233
**Epoch: 005/130 | Valid. Acc.: 48.560% | Loss: 1.4186
**Validation loss decreased (1.587118 --> 1.418553). Saving model ...
Time elapsed: 6.25 min
Epoch: 006/130 | Current Learning Rate: 0.100000
Epoch: 006/130 | Batch 0000/0313 | Loss: 1.4830
Epoch: 006/130 | Batch 0050/0313 | Loss: 1.3896
Epoch: 006/130 | Batch 0100/0313 | Loss: 1.5138
Epoch: 006/130 | Batch 0150/0313 | Loss: 1.4836
Epoch: 006/130 | Batch 0200/0313 | Loss: 1.5239
Epoch: 006/130 | Batch 0250/0313 | Loss: 1.4146
Epoch: 006/130 | Batch 0300/0313 | Loss: 1.3086
**Epoch: 006/130 | Train. Acc.: 49.273% | Loss: 1.4103
**Epoch: 006/130 | Valid. Acc.: 53.200% | Loss: 1.2996
**Validation loss decreased (1.418553 --> 1.299578). Saving model ...
Time elapsed: 7.50 min
Epoch: 007/130 | Current Learning Rate: 0.100000
Epoch: 007/130 | Batch 0000/0313 | Loss: 1.3299
Epoch: 007/130 | Batch 0050/0313 | Loss: 1.2772
Epoch: 007/130 | Batch 0100/0313 | Loss: 1.4562
Epoch: 007/130 | Batch 0150/0313 | Loss: 1.3178
Epoch: 007/130 | Batch 0200/0313 | Loss: 1.4023
Epoch: 007/130 | Batch 0250/0313 | Loss: 1.3798
Epoch: 007/130 | Batch 0300/0313 | Loss: 1.0932
**Epoch: 007/130 | Train. Acc.: 54.468% | Loss: 1.2605
**Epoch: 007/130 | Valid. Acc.: 57.410% | Loss: 1.1980
**Validation loss decreased (1.299578 --> 1.198037). Saving model ...
Time elapsed: 8.75 min
Epoch: 008/130 | Current Learning Rate: 0.100000
Epoch: 008/130 | Batch 0000/0313 | Loss: 1.2077
Epoch: 008/130 | Batch 0050/0313 | Loss: 1.3507
Epoch: 008/130 | Batch 0100/0313 | Loss: 1.2379
Epoch: 008/130 | Batch 0150/0313 | Loss: 1.0328
Epoch: 008/130 | Batch 0200/0313 | Loss: 1.2454
Epoch: 008/130 | Batch 0250/0313 | Loss: 1.2877
Epoch: 008/130 | Batch 0300/0313 | Loss: 1.0166
**Epoch: 008/130 | Train. Acc.: 55.788% | Loss: 1.2234
**Epoch: 008/130 | Valid. Acc.: 58.860% | Loss: 1.1491
**Validation loss decreased (1.198037 --> 1.149075). Saving model ...
Time elapsed: 10.00 min
Epoch: 009/130 | Current Learning Rate: 0.100000
Epoch: 009/130 | Batch 0000/0313 | Loss: 1.2935
Epoch: 009/130 | Batch 0050/0313 | Loss: 1.2610
Epoch: 009/130 | Batch 0100/0313 | Loss: 1.0386
Epoch: 009/130 | Batch 0150/0313 | Loss: 1.1568
Epoch: 009/130 | Batch 0200/0313 | Loss: 1.0980
Epoch: 009/130 | Batch 0250/0313 | Loss: 1.0412
Epoch: 009/130 | Batch 0300/0313 | Loss: 1.2291
**Epoch: 009/130 | Train. Acc.: 60.772% | Loss: 1.1115
**Epoch: 009/130 | Valid. Acc.: 64.320% | Loss: 1.0022
**Validation loss decreased (1.149075 --> 1.002201). Saving model ...
Time elapsed: 11.25 min
Epoch: 010/130 | Current Learning Rate: 0.100000
Epoch: 010/130 | Batch 0000/0313 | Loss: 1.0868
Epoch: 010/130 | Batch 0050/0313 | Loss: 1.1337
Epoch: 010/130 | Batch 0100/0313 | Loss: 1.0370
Epoch: 010/130 | Batch 0150/0313 | Loss: 1.0809
Epoch: 010/130 | Batch 0200/0313 | Loss: 1.2890
Epoch: 010/130 | Batch 0250/0313 | Loss: 0.9996
Epoch: 010/130 | Batch 0300/0313 | Loss: 1.1541
**Epoch: 010/130 | Train. Acc.: 56.022% | Loss: 1.2487
**Epoch: 010/130 | Valid. Acc.: 61.700% | Loss: 1.0931
Time elapsed: 12.50 min
Epoch: 011/130 | Current Learning Rate: 0.100000
Epoch: 011/130 | Batch 0000/0313 | Loss: 1.0964
Epoch: 011/130 | Batch 0050/0313 | Loss: 1.0248
Epoch: 011/130 | Batch 0100/0313 | Loss: 1.0535
Epoch: 011/130 | Batch 0150/0313 | Loss: 1.1572
Epoch: 011/130 | Batch 0200/0313 | Loss: 0.9852
Epoch: 011/130 | Batch 0250/0313 | Loss: 0.8077
Epoch: 011/130 | Batch 0300/0313 | Loss: 1.1132
**Epoch: 011/130 | Train. Acc.: 62.572% | Loss: 1.0571
**Epoch: 011/130 | Valid. Acc.: 66.980% | Loss: 0.9462
**Validation loss decreased (1.002201 --> 0.946249). Saving model ...
Time elapsed: 13.75 min
Epoch: 012/130 | Current Learning Rate: 0.100000
Epoch: 012/130 | Batch 0000/0313 | Loss: 1.0647
Epoch: 012/130 | Batch 0050/0313 | Loss: 0.9231
Epoch: 012/130 | Batch 0100/0313 | Loss: 1.1186
Epoch: 012/130 | Batch 0150/0313 | Loss: 0.8540
Epoch: 012/130 | Batch 0200/0313 | Loss: 0.9638
Epoch: 012/130 | Batch 0250/0313 | Loss: 0.9277
Epoch: 012/130 | Batch 0300/0313 | Loss: 0.8715
**Epoch: 012/130 | Train. Acc.: 66.350% | Loss: 0.9483
**Epoch: 012/130 | Valid. Acc.: 69.080% | Loss: 0.9004
**Validation loss decreased (0.946249 --> 0.900371). Saving model ...
Time elapsed: 15.01 min
Epoch: 013/130 | Current Learning Rate: 0.100000
Epoch: 013/130 | Batch 0000/0313 | Loss: 0.9258
Epoch: 013/130 | Batch 0050/0313 | Loss: 0.8405
Epoch: 013/130 | Batch 0100/0313 | Loss: 0.7841
Epoch: 013/130 | Batch 0150/0313 | Loss: 0.7959
Epoch: 013/130 | Batch 0200/0313 | Loss: 0.8197
Epoch: 013/130 | Batch 0250/0313 | Loss: 0.8716
Epoch: 013/130 | Batch 0300/0313 | Loss: 0.8368
**Epoch: 013/130 | Train. Acc.: 67.093% | Loss: 0.9314
**Epoch: 013/130 | Valid. Acc.: 70.050% | Loss: 0.8572
**Validation loss decreased (0.900371 --> 0.857227). Saving model ...
Time elapsed: 16.26 min
Epoch: 014/130 | Current Learning Rate: 0.100000
Epoch: 014/130 | Batch 0000/0313 | Loss: 0.8470
Epoch: 014/130 | Batch 0050/0313 | Loss: 0.8433
Epoch: 014/130 | Batch 0100/0313 | Loss: 0.5872
Epoch: 014/130 | Batch 0150/0313 | Loss: 0.8026
Epoch: 014/130 | Batch 0200/0313 | Loss: 0.9957
Epoch: 014/130 | Batch 0250/0313 | Loss: 0.8473
Epoch: 014/130 | Batch 0300/0313 | Loss: 0.6825
**Epoch: 014/130 | Train. Acc.: 64.885% | Loss: 0.9671
**Epoch: 014/130 | Valid. Acc.: 69.040% | Loss: 0.8930
Time elapsed: 17.50 min
Epoch: 015/130 | Current Learning Rate: 0.100000
Epoch: 015/130 | Batch 0000/0313 | Loss: 0.8726
Epoch: 015/130 | Batch 0050/0313 | Loss: 0.8489
Epoch: 015/130 | Batch 0100/0313 | Loss: 0.9531
Epoch: 015/130 | Batch 0150/0313 | Loss: 0.8276
Epoch: 015/130 | Batch 0200/0313 | Loss: 0.9747
Epoch: 015/130 | Batch 0250/0313 | Loss: 0.7539
Epoch: 015/130 | Batch 0300/0313 | Loss: 0.7639
**Epoch: 015/130 | Train. Acc.: 70.608% | Loss: 0.8469
**Epoch: 015/130 | Valid. Acc.: 72.170% | Loss: 0.8153
**Validation loss decreased (0.857227 --> 0.815260). Saving model ...
Time elapsed: 18.75 min
Epoch: 016/130 | Current Learning Rate: 0.100000
Epoch: 016/130 | Batch 0000/0313 | Loss: 0.8876
Epoch: 016/130 | Batch 0050/0313 | Loss: 0.6841
Epoch: 016/130 | Batch 0100/0313 | Loss: 0.8081
Epoch: 016/130 | Batch 0150/0313 | Loss: 0.8849
Epoch: 016/130 | Batch 0200/0313 | Loss: 0.6942
Epoch: 016/130 | Batch 0250/0313 | Loss: 0.7902
Epoch: 016/130 | Batch 0300/0313 | Loss: 0.7605
**Epoch: 016/130 | Train. Acc.: 71.420% | Loss: 0.8094
**Epoch: 016/130 | Valid. Acc.: 73.170% | Loss: 0.7487
**Validation loss decreased (0.815260 --> 0.748671). Saving model ...
Time elapsed: 20.00 min
Epoch: 017/130 | Current Learning Rate: 0.100000
Epoch: 017/130 | Batch 0000/0313 | Loss: 0.7122
Epoch: 017/130 | Batch 0050/0313 | Loss: 0.8666
Epoch: 017/130 | Batch 0100/0313 | Loss: 0.6188
Epoch: 017/130 | Batch 0150/0313 | Loss: 0.8188
Epoch: 017/130 | Batch 0200/0313 | Loss: 0.5950
Epoch: 017/130 | Batch 0250/0313 | Loss: 0.7126
Epoch: 017/130 | Batch 0300/0313 | Loss: 0.5636
**Epoch: 017/130 | Train. Acc.: 70.483% | Loss: 0.8602
**Epoch: 017/130 | Valid. Acc.: 73.720% | Loss: 0.8121
Time elapsed: 21.24 min
Epoch: 018/130 | Current Learning Rate: 0.100000
Epoch: 018/130 | Batch 0000/0313 | Loss: 0.6789
Epoch: 018/130 | Batch 0050/0313 | Loss: 0.8814
Epoch: 018/130 | Batch 0100/0313 | Loss: 0.8718
Epoch: 018/130 | Batch 0150/0313 | Loss: 0.8496
Epoch: 018/130 | Batch 0200/0313 | Loss: 0.7966
Epoch: 018/130 | Batch 0250/0313 | Loss: 0.7936
Epoch: 018/130 | Batch 0300/0313 | Loss: 0.5871
**Epoch: 018/130 | Train. Acc.: 57.693% | Loss: 1.2702
**Epoch: 018/130 | Valid. Acc.: 64.760% | Loss: 1.1414
Time elapsed: 22.49 min
Epoch: 019/130 | Current Learning Rate: 0.100000
Epoch: 019/130 | Batch 0000/0313 | Loss: 0.7117
Epoch: 019/130 | Batch 0050/0313 | Loss: 0.7972
Epoch: 019/130 | Batch 0100/0313 | Loss: 0.6059
Epoch: 019/130 | Batch 0150/0313 | Loss: 0.7343
Epoch: 019/130 | Batch 0200/0313 | Loss: 0.8383
Epoch: 019/130 | Batch 0250/0313 | Loss: 0.7302
Epoch: 019/130 | Batch 0300/0313 | Loss: 0.8654
**Epoch: 019/130 | Train. Acc.: 67.722% | Loss: 0.9397
**Epoch: 019/130 | Valid. Acc.: 72.950% | Loss: 0.8093
Time elapsed: 23.73 min
Epoch: 020/130 | Current Learning Rate: 0.100000
Epoch: 020/130 | Batch 0000/0313 | Loss: 0.6901
Epoch: 020/130 | Batch 0050/0313 | Loss: 0.6756
Epoch: 020/130 | Batch 0100/0313 | Loss: 0.6105
Epoch: 020/130 | Batch 0150/0313 | Loss: 0.7702
Epoch: 020/130 | Batch 0200/0313 | Loss: 0.6354
Epoch: 020/130 | Batch 0250/0313 | Loss: 0.5733
Epoch: 020/130 | Batch 0300/0313 | Loss: 0.6671
**Epoch: 020/130 | Train. Acc.: 71.210% | Loss: 0.8590
**Epoch: 020/130 | Valid. Acc.: 75.470% | Loss: 0.7593
Time elapsed: 24.97 min
Epoch: 021/130 | Current Learning Rate: 0.100000
Epoch: 021/130 | Batch 0000/0313 | Loss: 0.5566
Epoch: 021/130 | Batch 0050/0313 | Loss: 0.8451
Epoch: 021/130 | Batch 0100/0313 | Loss: 0.7176
Epoch: 021/130 | Batch 0150/0313 | Loss: 0.7156
Epoch: 021/130 | Batch 0200/0313 | Loss: 0.7958
Epoch: 021/130 | Batch 0250/0313 | Loss: 0.7385
Epoch: 021/130 | Batch 0300/0313 | Loss: 0.6830
**Epoch: 021/130 | Train. Acc.: 71.233% | Loss: 0.8415
**Epoch: 021/130 | Valid. Acc.: 71.620% | Loss: 0.8763
Time elapsed: 26.21 min
Epoch: 022/130 | Current Learning Rate: 0.100000
Epoch: 022/130 | Batch 0000/0313 | Loss: 0.7844
Epoch: 022/130 | Batch 0050/0313 | Loss: 0.7203
Epoch: 022/130 | Batch 0100/0313 | Loss: 0.7457
Epoch: 022/130 | Batch 0150/0313 | Loss: 0.5848
Epoch: 022/130 | Batch 0200/0313 | Loss: 0.7474
Epoch: 022/130 | Batch 0250/0313 | Loss: 0.6075
Epoch: 022/130 | Batch 0300/0313 | Loss: 0.5580
**Epoch: 022/130 | Train. Acc.: 70.552% | Loss: 0.8547
**Epoch: 022/130 | Valid. Acc.: 71.450% | Loss: 0.8731
Time elapsed: 27.45 min
Epoch: 023/130 | Current Learning Rate: 0.100000
Epoch: 023/130 | Batch 0000/0313 | Loss: 0.8028
Epoch: 023/130 | Batch 0050/0313 | Loss: 0.8665
Epoch: 023/130 | Batch 0100/0313 | Loss: 0.7170
Epoch: 023/130 | Batch 0150/0313 | Loss: 0.5785
Epoch: 023/130 | Batch 0200/0313 | Loss: 0.6968
Epoch: 023/130 | Batch 0250/0313 | Loss: 0.5440
Epoch: 023/130 | Batch 0300/0313 | Loss: 0.6758
**Epoch: 023/130 | Train. Acc.: 73.803% | Loss: 0.7528
**Epoch: 023/130 | Valid. Acc.: 77.230% | Loss: 0.6568
**Validation loss decreased (0.748671 --> 0.656830). Saving model ...
Time elapsed: 28.71 min
Epoch: 024/130 | Current Learning Rate: 0.100000
Epoch: 024/130 | Batch 0000/0313 | Loss: 0.4950
Epoch: 024/130 | Batch 0050/0313 | Loss: 0.6441
Epoch: 024/130 | Batch 0100/0313 | Loss: 0.5887
Epoch: 024/130 | Batch 0150/0313 | Loss: 0.7096
Epoch: 024/130 | Batch 0200/0313 | Loss: 0.5057
Epoch: 024/130 | Batch 0250/0313 | Loss: 0.5743
Epoch: 024/130 | Batch 0300/0313 | Loss: 0.4589
**Epoch: 024/130 | Train. Acc.: 71.960% | Loss: 0.8271
**Epoch: 024/130 | Valid. Acc.: 76.360% | Loss: 0.7102
Time elapsed: 29.95 min
Epoch: 025/130 | Current Learning Rate: 0.100000
Epoch: 025/130 | Batch 0000/0313 | Loss: 0.7436
Epoch: 025/130 | Batch 0050/0313 | Loss: 0.6443
Epoch: 025/130 | Batch 0100/0313 | Loss: 0.6942
Epoch: 025/130 | Batch 0150/0313 | Loss: 0.7661
Epoch: 025/130 | Batch 0200/0313 | Loss: 0.6327
Epoch: 025/130 | Batch 0250/0313 | Loss: 0.6209
Epoch: 025/130 | Batch 0300/0313 | Loss: 0.6672
**Epoch: 025/130 | Train. Acc.: 71.845% | Loss: 0.8178
**Epoch: 025/130 | Valid. Acc.: 73.670% | Loss: 0.7979
Time elapsed: 31.19 min
Epoch: 026/130 | Current Learning Rate: 0.100000
Epoch: 026/130 | Batch 0000/0313 | Loss: 0.6773
Epoch: 026/130 | Batch 0050/0313 | Loss: 0.7736
Epoch: 026/130 | Batch 0100/0313 | Loss: 0.5011
Epoch: 026/130 | Batch 0150/0313 | Loss: 0.6419
Epoch: 026/130 | Batch 0200/0313 | Loss: 0.4942
Epoch: 026/130 | Batch 0250/0313 | Loss: 0.7543
Epoch: 026/130 | Batch 0300/0313 | Loss: 0.6942
**Epoch: 026/130 | Train. Acc.: 73.847% | Loss: 0.7638
**Epoch: 026/130 | Valid. Acc.: 75.880% | Loss: 0.7196
Time elapsed: 32.43 min
Epoch: 027/130 | Current Learning Rate: 0.100000
Epoch: 027/130 | Batch 0000/0313 | Loss: 0.5388
Epoch: 027/130 | Batch 0050/0313 | Loss: 0.5993
Epoch: 027/130 | Batch 0100/0313 | Loss: 0.6437
Epoch: 027/130 | Batch 0150/0313 | Loss: 0.6955
Epoch: 027/130 | Batch 0200/0313 | Loss: 0.5387
Epoch: 027/130 | Batch 0250/0313 | Loss: 0.8452
Epoch: 027/130 | Batch 0300/0313 | Loss: 0.6038
**Epoch: 027/130 | Train. Acc.: 70.900% | Loss: 0.8458
**Epoch: 027/130 | Valid. Acc.: 75.980% | Loss: 0.7208
Time elapsed: 33.67 min
Epoch: 028/130 | Current Learning Rate: 0.100000
Epoch: 028/130 | Batch 0000/0313 | Loss: 0.8762
Epoch: 028/130 | Batch 0050/0313 | Loss: 0.5591
Epoch: 028/130 | Batch 0100/0313 | Loss: 0.8079
Epoch: 028/130 | Batch 0150/0313 | Loss: 0.7380
Epoch: 028/130 | Batch 0200/0313 | Loss: 0.5972
Epoch: 028/130 | Batch 0250/0313 | Loss: 0.5918
Epoch: 028/130 | Batch 0300/0313 | Loss: 0.5346
**Epoch: 028/130 | Train. Acc.: 75.308% | Loss: 0.7073
**Epoch: 028/130 | Valid. Acc.: 78.390% | Loss: 0.6198
**Validation loss decreased (0.656830 --> 0.619794). Saving model ...
Time elapsed: 34.92 min
Epoch: 029/130 | Current Learning Rate: 0.100000
Epoch: 029/130 | Batch 0000/0313 | Loss: 0.7071
Epoch: 029/130 | Batch 0050/0313 | Loss: 0.6581
Epoch: 029/130 | Batch 0100/0313 | Loss: 0.6807
Epoch: 029/130 | Batch 0150/0313 | Loss: 0.5840
Epoch: 029/130 | Batch 0200/0313 | Loss: 0.7317
Epoch: 029/130 | Batch 0250/0313 | Loss: 0.4756
Epoch: 029/130 | Batch 0300/0313 | Loss: 0.5682
**Epoch: 029/130 | Train. Acc.: 74.275% | Loss: 0.7597
**Epoch: 029/130 | Valid. Acc.: 77.470% | Loss: 0.6578
Time elapsed: 36.16 min
Epoch: 030/130 | Current Learning Rate: 0.100000
Epoch: 030/130 | Batch 0000/0313 | Loss: 0.5215
Epoch: 030/130 | Batch 0050/0313 | Loss: 0.6059
Epoch: 030/130 | Batch 0100/0313 | Loss: 0.8515
Epoch: 030/130 | Batch 0150/0313 | Loss: 0.6212
Epoch: 030/130 | Batch 0200/0313 | Loss: 0.6232
Epoch: 030/130 | Batch 0250/0313 | Loss: 0.5193
Epoch: 030/130 | Batch 0300/0313 | Loss: 0.6959
**Epoch: 030/130 | Train. Acc.: 75.113% | Loss: 0.7240
**Epoch: 030/130 | Valid. Acc.: 78.710% | Loss: 0.6291
Time elapsed: 37.40 min
Epoch: 031/130 | Current Learning Rate: 0.100000
Epoch: 031/130 | Batch 0000/0313 | Loss: 0.5639
Epoch: 031/130 | Batch 0050/0313 | Loss: 0.6098
Epoch: 031/130 | Batch 0100/0313 | Loss: 0.5493
Epoch: 031/130 | Batch 0150/0313 | Loss: 0.6453
Epoch: 031/130 | Batch 0200/0313 | Loss: 0.7362
Epoch: 031/130 | Batch 0250/0313 | Loss: 0.7976
Epoch: 031/130 | Batch 0300/0313 | Loss: 0.7279
**Epoch: 031/130 | Train. Acc.: 75.050% | Loss: 0.7088
**Epoch: 031/130 | Valid. Acc.: 78.560% | Loss: 0.6269
Time elapsed: 38.64 min
Epoch: 032/130 | Current Learning Rate: 0.100000
Epoch: 032/130 | Batch 0000/0313 | Loss: 0.5309
Epoch: 032/130 | Batch 0050/0313 | Loss: 0.6354
Epoch: 032/130 | Batch 0100/0313 | Loss: 0.5313
Epoch: 032/130 | Batch 0150/0313 | Loss: 0.6857
Epoch: 032/130 | Batch 0200/0313 | Loss: 0.6422
Epoch: 032/130 | Batch 0250/0313 | Loss: 0.5401
Epoch: 032/130 | Batch 0300/0313 | Loss: 0.5461
**Epoch: 032/130 | Train. Acc.: 75.960% | Loss: 0.7009
**Epoch: 032/130 | Valid. Acc.: 78.560% | Loss: 0.6463
Time elapsed: 39.88 min
Epoch: 033/130 | Current Learning Rate: 0.100000
Epoch: 033/130 | Batch 0000/0313 | Loss: 0.5651
Epoch: 033/130 | Batch 0050/0313 | Loss: 0.5773
Epoch: 033/130 | Batch 0100/0313 | Loss: 0.6332
Epoch: 033/130 | Batch 0150/0313 | Loss: 0.5495
Epoch: 033/130 | Batch 0200/0313 | Loss: 0.5379
Epoch: 033/130 | Batch 0250/0313 | Loss: 0.6927
Epoch: 033/130 | Batch 0300/0313 | Loss: 0.5606
**Epoch: 033/130 | Train. Acc.: 76.582% | Loss: 0.6835
**Epoch: 033/130 | Valid. Acc.: 78.680% | Loss: 0.6163
**Validation loss decreased (0.619794 --> 0.616346). Saving model ...
Time elapsed: 41.13 min
Epoch: 034/130 | Current Learning Rate: 0.100000
Epoch: 034/130 | Batch 0000/0313 | Loss: 0.6044
Epoch: 034/130 | Batch 0050/0313 | Loss: 0.7033
Epoch: 034/130 | Batch 0100/0313 | Loss: 0.5367
Epoch: 034/130 | Batch 0150/0313 | Loss: 0.5971
Epoch: 034/130 | Batch 0200/0313 | Loss: 0.7373
Epoch: 034/130 | Batch 0250/0313 | Loss: 0.6119
Epoch: 034/130 | Batch 0300/0313 | Loss: 0.6177
**Epoch: 034/130 | Train. Acc.: 73.427% | Loss: 0.7748
**Epoch: 034/130 | Valid. Acc.: 77.830% | Loss: 0.6467
Time elapsed: 42.37 min
Epoch: 035/130 | Current Learning Rate: 0.100000
Epoch: 035/130 | Batch 0000/0313 | Loss: 0.5499
Epoch: 035/130 | Batch 0050/0313 | Loss: 0.6015
Epoch: 035/130 | Batch 0100/0313 | Loss: 0.5331
Epoch: 035/130 | Batch 0150/0313 | Loss: 0.5855
Epoch: 035/130 | Batch 0200/0313 | Loss: 0.4822
Epoch: 035/130 | Batch 0250/0313 | Loss: 0.5776
Epoch: 035/130 | Batch 0300/0313 | Loss: 0.7855
**Epoch: 035/130 | Train. Acc.: 74.772% | Loss: 0.7350
**Epoch: 035/130 | Valid. Acc.: 76.970% | Loss: 0.6827
Time elapsed: 43.62 min
Epoch: 036/130 | Current Learning Rate: 0.100000
Epoch: 036/130 | Batch 0000/0313 | Loss: 0.6274
Epoch: 036/130 | Batch 0050/0313 | Loss: 0.6348
Epoch: 036/130 | Batch 0100/0313 | Loss: 0.5146
Epoch: 036/130 | Batch 0150/0313 | Loss: 0.5877
Epoch: 036/130 | Batch 0200/0313 | Loss: 0.5571
Epoch: 036/130 | Batch 0250/0313 | Loss: 0.4566
Epoch: 036/130 | Batch 0300/0313 | Loss: 0.5832
**Epoch: 036/130 | Train. Acc.: 74.435% | Loss: 0.7468
**Epoch: 036/130 | Valid. Acc.: 76.660% | Loss: 0.7076
Time elapsed: 44.86 min
Epoch: 037/130 | Current Learning Rate: 0.100000
Epoch: 037/130 | Batch 0000/0313 | Loss: 0.4595
Epoch: 037/130 | Batch 0050/0313 | Loss: 0.5615
Epoch: 037/130 | Batch 0100/0313 | Loss: 0.5767
Epoch: 037/130 | Batch 0150/0313 | Loss: 0.7250
Epoch: 037/130 | Batch 0200/0313 | Loss: 0.5307
Epoch: 037/130 | Batch 0250/0313 | Loss: 0.6183
Epoch: 037/130 | Batch 0300/0313 | Loss: 0.8052
**Epoch: 037/130 | Train. Acc.: 74.928% | Loss: 0.7318
**Epoch: 037/130 | Valid. Acc.: 78.530% | Loss: 0.6560
Time elapsed: 46.10 min
Epoch: 038/130 | Current Learning Rate: 0.100000
Epoch: 038/130 | Batch 0000/0313 | Loss: 0.6575
Epoch: 038/130 | Batch 0050/0313 | Loss: 0.5097
Epoch: 038/130 | Batch 0100/0313 | Loss: 0.5057
Epoch: 038/130 | Batch 0150/0313 | Loss: 0.5144
Epoch: 038/130 | Batch 0200/0313 | Loss: 0.4443
Epoch: 038/130 | Batch 0250/0313 | Loss: 0.5786
Epoch: 038/130 | Batch 0300/0313 | Loss: 0.5893
**Epoch: 038/130 | Train. Acc.: 79.440% | Loss: 0.5966
**Epoch: 038/130 | Valid. Acc.: 81.400% | Loss: 0.5524
**Validation loss decreased (0.616346 --> 0.552417). Saving model ...
Time elapsed: 47.36 min
Epoch: 039/130 | Current Learning Rate: 0.100000
Epoch: 039/130 | Batch 0000/0313 | Loss: 0.6686
Epoch: 039/130 | Batch 0050/0313 | Loss: 0.6343
Epoch: 039/130 | Batch 0100/0313 | Loss: 0.6421
Epoch: 039/130 | Batch 0150/0313 | Loss: 0.4926
Epoch: 039/130 | Batch 0200/0313 | Loss: 0.4284
Epoch: 039/130 | Batch 0250/0313 | Loss: 0.6092
Epoch: 039/130 | Batch 0300/0313 | Loss: 0.4667
**Epoch: 039/130 | Train. Acc.: 78.385% | Loss: 0.6169
**Epoch: 039/130 | Valid. Acc.: 80.820% | Loss: 0.5747
Time elapsed: 48.61 min
Epoch: 040/130 | Current Learning Rate: 0.100000
Epoch: 040/130 | Batch 0000/0313 | Loss: 0.5178
Epoch: 040/130 | Batch 0050/0313 | Loss: 0.5149
Epoch: 040/130 | Batch 0100/0313 | Loss: 0.5137
Epoch: 040/130 | Batch 0150/0313 | Loss: 0.4781
Epoch: 040/130 | Batch 0200/0313 | Loss: 0.6476
Epoch: 040/130 | Batch 0250/0313 | Loss: 0.5248
Epoch: 040/130 | Batch 0300/0313 | Loss: 0.4445
**Epoch: 040/130 | Train. Acc.: 70.987% | Loss: 0.8666
**Epoch: 040/130 | Valid. Acc.: 73.910% | Loss: 0.7923
Time elapsed: 49.85 min
Epoch: 041/130 | Current Learning Rate: 0.100000
Epoch: 041/130 | Batch 0000/0313 | Loss: 0.4427
Epoch: 041/130 | Batch 0050/0313 | Loss: 0.6214
Epoch: 041/130 | Batch 0100/0313 | Loss: 0.6258
Epoch: 041/130 | Batch 0150/0313 | Loss: 0.5680
Epoch: 041/130 | Batch 0200/0313 | Loss: 0.4523
Epoch: 041/130 | Batch 0250/0313 | Loss: 0.5451
Epoch: 041/130 | Batch 0300/0313 | Loss: 0.6274
**Epoch: 041/130 | Train. Acc.: 75.405% | Loss: 0.7216
**Epoch: 041/130 | Valid. Acc.: 75.850% | Loss: 0.7426
Time elapsed: 51.09 min
Epoch: 042/130 | Current Learning Rate: 0.100000
Epoch: 042/130 | Batch 0000/0313 | Loss: 0.5125
Epoch: 042/130 | Batch 0050/0313 | Loss: 0.5797
Epoch: 042/130 | Batch 0100/0313 | Loss: 0.5551
Epoch: 042/130 | Batch 0150/0313 | Loss: 0.5656
Epoch: 042/130 | Batch 0200/0313 | Loss: 0.5617
Epoch: 042/130 | Batch 0250/0313 | Loss: 0.6209
Epoch: 042/130 | Batch 0300/0313 | Loss: 0.5401
**Epoch: 042/130 | Train. Acc.: 70.950% | Loss: 0.9089
**Epoch: 042/130 | Valid. Acc.: 74.330% | Loss: 0.8688
Time elapsed: 52.34 min
Epoch: 043/130 | Current Learning Rate: 0.100000
Epoch: 043/130 | Batch 0000/0313 | Loss: 0.5755
Epoch: 043/130 | Batch 0050/0313 | Loss: 0.4905
Epoch: 043/130 | Batch 0100/0313 | Loss: 0.6762
Epoch: 043/130 | Batch 0150/0313 | Loss: 0.5185
Epoch: 043/130 | Batch 0200/0313 | Loss: 0.4500
Epoch: 043/130 | Batch 0250/0313 | Loss: 0.5293
Epoch: 043/130 | Batch 0300/0313 | Loss: 0.5562
**Epoch: 043/130 | Train. Acc.: 79.400% | Loss: 0.5906
**Epoch: 043/130 | Valid. Acc.: 81.760% | Loss: 0.5413
**Validation loss decreased (0.552417 --> 0.541265). Saving model ...
Time elapsed: 53.59 min
Epoch: 044/130 | Current Learning Rate: 0.100000
Epoch: 044/130 | Batch 0000/0313 | Loss: 0.5303
Epoch: 044/130 | Batch 0050/0313 | Loss: 0.5473
Epoch: 044/130 | Batch 0100/0313 | Loss: 0.5846
Epoch: 044/130 | Batch 0150/0313 | Loss: 0.3213
Epoch: 044/130 | Batch 0200/0313 | Loss: 0.4670
Epoch: 044/130 | Batch 0250/0313 | Loss: 0.5425
Epoch: 044/130 | Batch 0300/0313 | Loss: 0.3839
**Epoch: 044/130 | Train. Acc.: 74.922% | Loss: 0.7500
**Epoch: 044/130 | Valid. Acc.: 77.070% | Loss: 0.7047
Time elapsed: 54.83 min
Epoch: 045/130 | Current Learning Rate: 0.100000
Epoch: 045/130 | Batch 0000/0313 | Loss: 0.4680
Epoch: 045/130 | Batch 0050/0313 | Loss: 0.7232
Epoch: 045/130 | Batch 0100/0313 | Loss: 0.6356
Epoch: 045/130 | Batch 0150/0313 | Loss: 0.3988
Epoch: 045/130 | Batch 0200/0313 | Loss: 0.5558
Epoch: 045/130 | Batch 0250/0313 | Loss: 0.4119
Epoch: 045/130 | Batch 0300/0313 | Loss: 0.6006
**Epoch: 045/130 | Train. Acc.: 71.840% | Loss: 0.8598
**Epoch: 045/130 | Valid. Acc.: 73.960% | Loss: 0.8551
Time elapsed: 56.07 min
Epoch: 046/130 | Current Learning Rate: 0.100000
Epoch: 046/130 | Batch 0000/0313 | Loss: 0.4745
Epoch: 046/130 | Batch 0050/0313 | Loss: 0.6966
Epoch: 046/130 | Batch 0100/0313 | Loss: 0.4601
Epoch: 046/130 | Batch 0150/0313 | Loss: 0.4849
Epoch: 046/130 | Batch 0200/0313 | Loss: 0.5939
Epoch: 046/130 | Batch 0250/0313 | Loss: 0.5674
Epoch: 046/130 | Batch 0300/0313 | Loss: 0.5248
**Epoch: 046/130 | Train. Acc.: 76.502% | Loss: 0.6783
**Epoch: 046/130 | Valid. Acc.: 78.850% | Loss: 0.6232
Time elapsed: 57.31 min
Epoch: 047/130 | Current Learning Rate: 0.100000
Epoch: 047/130 | Batch 0000/0313 | Loss: 0.7031
Epoch: 047/130 | Batch 0050/0313 | Loss: 0.6050
Epoch: 047/130 | Batch 0100/0313 | Loss: 0.4452
Epoch: 047/130 | Batch 0150/0313 | Loss: 0.6576
Epoch: 047/130 | Batch 0200/0313 | Loss: 0.5209
Epoch: 047/130 | Batch 0250/0313 | Loss: 0.4059
Epoch: 047/130 | Batch 0300/0313 | Loss: 0.6363
**Epoch: 047/130 | Train. Acc.: 69.172% | Loss: 0.9916
**Epoch: 047/130 | Valid. Acc.: 72.020% | Loss: 0.9378
Time elapsed: 58.55 min
Epoch: 048/130 | Current Learning Rate: 0.100000
Epoch: 048/130 | Batch 0000/0313 | Loss: 0.5581
Epoch: 048/130 | Batch 0050/0313 | Loss: 0.5364
Epoch: 048/130 | Batch 0100/0313 | Loss: 0.5076
Epoch: 048/130 | Batch 0150/0313 | Loss: 0.5799
Epoch: 048/130 | Batch 0200/0313 | Loss: 0.4705
Epoch: 048/130 | Batch 0250/0313 | Loss: 0.4364
Epoch: 048/130 | Batch 0300/0313 | Loss: 0.6072
**Epoch: 048/130 | Train. Acc.: 69.608% | Loss: 0.9264
**Epoch: 048/130 | Valid. Acc.: 74.750% | Loss: 0.8228
Time elapsed: 59.80 min
Epoch: 049/130 | Current Learning Rate: 0.100000
Epoch: 049/130 | Batch 0000/0313 | Loss: 0.6244
Epoch: 049/130 | Batch 0050/0313 | Loss: 0.4210
Epoch: 049/130 | Batch 0100/0313 | Loss: 0.6105
Epoch: 049/130 | Batch 0150/0313 | Loss: 0.5853
Epoch: 049/130 | Batch 0200/0313 | Loss: 0.6134
Epoch: 049/130 | Batch 0250/0313 | Loss: 0.6042
Epoch: 049/130 | Batch 0300/0313 | Loss: 0.4971
**Epoch: 049/130 | Train. Acc.: 74.950% | Loss: 0.7605
**Epoch: 049/130 | Valid. Acc.: 77.710% | Loss: 0.6930
Time elapsed: 61.04 min
Epoch: 050/130 | Current Learning Rate: 0.100000
Epoch: 050/130 | Batch 0000/0313 | Loss: 0.4718
Epoch: 050/130 | Batch 0050/0313 | Loss: 0.6167
Epoch: 050/130 | Batch 0100/0313 | Loss: 0.5441
Epoch: 050/130 | Batch 0150/0313 | Loss: 0.4772
Epoch: 050/130 | Batch 0200/0313 | Loss: 0.5114
Epoch: 050/130 | Batch 0250/0313 | Loss: 0.4075
Epoch: 050/130 | Batch 0300/0313 | Loss: 0.7170
**Epoch: 050/130 | Train. Acc.: 74.405% | Loss: 0.7556
**Epoch: 050/130 | Valid. Acc.: 76.150% | Loss: 0.7631
Epoch 00050: reducing learning rate of group 0 to 5.0000e-02.
Time elapsed: 62.28 min
Epoch: 051/130 | Current Learning Rate: 0.050000
Epoch: 051/130 | Batch 0000/0313 | Loss: 0.4491
Epoch: 051/130 | Batch 0050/0313 | Loss: 0.5015
Epoch: 051/130 | Batch 0100/0313 | Loss: 0.3965
Epoch: 051/130 | Batch 0150/0313 | Loss: 0.3165
Epoch: 051/130 | Batch 0200/0313 | Loss: 0.3305
Epoch: 051/130 | Batch 0250/0313 | Loss: 0.5128
Epoch: 051/130 | Batch 0300/0313 | Loss: 0.3800
**Epoch: 051/130 | Train. Acc.: 84.672% | Loss: 0.4409
**Epoch: 051/130 | Valid. Acc.: 85.590% | Loss: 0.4205
**Validation loss decreased (0.541265 --> 0.420536). Saving model ...
Time elapsed: 63.53 min
Epoch: 052/130 | Current Learning Rate: 0.050000
Epoch: 052/130 | Batch 0000/0313 | Loss: 0.3002
Epoch: 052/130 | Batch 0050/0313 | Loss: 0.4334
Epoch: 052/130 | Batch 0100/0313 | Loss: 0.2845
Epoch: 052/130 | Batch 0150/0313 | Loss: 0.3740
Epoch: 052/130 | Batch 0200/0313 | Loss: 0.2934
Epoch: 052/130 | Batch 0250/0313 | Loss: 0.3234
Epoch: 052/130 | Batch 0300/0313 | Loss: 0.3317
**Epoch: 052/130 | Train. Acc.: 84.770% | Loss: 0.4392
**Epoch: 052/130 | Valid. Acc.: 86.340% | Loss: 0.4047
**Validation loss decreased (0.420536 --> 0.404735). Saving model ...
Time elapsed: 64.78 min
Epoch: 053/130 | Current Learning Rate: 0.050000
Epoch: 053/130 | Batch 0000/0313 | Loss: 0.3363
Epoch: 053/130 | Batch 0050/0313 | Loss: 0.2994
Epoch: 053/130 | Batch 0100/0313 | Loss: 0.1922
Epoch: 053/130 | Batch 0150/0313 | Loss: 0.2539
Epoch: 053/130 | Batch 0200/0313 | Loss: 0.3965
Epoch: 053/130 | Batch 0250/0313 | Loss: 0.5358
Epoch: 053/130 | Batch 0300/0313 | Loss: 0.4611
**Epoch: 053/130 | Train. Acc.: 85.188% | Loss: 0.4304
**Epoch: 053/130 | Valid. Acc.: 84.860% | Loss: 0.4515
Time elapsed: 66.02 min
Epoch: 054/130 | Current Learning Rate: 0.050000
Epoch: 054/130 | Batch 0000/0313 | Loss: 0.6432
Epoch: 054/130 | Batch 0050/0313 | Loss: 0.3818
Epoch: 054/130 | Batch 0100/0313 | Loss: 0.2172
Epoch: 054/130 | Batch 0150/0313 | Loss: 0.3493
Epoch: 054/130 | Batch 0200/0313 | Loss: 0.3614
Epoch: 054/130 | Batch 0250/0313 | Loss: 0.4045
Epoch: 054/130 | Batch 0300/0313 | Loss: 0.3303
**Epoch: 054/130 | Train. Acc.: 85.598% | Loss: 0.4207
**Epoch: 054/130 | Valid. Acc.: 86.460% | Loss: 0.4071
Time elapsed: 67.26 min
Epoch: 055/130 | Current Learning Rate: 0.050000
Epoch: 055/130 | Batch 0000/0313 | Loss: 0.3448
Epoch: 055/130 | Batch 0050/0313 | Loss: 0.3681
Epoch: 055/130 | Batch 0100/0313 | Loss: 0.4271
Epoch: 055/130 | Batch 0150/0313 | Loss: 0.3680
Epoch: 055/130 | Batch 0200/0313 | Loss: 0.3325
Epoch: 055/130 | Batch 0250/0313 | Loss: 0.3446
Epoch: 055/130 | Batch 0300/0313 | Loss: 0.3047
**Epoch: 055/130 | Train. Acc.: 84.630% | Loss: 0.4295
**Epoch: 055/130 | Valid. Acc.: 85.520% | Loss: 0.4212
Time elapsed: 68.50 min
Epoch: 056/130 | Current Learning Rate: 0.050000
Epoch: 056/130 | Batch 0000/0313 | Loss: 0.4723
Epoch: 056/130 | Batch 0050/0313 | Loss: 0.4568
Epoch: 056/130 | Batch 0100/0313 | Loss: 0.3404
Epoch: 056/130 | Batch 0150/0313 | Loss: 0.3421
Epoch: 056/130 | Batch 0200/0313 | Loss: 0.4593
Epoch: 056/130 | Batch 0250/0313 | Loss: 0.4097
Epoch: 056/130 | Batch 0300/0313 | Loss: 0.6782
**Epoch: 056/130 | Train. Acc.: 85.233% | Loss: 0.4270
**Epoch: 056/130 | Valid. Acc.: 85.550% | Loss: 0.4280
Time elapsed: 69.74 min
Epoch: 057/130 | Current Learning Rate: 0.050000
Epoch: 057/130 | Batch 0000/0313 | Loss: 0.2681
Epoch: 057/130 | Batch 0050/0313 | Loss: 0.3963
Epoch: 057/130 | Batch 0100/0313 | Loss: 0.3512
Epoch: 057/130 | Batch 0150/0313 | Loss: 0.2667
Epoch: 057/130 | Batch 0200/0313 | Loss: 0.3051
Epoch: 057/130 | Batch 0250/0313 | Loss: 0.5588
Epoch: 057/130 | Batch 0300/0313 | Loss: 0.4017
**Epoch: 057/130 | Train. Acc.: 86.052% | Loss: 0.4002
**Epoch: 057/130 | Valid. Acc.: 86.880% | Loss: 0.4033
**Validation loss decreased (0.404735 --> 0.403286). Saving model ...
Time elapsed: 70.99 min
Epoch: 058/130 | Current Learning Rate: 0.050000
Epoch: 058/130 | Batch 0000/0313 | Loss: 0.3937
Epoch: 058/130 | Batch 0050/0313 | Loss: 0.4032
Epoch: 058/130 | Batch 0100/0313 | Loss: 0.3488
Epoch: 058/130 | Batch 0150/0313 | Loss: 0.3358
Epoch: 058/130 | Batch 0200/0313 | Loss: 0.3834
Epoch: 058/130 | Batch 0250/0313 | Loss: 0.3765
Epoch: 058/130 | Batch 0300/0313 | Loss: 0.2435
**Epoch: 058/130 | Train. Acc.: 85.922% | Loss: 0.4011
**Epoch: 058/130 | Valid. Acc.: 86.510% | Loss: 0.3958
**Validation loss decreased (0.403286 --> 0.395759). Saving model ...
Time elapsed: 72.24 min
Epoch: 059/130 | Current Learning Rate: 0.050000
Epoch: 059/130 | Batch 0000/0313 | Loss: 0.4750
Epoch: 059/130 | Batch 0050/0313 | Loss: 0.3184
Epoch: 059/130 | Batch 0100/0313 | Loss: 0.3711
Epoch: 059/130 | Batch 0150/0313 | Loss: 0.3425
Epoch: 059/130 | Batch 0200/0313 | Loss: 0.2716
Epoch: 059/130 | Batch 0250/0313 | Loss: 0.3512
Epoch: 059/130 | Batch 0300/0313 | Loss: 0.3666
**Epoch: 059/130 | Train. Acc.: 84.022% | Loss: 0.4647
**Epoch: 059/130 | Valid. Acc.: 83.770% | Loss: 0.4857
Time elapsed: 73.48 min
Epoch: 060/130 | Current Learning Rate: 0.050000
Epoch: 060/130 | Batch 0000/0313 | Loss: 0.4228
Epoch: 060/130 | Batch 0050/0313 | Loss: 0.5093
Epoch: 060/130 | Batch 0100/0313 | Loss: 0.2540
Epoch: 060/130 | Batch 0150/0313 | Loss: 0.3212
Epoch: 060/130 | Batch 0200/0313 | Loss: 0.5099
Epoch: 060/130 | Batch 0250/0313 | Loss: 0.3626
Epoch: 060/130 | Batch 0300/0313 | Loss: 0.2961
**Epoch: 060/130 | Train. Acc.: 84.575% | Loss: 0.4454
**Epoch: 060/130 | Valid. Acc.: 85.260% | Loss: 0.4410
Time elapsed: 74.72 min
Epoch: 061/130 | Current Learning Rate: 0.050000
Epoch: 061/130 | Batch 0000/0313 | Loss: 0.4539
Epoch: 061/130 | Batch 0050/0313 | Loss: 0.1932
Epoch: 061/130 | Batch 0100/0313 | Loss: 0.3803
Epoch: 061/130 | Batch 0150/0313 | Loss: 0.4776
Epoch: 061/130 | Batch 0200/0313 | Loss: 0.4329
Epoch: 061/130 | Batch 0250/0313 | Loss: 0.4793
Epoch: 061/130 | Batch 0300/0313 | Loss: 0.3473
**Epoch: 061/130 | Train. Acc.: 83.727% | Loss: 0.4694
**Epoch: 061/130 | Valid. Acc.: 83.930% | Loss: 0.4881
Time elapsed: 75.96 min
Epoch: 062/130 | Current Learning Rate: 0.050000
Epoch: 062/130 | Batch 0000/0313 | Loss: 0.3965
Epoch: 062/130 | Batch 0050/0313 | Loss: 0.5455
Epoch: 062/130 | Batch 0100/0313 | Loss: 0.4314
Epoch: 062/130 | Batch 0150/0313 | Loss: 0.3708
Epoch: 062/130 | Batch 0200/0313 | Loss: 0.4091
Epoch: 062/130 | Batch 0250/0313 | Loss: 0.4636
Epoch: 062/130 | Batch 0300/0313 | Loss: 0.4671
**Epoch: 062/130 | Train. Acc.: 81.502% | Loss: 0.5440
**Epoch: 062/130 | Valid. Acc.: 82.370% | Loss: 0.5406
Time elapsed: 77.20 min
Epoch: 063/130 | Current Learning Rate: 0.050000
Epoch: 063/130 | Batch 0000/0313 | Loss: 0.4768
Epoch: 063/130 | Batch 0050/0313 | Loss: 0.4590
Epoch: 063/130 | Batch 0100/0313 | Loss: 0.3042
Epoch: 063/130 | Batch 0150/0313 | Loss: 0.3580
Epoch: 063/130 | Batch 0200/0313 | Loss: 0.4278
Epoch: 063/130 | Batch 0250/0313 | Loss: 0.4577
Epoch: 063/130 | Batch 0300/0313 | Loss: 0.4148
**Epoch: 063/130 | Train. Acc.: 83.890% | Loss: 0.4575
**Epoch: 063/130 | Valid. Acc.: 84.870% | Loss: 0.4497
Time elapsed: 78.44 min
Epoch: 064/130 | Current Learning Rate: 0.050000
Epoch: 064/130 | Batch 0000/0313 | Loss: 0.3463
Epoch: 064/130 | Batch 0050/0313 | Loss: 0.3480
Epoch: 064/130 | Batch 0100/0313 | Loss: 0.4307
Epoch: 064/130 | Batch 0150/0313 | Loss: 0.4850
Epoch: 064/130 | Batch 0200/0313 | Loss: 0.3289
Epoch: 064/130 | Batch 0250/0313 | Loss: 0.4302
Epoch: 064/130 | Batch 0300/0313 | Loss: 0.3258
**Epoch: 064/130 | Train. Acc.: 84.225% | Loss: 0.4587
**Epoch: 064/130 | Valid. Acc.: 84.990% | Loss: 0.4447
Time elapsed: 79.68 min
Epoch: 065/130 | Current Learning Rate: 0.050000
Epoch: 065/130 | Batch 0000/0313 | Loss: 0.2856
Epoch: 065/130 | Batch 0050/0313 | Loss: 0.3287
Epoch: 065/130 | Batch 0100/0313 | Loss: 0.3392
Epoch: 065/130 | Batch 0150/0313 | Loss: 0.3216
Epoch: 065/130 | Batch 0200/0313 | Loss: 0.4193
Epoch: 065/130 | Batch 0250/0313 | Loss: 0.3839
Epoch: 065/130 | Batch 0300/0313 | Loss: 0.3656
**Epoch: 065/130 | Train. Acc.: 85.312% | Loss: 0.4209
**Epoch: 065/130 | Valid. Acc.: 86.280% | Loss: 0.4161
Epoch 00065: reducing learning rate of group 0 to 2.5000e-02.
Time elapsed: 80.93 min
Epoch: 066/130 | Current Learning Rate: 0.025000
Epoch: 066/130 | Batch 0000/0313 | Loss: 0.4712
Epoch: 066/130 | Batch 0050/0313 | Loss: 0.3339
Epoch: 066/130 | Batch 0100/0313 | Loss: 0.2874
Epoch: 066/130 | Batch 0150/0313 | Loss: 0.2597
Epoch: 066/130 | Batch 0200/0313 | Loss: 0.3069
Epoch: 066/130 | Batch 0250/0313 | Loss: 0.2649
Epoch: 066/130 | Batch 0300/0313 | Loss: 0.3679
**Epoch: 066/130 | Train. Acc.: 91.405% | Loss: 0.2509
**Epoch: 066/130 | Valid. Acc.: 91.010% | Loss: 0.2722
**Validation loss decreased (0.395759 --> 0.272238). Saving model ...
Time elapsed: 82.18 min
Epoch: 067/130 | Current Learning Rate: 0.025000
Epoch: 067/130 | Batch 0000/0313 | Loss: 0.1587
Epoch: 067/130 | Batch 0050/0313 | Loss: 0.1278
Epoch: 067/130 | Batch 0100/0313 | Loss: 0.3240
Epoch: 067/130 | Batch 0150/0313 | Loss: 0.2429
Epoch: 067/130 | Batch 0200/0313 | Loss: 0.2892
Epoch: 067/130 | Batch 0250/0313 | Loss: 0.2637
Epoch: 067/130 | Batch 0300/0313 | Loss: 0.3514
**Epoch: 067/130 | Train. Acc.: 90.925% | Loss: 0.2612
**Epoch: 067/130 | Valid. Acc.: 90.350% | Loss: 0.2908
Time elapsed: 83.42 min
Epoch: 068/130 | Current Learning Rate: 0.025000
Epoch: 068/130 | Batch 0000/0313 | Loss: 0.3742
Epoch: 068/130 | Batch 0050/0313 | Loss: 0.1489
Epoch: 068/130 | Batch 0100/0313 | Loss: 0.1600
Epoch: 068/130 | Batch 0150/0313 | Loss: 0.1727
Epoch: 068/130 | Batch 0200/0313 | Loss: 0.2241
Epoch: 068/130 | Batch 0250/0313 | Loss: 0.2250
Epoch: 068/130 | Batch 0300/0313 | Loss: 0.2468
**Epoch: 068/130 | Train. Acc.: 91.350% | Loss: 0.2499
**Epoch: 068/130 | Valid. Acc.: 90.760% | Loss: 0.2830
Time elapsed: 84.67 min
Epoch: 069/130 | Current Learning Rate: 0.025000
Epoch: 069/130 | Batch 0000/0313 | Loss: 0.1961
Epoch: 069/130 | Batch 0050/0313 | Loss: 0.2411
Epoch: 069/130 | Batch 0100/0313 | Loss: 0.1817
Epoch: 069/130 | Batch 0150/0313 | Loss: 0.1854
Epoch: 069/130 | Batch 0200/0313 | Loss: 0.2448
Epoch: 069/130 | Batch 0250/0313 | Loss: 0.2231
Epoch: 069/130 | Batch 0300/0313 | Loss: 0.3155
**Epoch: 069/130 | Train. Acc.: 90.487% | Loss: 0.2712
**Epoch: 069/130 | Valid. Acc.: 89.360% | Loss: 0.3410
Time elapsed: 85.91 min
Epoch: 070/130 | Current Learning Rate: 0.025000
Epoch: 070/130 | Batch 0000/0313 | Loss: 0.2675
Epoch: 070/130 | Batch 0050/0313 | Loss: 0.3142
Epoch: 070/130 | Batch 0100/0313 | Loss: 0.1992
Epoch: 070/130 | Batch 0150/0313 | Loss: 0.2564
Epoch: 070/130 | Batch 0200/0313 | Loss: 0.2149
Epoch: 070/130 | Batch 0250/0313 | Loss: 0.3085
Epoch: 070/130 | Batch 0300/0313 | Loss: 0.3444
**Epoch: 070/130 | Train. Acc.: 90.710% | Loss: 0.2649
**Epoch: 070/130 | Valid. Acc.: 89.770% | Loss: 0.3030
Time elapsed: 87.16 min
Epoch: 071/130 | Current Learning Rate: 0.025000
Epoch: 071/130 | Batch 0000/0313 | Loss: 0.2446
Epoch: 071/130 | Batch 0050/0313 | Loss: 0.1753
Epoch: 071/130 | Batch 0100/0313 | Loss: 0.1872
Epoch: 071/130 | Batch 0150/0313 | Loss: 0.2506
Epoch: 071/130 | Batch 0200/0313 | Loss: 0.2331
Epoch: 071/130 | Batch 0250/0313 | Loss: 0.3011
Epoch: 071/130 | Batch 0300/0313 | Loss: 0.2527
**Epoch: 071/130 | Train. Acc.: 91.392% | Loss: 0.2476
**Epoch: 071/130 | Valid. Acc.: 90.330% | Loss: 0.2994
Time elapsed: 88.40 min
Epoch: 072/130 | Current Learning Rate: 0.025000
Epoch: 072/130 | Batch 0000/0313 | Loss: 0.2337
Epoch: 072/130 | Batch 0050/0313 | Loss: 0.1896
Epoch: 072/130 | Batch 0100/0313 | Loss: 0.1960
Epoch: 072/130 | Batch 0150/0313 | Loss: 0.2831
Epoch: 072/130 | Batch 0200/0313 | Loss: 0.2812
Epoch: 072/130 | Batch 0250/0313 | Loss: 0.2430
Epoch: 072/130 | Batch 0300/0313 | Loss: 0.2549
**Epoch: 072/130 | Train. Acc.: 91.325% | Loss: 0.2523
**Epoch: 072/130 | Valid. Acc.: 89.810% | Loss: 0.3059
Time elapsed: 89.64 min
Epoch: 073/130 | Current Learning Rate: 0.025000
Epoch: 073/130 | Batch 0000/0313 | Loss: 0.2364
Epoch: 073/130 | Batch 0050/0313 | Loss: 0.3830
Epoch: 073/130 | Batch 0100/0313 | Loss: 0.2233
Epoch: 073/130 | Batch 0150/0313 | Loss: 0.2544
Epoch: 073/130 | Batch 0200/0313 | Loss: 0.1470
Epoch: 073/130 | Batch 0250/0313 | Loss: 0.2879
Epoch: 073/130 | Batch 0300/0313 | Loss: 0.1434
**Epoch: 073/130 | Train. Acc.: 91.530% | Loss: 0.2396
**Epoch: 073/130 | Valid. Acc.: 90.400% | Loss: 0.3022
Epoch 00073: reducing learning rate of group 0 to 1.2500e-02.
Time elapsed: 90.88 min
Epoch: 074/130 | Current Learning Rate: 0.012500
Epoch: 074/130 | Batch 0000/0313 | Loss: 0.1952
Epoch: 074/130 | Batch 0050/0313 | Loss: 0.1330
Epoch: 074/130 | Batch 0100/0313 | Loss: 0.1271
Epoch: 074/130 | Batch 0150/0313 | Loss: 0.2300
Epoch: 074/130 | Batch 0200/0313 | Loss: 0.1645
Epoch: 074/130 | Batch 0250/0313 | Loss: 0.2071
Epoch: 074/130 | Batch 0300/0313 | Loss: 0.2155
**Epoch: 074/130 | Train. Acc.: 95.207% | Loss: 0.1387
**Epoch: 074/130 | Valid. Acc.: 92.840% | Loss: 0.2176
**Validation loss decreased (0.272238 --> 0.217602). Saving model ...
Time elapsed: 92.13 min
Epoch: 075/130 | Current Learning Rate: 0.012500
Epoch: 075/130 | Batch 0000/0313 | Loss: 0.1410
Epoch: 075/130 | Batch 0050/0313 | Loss: 0.2034
Epoch: 075/130 | Batch 0100/0313 | Loss: 0.1878
Epoch: 075/130 | Batch 0150/0313 | Loss: 0.1633
Epoch: 075/130 | Batch 0200/0313 | Loss: 0.1664
Epoch: 075/130 | Batch 0250/0313 | Loss: 0.1585
Epoch: 075/130 | Batch 0300/0313 | Loss: 0.1319
**Epoch: 075/130 | Train. Acc.: 95.282% | Loss: 0.1404
**Epoch: 075/130 | Valid. Acc.: 92.710% | Loss: 0.2258
Time elapsed: 93.37 min
Epoch: 076/130 | Current Learning Rate: 0.012500
Epoch: 076/130 | Batch 0000/0313 | Loss: 0.1705
Epoch: 076/130 | Batch 0050/0313 | Loss: 0.2213
Epoch: 076/130 | Batch 0100/0313 | Loss: 0.1181
Epoch: 076/130 | Batch 0150/0313 | Loss: 0.1080
Epoch: 076/130 | Batch 0200/0313 | Loss: 0.1935
Epoch: 076/130 | Batch 0250/0313 | Loss: 0.0525
Epoch: 076/130 | Batch 0300/0313 | Loss: 0.1463
**Epoch: 076/130 | Train. Acc.: 94.870% | Loss: 0.1452
**Epoch: 076/130 | Valid. Acc.: 92.070% | Loss: 0.2497
Time elapsed: 94.61 min
Epoch: 077/130 | Current Learning Rate: 0.012500
Epoch: 077/130 | Batch 0000/0313 | Loss: 0.0959
Epoch: 077/130 | Batch 0050/0313 | Loss: 0.1101
Epoch: 077/130 | Batch 0100/0313 | Loss: 0.1683
Epoch: 077/130 | Batch 0150/0313 | Loss: 0.1010
Epoch: 077/130 | Batch 0200/0313 | Loss: 0.3551
Epoch: 077/130 | Batch 0250/0313 | Loss: 0.1178
Epoch: 077/130 | Batch 0300/0313 | Loss: 0.0964
**Epoch: 077/130 | Train. Acc.: 95.042% | Loss: 0.1403
**Epoch: 077/130 | Valid. Acc.: 92.140% | Loss: 0.2445
Time elapsed: 95.85 min
Epoch: 078/130 | Current Learning Rate: 0.012500
Epoch: 078/130 | Batch 0000/0313 | Loss: 0.0896
Epoch: 078/130 | Batch 0050/0313 | Loss: 0.1009
Epoch: 078/130 | Batch 0100/0313 | Loss: 0.1014
Epoch: 078/130 | Batch 0150/0313 | Loss: 0.2396
Epoch: 078/130 | Batch 0200/0313 | Loss: 0.1180
Epoch: 078/130 | Batch 0250/0313 | Loss: 0.1448
Epoch: 078/130 | Batch 0300/0313 | Loss: 0.1383
**Epoch: 078/130 | Train. Acc.: 95.000% | Loss: 0.1387
**Epoch: 078/130 | Valid. Acc.: 91.760% | Loss: 0.2625
Time elapsed: 97.09 min
Epoch: 079/130 | Current Learning Rate: 0.012500
Epoch: 079/130 | Batch 0000/0313 | Loss: 0.1183
Epoch: 079/130 | Batch 0050/0313 | Loss: 0.1743
Epoch: 079/130 | Batch 0100/0313 | Loss: 0.1575
Epoch: 079/130 | Batch 0150/0313 | Loss: 0.1507
Epoch: 079/130 | Batch 0200/0313 | Loss: 0.1428
Epoch: 079/130 | Batch 0250/0313 | Loss: 0.1484
Epoch: 079/130 | Batch 0300/0313 | Loss: 0.1492
**Epoch: 079/130 | Train. Acc.: 94.432% | Loss: 0.1580
**Epoch: 079/130 | Valid. Acc.: 91.070% | Loss: 0.2695
Time elapsed: 98.33 min
Epoch: 080/130 | Current Learning Rate: 0.012500
Epoch: 080/130 | Batch 0000/0313 | Loss: 0.1027
Epoch: 080/130 | Batch 0050/0313 | Loss: 0.1027
Epoch: 080/130 | Batch 0100/0313 | Loss: 0.1683
Epoch: 080/130 | Batch 0150/0313 | Loss: 0.2539
Epoch: 080/130 | Batch 0200/0313 | Loss: 0.1338
Epoch: 080/130 | Batch 0250/0313 | Loss: 0.2685
Epoch: 080/130 | Batch 0300/0313 | Loss: 0.1894
**Epoch: 080/130 | Train. Acc.: 95.715% | Loss: 0.1247
**Epoch: 080/130 | Valid. Acc.: 92.250% | Loss: 0.2503
Time elapsed: 99.57 min
Epoch: 081/130 | Current Learning Rate: 0.012500
Epoch: 081/130 | Batch 0000/0313 | Loss: 0.1541
Epoch: 081/130 | Batch 0050/0313 | Loss: 0.1563
Epoch: 081/130 | Batch 0100/0313 | Loss: 0.1021
Epoch: 081/130 | Batch 0150/0313 | Loss: 0.0734
Epoch: 081/130 | Batch 0200/0313 | Loss: 0.1955
Epoch: 081/130 | Batch 0250/0313 | Loss: 0.1973
Epoch: 081/130 | Batch 0300/0313 | Loss: 0.1311
**Epoch: 081/130 | Train. Acc.: 95.595% | Loss: 0.1248
**Epoch: 081/130 | Valid. Acc.: 92.040% | Loss: 0.2571
Epoch 00081: reducing learning rate of group 0 to 6.2500e-03.
Time elapsed: 100.81 min
Epoch: 082/130 | Current Learning Rate: 0.006250
Epoch: 082/130 | Batch 0000/0313 | Loss: 0.1288
Epoch: 082/130 | Batch 0050/0313 | Loss: 0.1163
Epoch: 082/130 | Batch 0100/0313 | Loss: 0.0677
Epoch: 082/130 | Batch 0150/0313 | Loss: 0.1374
Epoch: 082/130 | Batch 0200/0313 | Loss: 0.1690
Epoch: 082/130 | Batch 0250/0313 | Loss: 0.0627
Epoch: 082/130 | Batch 0300/0313 | Loss: 0.0956
**Epoch: 082/130 | Train. Acc.: 97.450% | Loss: 0.0739
**Epoch: 082/130 | Valid. Acc.: 93.350% | Loss: 0.2235
Time elapsed: 102.05 min
Epoch: 083/130 | Current Learning Rate: 0.006250
Epoch: 083/130 | Batch 0000/0313 | Loss: 0.0772
Epoch: 083/130 | Batch 0050/0313 | Loss: 0.0518
Epoch: 083/130 | Batch 0100/0313 | Loss: 0.1004
Epoch: 083/130 | Batch 0150/0313 | Loss: 0.0680
Epoch: 083/130 | Batch 0200/0313 | Loss: 0.0973
Epoch: 083/130 | Batch 0250/0313 | Loss: 0.0565
Epoch: 083/130 | Batch 0300/0313 | Loss: 0.0828
**Epoch: 083/130 | Train. Acc.: 97.540% | Loss: 0.0750
**Epoch: 083/130 | Valid. Acc.: 93.280% | Loss: 0.2240
Time elapsed: 103.29 min
Epoch: 084/130 | Current Learning Rate: 0.006250
Epoch: 084/130 | Batch 0000/0313 | Loss: 0.1069
Epoch: 084/130 | Batch 0050/0313 | Loss: 0.0959
Epoch: 084/130 | Batch 0100/0313 | Loss: 0.1022
Epoch: 084/130 | Batch 0150/0313 | Loss: 0.0288
Epoch: 084/130 | Batch 0200/0313 | Loss: 0.1174
Epoch: 084/130 | Batch 0250/0313 | Loss: 0.0803
Epoch: 084/130 | Batch 0300/0313 | Loss: 0.0918
**Epoch: 084/130 | Train. Acc.: 97.550% | Loss: 0.0741
**Epoch: 084/130 | Valid. Acc.: 93.310% | Loss: 0.2214
Time elapsed: 104.53 min
Epoch: 085/130 | Current Learning Rate: 0.006250
Epoch: 085/130 | Batch 0000/0313 | Loss: 0.0287
Epoch: 085/130 | Batch 0050/0313 | Loss: 0.0680
Epoch: 085/130 | Batch 0100/0313 | Loss: 0.0981
Epoch: 085/130 | Batch 0150/0313 | Loss: 0.1279
Epoch: 085/130 | Batch 0200/0313 | Loss: 0.0711
Epoch: 085/130 | Batch 0250/0313 | Loss: 0.0847
Epoch: 085/130 | Batch 0300/0313 | Loss: 0.0444
**Epoch: 085/130 | Train. Acc.: 97.523% | Loss: 0.0702
**Epoch: 085/130 | Valid. Acc.: 93.290% | Loss: 0.2305
Time elapsed: 105.77 min
Epoch: 086/130 | Current Learning Rate: 0.006250
Epoch: 086/130 | Batch 0000/0313 | Loss: 0.0780
Epoch: 086/130 | Batch 0050/0313 | Loss: 0.0595
Epoch: 086/130 | Batch 0100/0313 | Loss: 0.0236
Epoch: 086/130 | Batch 0150/0313 | Loss: 0.0437
Epoch: 086/130 | Batch 0200/0313 | Loss: 0.0398
Epoch: 086/130 | Batch 0250/0313 | Loss: 0.0715
Epoch: 086/130 | Batch 0300/0313 | Loss: 0.0750
**Epoch: 086/130 | Train. Acc.: 97.927% | Loss: 0.0628
**Epoch: 086/130 | Valid. Acc.: 93.330% | Loss: 0.2264
Time elapsed: 107.01 min
Epoch: 087/130 | Current Learning Rate: 0.006250
Epoch: 087/130 | Batch 0000/0313 | Loss: 0.0829
Epoch: 087/130 | Batch 0050/0313 | Loss: 0.1004
Epoch: 087/130 | Batch 0100/0313 | Loss: 0.0448
Epoch: 087/130 | Batch 0150/0313 | Loss: 0.0737
Epoch: 087/130 | Batch 0200/0313 | Loss: 0.0971
Epoch: 087/130 | Batch 0250/0313 | Loss: 0.1471
Epoch: 087/130 | Batch 0300/0313 | Loss: 0.0941
**Epoch: 087/130 | Train. Acc.: 97.803% | Loss: 0.0639
**Epoch: 087/130 | Valid. Acc.: 93.260% | Loss: 0.2406
Time elapsed: 108.25 min
Epoch: 088/130 | Current Learning Rate: 0.006250
Epoch: 088/130 | Batch 0000/0313 | Loss: 0.0919
Epoch: 088/130 | Batch 0050/0313 | Loss: 0.0418
Epoch: 088/130 | Batch 0100/0313 | Loss: 0.0795
Epoch: 088/130 | Batch 0150/0313 | Loss: 0.0397
Epoch: 088/130 | Batch 0200/0313 | Loss: 0.0234
Epoch: 088/130 | Batch 0250/0313 | Loss: 0.1192
Epoch: 088/130 | Batch 0300/0313 | Loss: 0.1041
**Epoch: 088/130 | Train. Acc.: 97.733% | Loss: 0.0658
**Epoch: 088/130 | Valid. Acc.: 93.280% | Loss: 0.2416
Epoch 00088: reducing learning rate of group 0 to 3.1250e-03.
Time elapsed: 109.49 min
Epoch: 089/130 | Current Learning Rate: 0.003125
Epoch: 089/130 | Batch 0000/0313 | Loss: 0.1016
Epoch: 089/130 | Batch 0050/0313 | Loss: 0.0352
Epoch: 089/130 | Batch 0100/0313 | Loss: 0.0601
Epoch: 089/130 | Batch 0150/0313 | Loss: 0.0604
Epoch: 089/130 | Batch 0200/0313 | Loss: 0.0277
Epoch: 089/130 | Batch 0250/0313 | Loss: 0.0429
Epoch: 089/130 | Batch 0300/0313 | Loss: 0.0527
**Epoch: 089/130 | Train. Acc.: 98.722% | Loss: 0.0402
**Epoch: 089/130 | Valid. Acc.: 93.900% | Loss: 0.2132
**Validation loss decreased (0.217602 --> 0.213223). Saving model ...
Time elapsed: 110.74 min
Epoch: 090/130 | Current Learning Rate: 0.003125
Epoch: 090/130 | Batch 0000/0313 | Loss: 0.0977
Epoch: 090/130 | Batch 0050/0313 | Loss: 0.0682
Epoch: 090/130 | Batch 0100/0313 | Loss: 0.0668
Epoch: 090/130 | Batch 0150/0313 | Loss: 0.0158
Epoch: 090/130 | Batch 0200/0313 | Loss: 0.0591
Epoch: 090/130 | Batch 0250/0313 | Loss: 0.0735
Epoch: 090/130 | Batch 0300/0313 | Loss: 0.0404
**Epoch: 090/130 | Train. Acc.: 98.907% | Loss: 0.0353
**Epoch: 090/130 | Valid. Acc.: 94.330% | Loss: 0.2061
**Validation loss decreased (0.213223 --> 0.206141). Saving model ...
Time elapsed: 111.99 min
Epoch: 091/130 | Current Learning Rate: 0.003125
Epoch: 091/130 | Batch 0000/0313 | Loss: 0.0357
Epoch: 091/130 | Batch 0050/0313 | Loss: 0.0602
Epoch: 091/130 | Batch 0100/0313 | Loss: 0.0145
Epoch: 091/130 | Batch 0150/0313 | Loss: 0.0818
Epoch: 091/130 | Batch 0200/0313 | Loss: 0.0658
Epoch: 091/130 | Batch 0250/0313 | Loss: 0.0388
Epoch: 091/130 | Batch 0300/0313 | Loss: 0.0499
**Epoch: 091/130 | Train. Acc.: 98.887% | Loss: 0.0356
**Epoch: 091/130 | Valid. Acc.: 93.970% | Loss: 0.2110
Time elapsed: 113.23 min
Epoch: 092/130 | Current Learning Rate: 0.003125
Epoch: 092/130 | Batch 0000/0313 | Loss: 0.0276
Epoch: 092/130 | Batch 0050/0313 | Loss: 0.0231
Epoch: 092/130 | Batch 0100/0313 | Loss: 0.0147
Epoch: 092/130 | Batch 0150/0313 | Loss: 0.0301
Epoch: 092/130 | Batch 0200/0313 | Loss: 0.0420
Epoch: 092/130 | Batch 0250/0313 | Loss: 0.0460
Epoch: 092/130 | Batch 0300/0313 | Loss: 0.0387
**Epoch: 092/130 | Train. Acc.: 99.047% | Loss: 0.0301
**Epoch: 092/130 | Valid. Acc.: 93.930% | Loss: 0.2137
Time elapsed: 114.47 min
Epoch: 093/130 | Current Learning Rate: 0.003125
Epoch: 093/130 | Batch 0000/0313 | Loss: 0.0296
Epoch: 093/130 | Batch 0050/0313 | Loss: 0.0322
Epoch: 093/130 | Batch 0100/0313 | Loss: 0.0155
Epoch: 093/130 | Batch 0150/0313 | Loss: 0.0531
Epoch: 093/130 | Batch 0200/0313 | Loss: 0.0495
Epoch: 093/130 | Batch 0250/0313 | Loss: 0.0408
Epoch: 093/130 | Batch 0300/0313 | Loss: 0.0228
**Epoch: 093/130 | Train. Acc.: 98.830% | Loss: 0.0363
**Epoch: 093/130 | Valid. Acc.: 93.890% | Loss: 0.2264
Time elapsed: 115.71 min
Epoch: 094/130 | Current Learning Rate: 0.003125
Epoch: 094/130 | Batch 0000/0313 | Loss: 0.0186
Epoch: 094/130 | Batch 0050/0313 | Loss: 0.0642
Epoch: 094/130 | Batch 0100/0313 | Loss: 0.0355
Epoch: 094/130 | Batch 0150/0313 | Loss: 0.0210
Epoch: 094/130 | Batch 0200/0313 | Loss: 0.0360
Epoch: 094/130 | Batch 0250/0313 | Loss: 0.0151
Epoch: 094/130 | Batch 0300/0313 | Loss: 0.0486
**Epoch: 094/130 | Train. Acc.: 98.930% | Loss: 0.0348
**Epoch: 094/130 | Valid. Acc.: 93.660% | Loss: 0.2285
Time elapsed: 116.95 min
Epoch: 095/130 | Current Learning Rate: 0.003125
Epoch: 095/130 | Batch 0000/0313 | Loss: 0.0255
Epoch: 095/130 | Batch 0050/0313 | Loss: 0.0281
Epoch: 095/130 | Batch 0100/0313 | Loss: 0.0204
Epoch: 095/130 | Batch 0150/0313 | Loss: 0.0328
Epoch: 095/130 | Batch 0200/0313 | Loss: 0.0327
Epoch: 095/130 | Batch 0250/0313 | Loss: 0.0357
Epoch: 095/130 | Batch 0300/0313 | Loss: 0.0285
**Epoch: 095/130 | Train. Acc.: 99.252% | Loss: 0.0254
**Epoch: 095/130 | Valid. Acc.: 94.080% | Loss: 0.2167
Time elapsed: 118.19 min
Epoch: 096/130 | Current Learning Rate: 0.003125
Epoch: 096/130 | Batch 0000/0313 | Loss: 0.0475
Epoch: 096/130 | Batch 0050/0313 | Loss: 0.0586
Epoch: 096/130 | Batch 0100/0313 | Loss: 0.0288
Epoch: 096/130 | Batch 0150/0313 | Loss: 0.0455
Epoch: 096/130 | Batch 0200/0313 | Loss: 0.0338
Epoch: 096/130 | Batch 0250/0313 | Loss: 0.0814
Epoch: 096/130 | Batch 0300/0313 | Loss: 0.0238
**Epoch: 096/130 | Train. Acc.: 99.138% | Loss: 0.0278
**Epoch: 096/130 | Valid. Acc.: 94.260% | Loss: 0.2180
Time elapsed: 119.43 min
Epoch: 097/130 | Current Learning Rate: 0.003125
Epoch: 097/130 | Batch 0000/0313 | Loss: 0.0143
Epoch: 097/130 | Batch 0050/0313 | Loss: 0.0353
Epoch: 097/130 | Batch 0100/0313 | Loss: 0.0477
Epoch: 097/130 | Batch 0150/0313 | Loss: 0.0343
Epoch: 097/130 | Batch 0200/0313 | Loss: 0.0322
Epoch: 097/130 | Batch 0250/0313 | Loss: 0.0641
Epoch: 097/130 | Batch 0300/0313 | Loss: 0.0357
**Epoch: 097/130 | Train. Acc.: 99.145% | Loss: 0.0281
**Epoch: 097/130 | Valid. Acc.: 94.040% | Loss: 0.2256
Epoch 00097: reducing learning rate of group 0 to 1.5625e-03.
Time elapsed: 120.67 min
Epoch: 098/130 | Current Learning Rate: 0.001563
Epoch: 098/130 | Batch 0000/0313 | Loss: 0.0536
Epoch: 098/130 | Batch 0050/0313 | Loss: 0.0345
Epoch: 098/130 | Batch 0100/0313 | Loss: 0.0347
Epoch: 098/130 | Batch 0150/0313 | Loss: 0.0212
Epoch: 098/130 | Batch 0200/0313 | Loss: 0.0256
Epoch: 098/130 | Batch 0250/0313 | Loss: 0.0203
Epoch: 098/130 | Batch 0300/0313 | Loss: 0.0245
**Epoch: 098/130 | Train. Acc.: 99.375% | Loss: 0.0210
**Epoch: 098/130 | Valid. Acc.: 94.090% | Loss: 0.2130
Time elapsed: 121.92 min
Epoch: 099/130 | Current Learning Rate: 0.001563
Epoch: 099/130 | Batch 0000/0313 | Loss: 0.0169
Epoch: 099/130 | Batch 0050/0313 | Loss: 0.0203
Epoch: 099/130 | Batch 0100/0313 | Loss: 0.0432
Epoch: 099/130 | Batch 0150/0313 | Loss: 0.0522
Epoch: 099/130 | Batch 0200/0313 | Loss: 0.0336
Epoch: 099/130 | Batch 0250/0313 | Loss: 0.0243
Epoch: 099/130 | Batch 0300/0313 | Loss: 0.0471
**Epoch: 099/130 | Train. Acc.: 99.367% | Loss: 0.0203
**Epoch: 099/130 | Valid. Acc.: 94.240% | Loss: 0.2189
Time elapsed: 123.16 min
Epoch: 100/130 | Current Learning Rate: 0.001563
Epoch: 100/130 | Batch 0000/0313 | Loss: 0.0138
Epoch: 100/130 | Batch 0050/0313 | Loss: 0.0181
Epoch: 100/130 | Batch 0100/0313 | Loss: 0.0045
Epoch: 100/130 | Batch 0150/0313 | Loss: 0.1036
Epoch: 100/130 | Batch 0200/0313 | Loss: 0.0470
Epoch: 100/130 | Batch 0250/0313 | Loss: 0.0155
Epoch: 100/130 | Batch 0300/0313 | Loss: 0.0073
**Epoch: 100/130 | Train. Acc.: 99.457% | Loss: 0.0189
**Epoch: 100/130 | Valid. Acc.: 94.190% | Loss: 0.2181
Time elapsed: 124.41 min
Epoch: 101/130 | Current Learning Rate: 0.001563
Epoch: 101/130 | Batch 0000/0313 | Loss: 0.0364
Epoch: 101/130 | Batch 0050/0313 | Loss: 0.0169
Epoch: 101/130 | Batch 0100/0313 | Loss: 0.0073
Epoch: 101/130 | Batch 0150/0313 | Loss: 0.0326
Epoch: 101/130 | Batch 0200/0313 | Loss: 0.0119
Epoch: 101/130 | Batch 0250/0313 | Loss: 0.0335
Epoch: 101/130 | Batch 0300/0313 | Loss: 0.0229
**Epoch: 101/130 | Train. Acc.: 99.490% | Loss: 0.0179
**Epoch: 101/130 | Valid. Acc.: 94.160% | Loss: 0.2171
Time elapsed: 125.65 min
Epoch: 102/130 | Current Learning Rate: 0.001563
Epoch: 102/130 | Batch 0000/0313 | Loss: 0.0275
Epoch: 102/130 | Batch 0050/0313 | Loss: 0.0220
Epoch: 102/130 | Batch 0100/0313 | Loss: 0.0122
Epoch: 102/130 | Batch 0150/0313 | Loss: 0.0602
Epoch: 102/130 | Batch 0200/0313 | Loss: 0.0133
Epoch: 102/130 | Batch 0250/0313 | Loss: 0.0076
Epoch: 102/130 | Batch 0300/0313 | Loss: 0.0417
**Epoch: 102/130 | Train. Acc.: 99.530% | Loss: 0.0176
**Epoch: 102/130 | Valid. Acc.: 94.240% | Loss: 0.2215
Time elapsed: 126.90 min
Epoch: 103/130 | Current Learning Rate: 0.001563
Epoch: 103/130 | Batch 0000/0313 | Loss: 0.0279
Epoch: 103/130 | Batch 0050/0313 | Loss: 0.0225
Epoch: 103/130 | Batch 0100/0313 | Loss: 0.0199
Epoch: 103/130 | Batch 0150/0313 | Loss: 0.0063
Epoch: 103/130 | Batch 0200/0313 | Loss: 0.0119
Epoch: 103/130 | Batch 0250/0313 | Loss: 0.0323
Epoch: 103/130 | Batch 0300/0313 | Loss: 0.0277
**Epoch: 103/130 | Train. Acc.: 99.545% | Loss: 0.0168
**Epoch: 103/130 | Valid. Acc.: 94.360% | Loss: 0.2169
Time elapsed: 128.14 min
Epoch: 104/130 | Current Learning Rate: 0.001563
Epoch: 104/130 | Batch 0000/0313 | Loss: 0.0142
Epoch: 104/130 | Batch 0050/0313 | Loss: 0.0327
Epoch: 104/130 | Batch 0100/0313 | Loss: 0.0275
Epoch: 104/130 | Batch 0150/0313 | Loss: 0.0113
Epoch: 104/130 | Batch 0200/0313 | Loss: 0.0119
Epoch: 104/130 | Batch 0250/0313 | Loss: 0.0103
Epoch: 104/130 | Batch 0300/0313 | Loss: 0.0186
**Epoch: 104/130 | Train. Acc.: 99.535% | Loss: 0.0161
**Epoch: 104/130 | Valid. Acc.: 94.260% | Loss: 0.2270
Epoch 00104: reducing learning rate of group 0 to 7.8125e-04.
Time elapsed: 129.38 min
Epoch: 105/130 | Current Learning Rate: 0.000781
Epoch: 105/130 | Batch 0000/0313 | Loss: 0.0350
Epoch: 105/130 | Batch 0050/0313 | Loss: 0.0204
Epoch: 105/130 | Batch 0100/0313 | Loss: 0.0158
Epoch: 105/130 | Batch 0150/0313 | Loss: 0.0099
Epoch: 105/130 | Batch 0200/0313 | Loss: 0.0126
Epoch: 105/130 | Batch 0250/0313 | Loss: 0.0178
Epoch: 105/130 | Batch 0300/0313 | Loss: 0.0326
**Epoch: 105/130 | Train. Acc.: 99.525% | Loss: 0.0169
**Epoch: 105/130 | Valid. Acc.: 94.410% | Loss: 0.2157
Time elapsed: 130.63 min
Epoch: 106/130 | Current Learning Rate: 0.000781
Epoch: 106/130 | Batch 0000/0313 | Loss: 0.0198
Epoch: 106/130 | Batch 0050/0313 | Loss: 0.0091
Epoch: 106/130 | Batch 0100/0313 | Loss: 0.0113
Epoch: 106/130 | Batch 0150/0313 | Loss: 0.0204
Epoch: 106/130 | Batch 0200/0313 | Loss: 0.0051
Epoch: 106/130 | Batch 0250/0313 | Loss: 0.0150
Epoch: 106/130 | Batch 0300/0313 | Loss: 0.0077
**Epoch: 106/130 | Train. Acc.: 99.630% | Loss: 0.0142
**Epoch: 106/130 | Valid. Acc.: 94.420% | Loss: 0.2152
Time elapsed: 131.87 min
Epoch: 107/130 | Current Learning Rate: 0.000781
Epoch: 107/130 | Batch 0000/0313 | Loss: 0.0189
Epoch: 107/130 | Batch 0050/0313 | Loss: 0.0090
Epoch: 107/130 | Batch 0100/0313 | Loss: 0.0057
Epoch: 107/130 | Batch 0150/0313 | Loss: 0.0252
Epoch: 107/130 | Batch 0200/0313 | Loss: 0.0054
Epoch: 107/130 | Batch 0250/0313 | Loss: 0.0269
Epoch: 107/130 | Batch 0300/0313 | Loss: 0.0183
**Epoch: 107/130 | Train. Acc.: 99.618% | Loss: 0.0136
**Epoch: 107/130 | Valid. Acc.: 94.420% | Loss: 0.2170
Time elapsed: 133.11 min
Epoch: 108/130 | Current Learning Rate: 0.000781
Epoch: 108/130 | Batch 0000/0313 | Loss: 0.0338
Epoch: 108/130 | Batch 0050/0313 | Loss: 0.0055
Epoch: 108/130 | Batch 0100/0313 | Loss: 0.0079
Epoch: 108/130 | Batch 0150/0313 | Loss: 0.0218
Epoch: 108/130 | Batch 0200/0313 | Loss: 0.0129
Epoch: 108/130 | Batch 0250/0313 | Loss: 0.0040
Epoch: 108/130 | Batch 0300/0313 | Loss: 0.0064
**Epoch: 108/130 | Train. Acc.: 99.677% | Loss: 0.0120
**Epoch: 108/130 | Valid. Acc.: 94.370% | Loss: 0.2155
Time elapsed: 134.35 min
Epoch: 109/130 | Current Learning Rate: 0.000781
Epoch: 109/130 | Batch 0000/0313 | Loss: 0.0114
Epoch: 109/130 | Batch 0050/0313 | Loss: 0.0108
Epoch: 109/130 | Batch 0100/0313 | Loss: 0.0241
Epoch: 109/130 | Batch 0150/0313 | Loss: 0.0062
Epoch: 109/130 | Batch 0200/0313 | Loss: 0.0278
Epoch: 109/130 | Batch 0250/0313 | Loss: 0.0099
Epoch: 109/130 | Batch 0300/0313 | Loss: 0.0059
**Epoch: 109/130 | Train. Acc.: 99.668% | Loss: 0.0125
**Epoch: 109/130 | Valid. Acc.: 94.390% | Loss: 0.2186
Time elapsed: 135.59 min
Epoch: 110/130 | Current Learning Rate: 0.000781
Epoch: 110/130 | Batch 0000/0313 | Loss: 0.0040
Epoch: 110/130 | Batch 0050/0313 | Loss: 0.0074
Epoch: 110/130 | Batch 0100/0313 | Loss: 0.0145
Epoch: 110/130 | Batch 0150/0313 | Loss: 0.0079
Epoch: 110/130 | Batch 0200/0313 | Loss: 0.0208
Epoch: 110/130 | Batch 0250/0313 | Loss: 0.0217
Epoch: 110/130 | Batch 0300/0313 | Loss: 0.0065
**Epoch: 110/130 | Train. Acc.: 99.655% | Loss: 0.0128
**Epoch: 110/130 | Valid. Acc.: 94.230% | Loss: 0.2176
Time elapsed: 136.83 min
Epoch: 111/130 | Current Learning Rate: 0.000781
Epoch: 111/130 | Batch 0000/0313 | Loss: 0.0096
Epoch: 111/130 | Batch 0050/0313 | Loss: 0.0122
Epoch: 111/130 | Batch 0100/0313 | Loss: 0.0059
Epoch: 111/130 | Batch 0150/0313 | Loss: 0.0143
Epoch: 111/130 | Batch 0200/0313 | Loss: 0.0101
Epoch: 111/130 | Batch 0250/0313 | Loss: 0.0084
Epoch: 111/130 | Batch 0300/0313 | Loss: 0.0103
**Epoch: 111/130 | Train. Acc.: 99.615% | Loss: 0.0129
**Epoch: 111/130 | Valid. Acc.: 94.340% | Loss: 0.2225
Epoch 00111: reducing learning rate of group 0 to 3.9063e-04.
Time elapsed: 138.07 min
Epoch: 112/130 | Current Learning Rate: 0.000391
Epoch: 112/130 | Batch 0000/0313 | Loss: 0.0129
Epoch: 112/130 | Batch 0050/0313 | Loss: 0.0099
Epoch: 112/130 | Batch 0100/0313 | Loss: 0.0463
Epoch: 112/130 | Batch 0150/0313 | Loss: 0.0073
Epoch: 112/130 | Batch 0200/0313 | Loss: 0.0089
Epoch: 112/130 | Batch 0250/0313 | Loss: 0.0248
Epoch: 112/130 | Batch 0300/0313 | Loss: 0.0072
**Epoch: 112/130 | Train. Acc.: 99.670% | Loss: 0.0120
**Epoch: 112/130 | Valid. Acc.: 94.220% | Loss: 0.2202
Time elapsed: 139.31 min
Epoch: 113/130 | Current Learning Rate: 0.000391
Epoch: 113/130 | Batch 0000/0313 | Loss: 0.0108
Epoch: 113/130 | Batch 0050/0313 | Loss: 0.0209
Epoch: 113/130 | Batch 0100/0313 | Loss: 0.0373
Epoch: 113/130 | Batch 0150/0313 | Loss: 0.0220
Epoch: 113/130 | Batch 0200/0313 | Loss: 0.0313
Epoch: 113/130 | Batch 0250/0313 | Loss: 0.0914
Epoch: 113/130 | Batch 0300/0313 | Loss: 0.0055
**Epoch: 113/130 | Train. Acc.: 99.668% | Loss: 0.0119
**Epoch: 113/130 | Valid. Acc.: 94.360% | Loss: 0.2189
Time elapsed: 140.55 min
Epoch: 114/130 | Current Learning Rate: 0.000391
Epoch: 114/130 | Batch 0000/0313 | Loss: 0.0097
Epoch: 114/130 | Batch 0050/0313 | Loss: 0.0194
Epoch: 114/130 | Batch 0100/0313 | Loss: 0.0061
Epoch: 114/130 | Batch 0150/0313 | Loss: 0.0142
Epoch: 114/130 | Batch 0200/0313 | Loss: 0.0034
Epoch: 114/130 | Batch 0250/0313 | Loss: 0.0093
Epoch: 114/130 | Batch 0300/0313 | Loss: 0.0098
**Epoch: 114/130 | Train. Acc.: 99.700% | Loss: 0.0111
**Epoch: 114/130 | Valid. Acc.: 94.550% | Loss: 0.2191
Time elapsed: 141.79 min
Epoch: 115/130 | Current Learning Rate: 0.000391
Epoch: 115/130 | Batch 0000/0313 | Loss: 0.0100
Epoch: 115/130 | Batch 0050/0313 | Loss: 0.0215
Epoch: 115/130 | Batch 0100/0313 | Loss: 0.0151
Epoch: 115/130 | Batch 0150/0313 | Loss: 0.0021
Epoch: 115/130 | Batch 0200/0313 | Loss: 0.0108
Epoch: 115/130 | Batch 0250/0313 | Loss: 0.0068
Epoch: 115/130 | Batch 0300/0313 | Loss: 0.0043
**Epoch: 115/130 | Train. Acc.: 99.733% | Loss: 0.0099
**Epoch: 115/130 | Valid. Acc.: 94.550% | Loss: 0.2173
Time elapsed: 143.03 min
Epoch: 116/130 | Current Learning Rate: 0.000391
Epoch: 116/130 | Batch 0000/0313 | Loss: 0.0261
Epoch: 116/130 | Batch 0050/0313 | Loss: 0.0133
Epoch: 116/130 | Batch 0100/0313 | Loss: 0.0323
Epoch: 116/130 | Batch 0150/0313 | Loss: 0.0033
Epoch: 116/130 | Batch 0200/0313 | Loss: 0.0051
Epoch: 116/130 | Batch 0250/0313 | Loss: 0.0067
Epoch: 116/130 | Batch 0300/0313 | Loss: 0.0170
**Epoch: 116/130 | Train. Acc.: 99.708% | Loss: 0.0106
**Epoch: 116/130 | Valid. Acc.: 94.450% | Loss: 0.2199
Time elapsed: 144.28 min
Epoch: 117/130 | Current Learning Rate: 0.000391
Epoch: 117/130 | Batch 0000/0313 | Loss: 0.0045
Epoch: 117/130 | Batch 0050/0313 | Loss: 0.0188
Epoch: 117/130 | Batch 0100/0313 | Loss: 0.0167
Epoch: 117/130 | Batch 0150/0313 | Loss: 0.0185
Epoch: 117/130 | Batch 0200/0313 | Loss: 0.0239
Epoch: 117/130 | Batch 0250/0313 | Loss: 0.0102
Epoch: 117/130 | Batch 0300/0313 | Loss: 0.0397
**Epoch: 117/130 | Train. Acc.: 99.722% | Loss: 0.0109
**Epoch: 117/130 | Valid. Acc.: 94.410% | Loss: 0.2184
Time elapsed: 145.52 min
Epoch: 118/130 | Current Learning Rate: 0.000391
Epoch: 118/130 | Batch 0000/0313 | Loss: 0.0061
Epoch: 118/130 | Batch 0050/0313 | Loss: 0.0153
Epoch: 118/130 | Batch 0100/0313 | Loss: 0.0140
Epoch: 118/130 | Batch 0150/0313 | Loss: 0.0044
Epoch: 118/130 | Batch 0200/0313 | Loss: 0.0397
Epoch: 118/130 | Batch 0250/0313 | Loss: 0.0056
Epoch: 118/130 | Batch 0300/0313 | Loss: 0.0262
**Epoch: 118/130 | Train. Acc.: 99.737% | Loss: 0.0103
**Epoch: 118/130 | Valid. Acc.: 94.550% | Loss: 0.2231
Epoch 00118: reducing learning rate of group 0 to 1.9531e-04.
Time elapsed: 146.76 min
Epoch: 119/130 | Current Learning Rate: 0.000195
Epoch: 119/130 | Batch 0000/0313 | Loss: 0.0111
Epoch: 119/130 | Batch 0050/0313 | Loss: 0.0394
Epoch: 119/130 | Batch 0100/0313 | Loss: 0.0358
Epoch: 119/130 | Batch 0150/0313 | Loss: 0.0598
Epoch: 119/130 | Batch 0200/0313 | Loss: 0.0181
Epoch: 119/130 | Batch 0250/0313 | Loss: 0.0295
Epoch: 119/130 | Batch 0300/0313 | Loss: 0.0142
**Epoch: 119/130 | Train. Acc.: 99.765% | Loss: 0.0100
**Epoch: 119/130 | Valid. Acc.: 94.480% | Loss: 0.2193
Time elapsed: 148.00 min
Epoch: 120/130 | Current Learning Rate: 0.000195
Epoch: 120/130 | Batch 0000/0313 | Loss: 0.0090
Epoch: 120/130 | Batch 0050/0313 | Loss: 0.0136
Epoch: 120/130 | Batch 0100/0313 | Loss: 0.0304
Epoch: 120/130 | Batch 0150/0313 | Loss: 0.0061
Epoch: 120/130 | Batch 0200/0313 | Loss: 0.0113
Epoch: 120/130 | Batch 0250/0313 | Loss: 0.0110
Epoch: 120/130 | Batch 0300/0313 | Loss: 0.0092
**Epoch: 120/130 | Train. Acc.: 99.757% | Loss: 0.0095
**Epoch: 120/130 | Valid. Acc.: 94.580% | Loss: 0.2201
Time elapsed: 149.24 min
Epoch: 121/130 | Current Learning Rate: 0.000195
Epoch: 121/130 | Batch 0000/0313 | Loss: 0.0502
Epoch: 121/130 | Batch 0050/0313 | Loss: 0.0250
Epoch: 121/130 | Batch 0100/0313 | Loss: 0.0115
Epoch: 121/130 | Batch 0150/0313 | Loss: 0.0380
Epoch: 121/130 | Batch 0200/0313 | Loss: 0.0141
Epoch: 121/130 | Batch 0250/0313 | Loss: 0.0059
Epoch: 121/130 | Batch 0300/0313 | Loss: 0.0190
**Epoch: 121/130 | Train. Acc.: 99.765% | Loss: 0.0095
**Epoch: 121/130 | Valid. Acc.: 94.550% | Loss: 0.2183
Time elapsed: 150.48 min
Epoch: 122/130 | Current Learning Rate: 0.000195
Epoch: 122/130 | Batch 0000/0313 | Loss: 0.0534
Epoch: 122/130 | Batch 0050/0313 | Loss: 0.0026
Epoch: 122/130 | Batch 0100/0313 | Loss: 0.0085
Epoch: 122/130 | Batch 0150/0313 | Loss: 0.0349
Epoch: 122/130 | Batch 0200/0313 | Loss: 0.0096
Epoch: 122/130 | Batch 0250/0313 | Loss: 0.0223
Epoch: 122/130 | Batch 0300/0313 | Loss: 0.0310
**Epoch: 122/130 | Train. Acc.: 99.733% | Loss: 0.0105
**Epoch: 122/130 | Valid. Acc.: 94.600% | Loss: 0.2210
Time elapsed: 151.72 min
Epoch: 123/130 | Current Learning Rate: 0.000195
Epoch: 123/130 | Batch 0000/0313 | Loss: 0.0139
Epoch: 123/130 | Batch 0050/0313 | Loss: 0.0070
Epoch: 123/130 | Batch 0100/0313 | Loss: 0.0071
Epoch: 123/130 | Batch 0150/0313 | Loss: 0.0075
Epoch: 123/130 | Batch 0200/0313 | Loss: 0.0134
Epoch: 123/130 | Batch 0250/0313 | Loss: 0.0087
Epoch: 123/130 | Batch 0300/0313 | Loss: 0.0036
**Epoch: 123/130 | Train. Acc.: 99.752% | Loss: 0.0100
**Epoch: 123/130 | Valid. Acc.: 94.540% | Loss: 0.2213
Time elapsed: 152.96 min
Epoch: 124/130 | Current Learning Rate: 0.000195
Epoch: 124/130 | Batch 0000/0313 | Loss: 0.0066
Epoch: 124/130 | Batch 0050/0313 | Loss: 0.0045
Epoch: 124/130 | Batch 0100/0313 | Loss: 0.0454
Epoch: 124/130 | Batch 0150/0313 | Loss: 0.0038
Epoch: 124/130 | Batch 0200/0313 | Loss: 0.0231
Epoch: 124/130 | Batch 0250/0313 | Loss: 0.0033
Epoch: 124/130 | Batch 0300/0313 | Loss: 0.0186
**Epoch: 124/130 | Train. Acc.: 99.763% | Loss: 0.0093
**Epoch: 124/130 | Valid. Acc.: 94.570% | Loss: 0.2196
Time elapsed: 154.21 min
Epoch: 125/130 | Current Learning Rate: 0.000195
Epoch: 125/130 | Batch 0000/0313 | Loss: 0.0037
Epoch: 125/130 | Batch 0050/0313 | Loss: 0.0284
Epoch: 125/130 | Batch 0100/0313 | Loss: 0.0162
Epoch: 125/130 | Batch 0150/0313 | Loss: 0.0108
Epoch: 125/130 | Batch 0200/0313 | Loss: 0.0095
Epoch: 125/130 | Batch 0250/0313 | Loss: 0.0151
Epoch: 125/130 | Batch 0300/0313 | Loss: 0.0156
**Epoch: 125/130 | Train. Acc.: 99.733% | Loss: 0.0094
**Epoch: 125/130 | Valid. Acc.: 94.490% | Loss: 0.2216
Epoch 00125: reducing learning rate of group 0 to 9.7656e-05.
Time elapsed: 155.45 min
Epoch: 126/130 | Current Learning Rate: 0.000098
Epoch: 126/130 | Batch 0000/0313 | Loss: 0.0289
Epoch: 126/130 | Batch 0050/0313 | Loss: 0.0310
Epoch: 126/130 | Batch 0100/0313 | Loss: 0.0028
Epoch: 126/130 | Batch 0150/0313 | Loss: 0.0030
Epoch: 126/130 | Batch 0200/0313 | Loss: 0.0054
Epoch: 126/130 | Batch 0250/0313 | Loss: 0.0056
Epoch: 126/130 | Batch 0300/0313 | Loss: 0.0055
**Epoch: 126/130 | Train. Acc.: 99.722% | Loss: 0.0104
**Epoch: 126/130 | Valid. Acc.: 94.530% | Loss: 0.2188
Time elapsed: 156.69 min
Epoch: 127/130 | Current Learning Rate: 0.000098
Epoch: 127/130 | Batch 0000/0313 | Loss: 0.0034
Epoch: 127/130 | Batch 0050/0313 | Loss: 0.0111
Epoch: 127/130 | Batch 0100/0313 | Loss: 0.0047
Epoch: 127/130 | Batch 0150/0313 | Loss: 0.0220
Epoch: 127/130 | Batch 0200/0313 | Loss: 0.0303
Epoch: 127/130 | Batch 0250/0313 | Loss: 0.0047
Epoch: 127/130 | Batch 0300/0313 | Loss: 0.0082
**Epoch: 127/130 | Train. Acc.: 99.772% | Loss: 0.0090
**Epoch: 127/130 | Valid. Acc.: 94.640% | Loss: 0.2167
Time elapsed: 157.93 min
Epoch: 128/130 | Current Learning Rate: 0.000098
Epoch: 128/130 | Batch 0000/0313 | Loss: 0.0064
Epoch: 128/130 | Batch 0050/0313 | Loss: 0.0183
Epoch: 128/130 | Batch 0100/0313 | Loss: 0.0041
Epoch: 128/130 | Batch 0150/0313 | Loss: 0.0461
Epoch: 128/130 | Batch 0200/0313 | Loss: 0.0120
Epoch: 128/130 | Batch 0250/0313 | Loss: 0.0065
Epoch: 128/130 | Batch 0300/0313 | Loss: 0.0024
**Epoch: 128/130 | Train. Acc.: 99.688% | Loss: 0.0103
**Epoch: 128/130 | Valid. Acc.: 94.540% | Loss: 0.2179
Time elapsed: 159.18 min
Epoch: 129/130 | Current Learning Rate: 0.000098
Epoch: 129/130 | Batch 0000/0313 | Loss: 0.0125
Epoch: 129/130 | Batch 0050/0313 | Loss: 0.0081
Epoch: 129/130 | Batch 0100/0313 | Loss: 0.0079
Epoch: 129/130 | Batch 0150/0313 | Loss: 0.0151
Epoch: 129/130 | Batch 0200/0313 | Loss: 0.0611
Epoch: 129/130 | Batch 0250/0313 | Loss: 0.0359
Epoch: 129/130 | Batch 0300/0313 | Loss: 0.0054
**Epoch: 129/130 | Train. Acc.: 99.730% | Loss: 0.0101
**Epoch: 129/130 | Valid. Acc.: 94.460% | Loss: 0.2238
Time elapsed: 160.42 min
Epoch: 130/130 | Current Learning Rate: 0.000098
Epoch: 130/130 | Batch 0000/0313 | Loss: 0.0083
Epoch: 130/130 | Batch 0050/0313 | Loss: 0.0065
Epoch: 130/130 | Batch 0100/0313 | Loss: 0.0175
Epoch: 130/130 | Batch 0150/0313 | Loss: 0.0029
Epoch: 130/130 | Batch 0200/0313 | Loss: 0.0109
Epoch: 130/130 | Batch 0250/0313 | Loss: 0.0048
Epoch: 130/130 | Batch 0300/0313 | Loss: 0.0106
**Epoch: 130/130 | Train. Acc.: 99.803% | Loss: 0.0089
**Epoch: 130/130 | Valid. Acc.: 94.560% | Loss: 0.2177
Time elapsed: 161.67 min
Total Training Time: 161.67 min
Model: ResNet101
Test Loss: 0.2210
Test Accuracy (Overall): 93.73%

Test Accuracy of Airplane: 95% (956/1000)
Test Accuracy of      Car: 97% (973/1000)
Test Accuracy of     Bird: 91% (918/1000)
Test Accuracy of      Cat: 86% (862/1000)
Test Accuracy of     Deer: 94% (949/1000)
Test Accuracy of      Dog: 88% (888/1000)
Test Accuracy of     Frog: 96% (963/1000)
Test Accuracy of    Horse: 94% (945/1000)
Test Accuracy of     Ship: 96% (968/1000)
Test Accuracy of    Truck: 95% (951/1000)
Training ResNet152 for 130 epochs with initial learning rate 0.1...
Epoch: 001/130 | Current Learning Rate: 0.100000
Epoch: 001/130 | Batch 0000/0313 | Loss: 2.4745
Epoch: 001/130 | Batch 0050/0313 | Loss: 2.6569
Epoch: 001/130 | Batch 0100/0313 | Loss: 2.3853
Epoch: 001/130 | Batch 0150/0313 | Loss: 2.4286
Epoch: 001/130 | Batch 0200/0313 | Loss: 2.3368
Epoch: 001/130 | Batch 0250/0313 | Loss: 2.3875
Epoch: 001/130 | Batch 0300/0313 | Loss: 2.2866
**Epoch: 001/130 | Train. Acc.: 10.080% | Loss: 2.3506
**Epoch: 001/130 | Valid. Acc.: 10.460% | Loss: 2.3994
**Validation loss decreased (inf --> 2.399402). Saving model ...
Time elapsed: 1.78 min
Epoch: 002/130 | Current Learning Rate: 0.100000
Epoch: 002/130 | Batch 0000/0313 | Loss: 2.3240
Epoch: 002/130 | Batch 0050/0313 | Loss: 2.3733
Epoch: 002/130 | Batch 0100/0313 | Loss: 2.2794
Epoch: 002/130 | Batch 0150/0313 | Loss: 2.1704
Epoch: 002/130 | Batch 0200/0313 | Loss: 2.1145
Epoch: 002/130 | Batch 0250/0313 | Loss: 2.0199
Epoch: 002/130 | Batch 0300/0313 | Loss: 2.0433
**Epoch: 002/130 | Train. Acc.: 17.890% | Loss: 2.1310
**Epoch: 002/130 | Valid. Acc.: 19.250% | Loss: 2.1015
**Validation loss decreased (2.399402 --> 2.101453). Saving model ...
Time elapsed: 3.56 min
Epoch: 003/130 | Current Learning Rate: 0.100000
Epoch: 003/130 | Batch 0000/0313 | Loss: 2.0514
Epoch: 003/130 | Batch 0050/0313 | Loss: 1.9691
Epoch: 003/130 | Batch 0100/0313 | Loss: 1.9084
Epoch: 003/130 | Batch 0150/0313 | Loss: 2.0355
Epoch: 003/130 | Batch 0200/0313 | Loss: 1.9987
Epoch: 003/130 | Batch 0250/0313 | Loss: 1.8802
Epoch: 003/130 | Batch 0300/0313 | Loss: 1.9540
**Epoch: 003/130 | Train. Acc.: 24.758% | Loss: 1.9448
**Epoch: 003/130 | Valid. Acc.: 29.430% | Loss: 1.8525
**Validation loss decreased (2.101453 --> 1.852493). Saving model ...
Time elapsed: 5.35 min
Epoch: 004/130 | Current Learning Rate: 0.100000
Epoch: 004/130 | Batch 0000/0313 | Loss: 1.9179
Epoch: 004/130 | Batch 0050/0313 | Loss: 1.7368
Epoch: 004/130 | Batch 0100/0313 | Loss: 1.7579
Epoch: 004/130 | Batch 0150/0313 | Loss: 1.7184
Epoch: 004/130 | Batch 0200/0313 | Loss: 1.7359
Epoch: 004/130 | Batch 0250/0313 | Loss: 1.8172
Epoch: 004/130 | Batch 0300/0313 | Loss: 1.8961
**Epoch: 004/130 | Train. Acc.: 35.715% | Loss: 1.7098
**Epoch: 004/130 | Valid. Acc.: 35.280% | Loss: 1.7101
**Validation loss decreased (1.852493 --> 1.710122). Saving model ...
Time elapsed: 7.13 min
Epoch: 005/130 | Current Learning Rate: 0.100000
Epoch: 005/130 | Batch 0000/0313 | Loss: 1.7153
Epoch: 005/130 | Batch 0050/0313 | Loss: 1.5926
Epoch: 005/130 | Batch 0100/0313 | Loss: 1.6673
Epoch: 005/130 | Batch 0150/0313 | Loss: 1.7755
Epoch: 005/130 | Batch 0200/0313 | Loss: 1.7031
Epoch: 005/130 | Batch 0250/0313 | Loss: 1.7695
Epoch: 005/130 | Batch 0300/0313 | Loss: 1.5550
**Epoch: 005/130 | Train. Acc.: 39.417% | Loss: 1.6811
**Epoch: 005/130 | Valid. Acc.: 39.770% | Loss: 1.6692
**Validation loss decreased (1.710122 --> 1.669219). Saving model ...
Time elapsed: 8.90 min
Epoch: 006/130 | Current Learning Rate: 0.100000
Epoch: 006/130 | Batch 0000/0313 | Loss: 1.6563
Epoch: 006/130 | Batch 0050/0313 | Loss: 1.5731
Epoch: 006/130 | Batch 0100/0313 | Loss: 1.6124
Epoch: 006/130 | Batch 0150/0313 | Loss: 1.9095
Epoch: 006/130 | Batch 0200/0313 | Loss: 1.4465
Epoch: 006/130 | Batch 0250/0313 | Loss: 1.4257
Epoch: 006/130 | Batch 0300/0313 | Loss: 1.3624
**Epoch: 006/130 | Train. Acc.: 45.958% | Loss: 1.4908
**Epoch: 006/130 | Valid. Acc.: 49.560% | Loss: 1.4014
**Validation loss decreased (1.669219 --> 1.401435). Saving model ...
Time elapsed: 10.68 min
Epoch: 007/130 | Current Learning Rate: 0.100000
Epoch: 007/130 | Batch 0000/0313 | Loss: 1.4534
Epoch: 007/130 | Batch 0050/0313 | Loss: 1.4043
Epoch: 007/130 | Batch 0100/0313 | Loss: 1.4491
Epoch: 007/130 | Batch 0150/0313 | Loss: 1.3722
Epoch: 007/130 | Batch 0200/0313 | Loss: 1.2069
Epoch: 007/130 | Batch 0250/0313 | Loss: 1.4244
Epoch: 007/130 | Batch 0300/0313 | Loss: 1.3374
**Epoch: 007/130 | Train. Acc.: 50.123% | Loss: 1.3873
**Epoch: 007/130 | Valid. Acc.: 56.100% | Loss: 1.2590
**Validation loss decreased (1.401435 --> 1.259005). Saving model ...
Time elapsed: 12.46 min
Epoch: 008/130 | Current Learning Rate: 0.100000
Epoch: 008/130 | Batch 0000/0313 | Loss: 1.2575
Epoch: 008/130 | Batch 0050/0313 | Loss: 1.3135
Epoch: 008/130 | Batch 0100/0313 | Loss: 1.2143
Epoch: 008/130 | Batch 0150/0313 | Loss: 1.3150
Epoch: 008/130 | Batch 0200/0313 | Loss: 1.1652
Epoch: 008/130 | Batch 0250/0313 | Loss: 1.2183
Epoch: 008/130 | Batch 0300/0313 | Loss: 1.1823
**Epoch: 008/130 | Train. Acc.: 50.530% | Loss: 1.4049
**Epoch: 008/130 | Valid. Acc.: 55.520% | Loss: 1.2932
Time elapsed: 14.22 min
Epoch: 009/130 | Current Learning Rate: 0.100000
Epoch: 009/130 | Batch 0000/0313 | Loss: 1.2669
Epoch: 009/130 | Batch 0050/0313 | Loss: 1.3283
Epoch: 009/130 | Batch 0100/0313 | Loss: 1.2568
Epoch: 009/130 | Batch 0150/0313 | Loss: 1.2638
Epoch: 009/130 | Batch 0200/0313 | Loss: 1.0857
Epoch: 009/130 | Batch 0250/0313 | Loss: 1.1959
Epoch: 009/130 | Batch 0300/0313 | Loss: 1.0205
**Epoch: 009/130 | Train. Acc.: 52.383% | Loss: 1.3313
**Epoch: 009/130 | Valid. Acc.: 57.220% | Loss: 1.1989
**Validation loss decreased (1.259005 --> 1.198894). Saving model ...
Time elapsed: 16.00 min
Epoch: 010/130 | Current Learning Rate: 0.100000
Epoch: 010/130 | Batch 0000/0313 | Loss: 1.1485
Epoch: 010/130 | Batch 0050/0313 | Loss: 0.9732
Epoch: 010/130 | Batch 0100/0313 | Loss: 1.0997
Epoch: 010/130 | Batch 0150/0313 | Loss: 1.0804
Epoch: 010/130 | Batch 0200/0313 | Loss: 1.1527
Epoch: 010/130 | Batch 0250/0313 | Loss: 1.1890
Epoch: 010/130 | Batch 0300/0313 | Loss: 1.1452
**Epoch: 010/130 | Train. Acc.: 60.900% | Loss: 1.0953
**Epoch: 010/130 | Valid. Acc.: 66.400% | Loss: 0.9372
**Validation loss decreased (1.198894 --> 0.937159). Saving model ...
Time elapsed: 17.78 min
Epoch: 011/130 | Current Learning Rate: 0.100000
Epoch: 011/130 | Batch 0000/0313 | Loss: 0.9746
Epoch: 011/130 | Batch 0050/0313 | Loss: 0.9750
Epoch: 011/130 | Batch 0100/0313 | Loss: 1.0514
Epoch: 011/130 | Batch 0150/0313 | Loss: 1.2516
Epoch: 011/130 | Batch 0200/0313 | Loss: 0.9029
Epoch: 011/130 | Batch 0250/0313 | Loss: 0.9936
Epoch: 011/130 | Batch 0300/0313 | Loss: 1.0655
**Epoch: 011/130 | Train. Acc.: 61.933% | Loss: 1.0687
**Epoch: 011/130 | Valid. Acc.: 63.290% | Loss: 1.0391
Time elapsed: 19.54 min
Epoch: 012/130 | Current Learning Rate: 0.100000
Epoch: 012/130 | Batch 0000/0313 | Loss: 1.0608
Epoch: 012/130 | Batch 0050/0313 | Loss: 0.9756
Epoch: 012/130 | Batch 0100/0313 | Loss: 1.0665
Epoch: 012/130 | Batch 0150/0313 | Loss: 1.0081
Epoch: 012/130 | Batch 0200/0313 | Loss: 0.8958
Epoch: 012/130 | Batch 0250/0313 | Loss: 0.9752
Epoch: 012/130 | Batch 0300/0313 | Loss: 0.9281
**Epoch: 012/130 | Train. Acc.: 61.655% | Loss: 1.1013
**Epoch: 012/130 | Valid. Acc.: 67.070% | Loss: 0.9592
Time elapsed: 21.31 min
Epoch: 013/130 | Current Learning Rate: 0.100000
Epoch: 013/130 | Batch 0000/0313 | Loss: 0.9954
Epoch: 013/130 | Batch 0050/0313 | Loss: 0.9682
Epoch: 013/130 | Batch 0100/0313 | Loss: 0.8951
Epoch: 013/130 | Batch 0150/0313 | Loss: 1.0486
Epoch: 013/130 | Batch 0200/0313 | Loss: 0.9734
Epoch: 013/130 | Batch 0250/0313 | Loss: 0.9913
Epoch: 013/130 | Batch 0300/0313 | Loss: 0.9292
**Epoch: 013/130 | Train. Acc.: 59.798% | Loss: 1.1553
**Epoch: 013/130 | Valid. Acc.: 64.010% | Loss: 1.0388
Time elapsed: 23.07 min
Epoch: 014/130 | Current Learning Rate: 0.100000
Epoch: 014/130 | Batch 0000/0313 | Loss: 0.9614
Epoch: 014/130 | Batch 0050/0313 | Loss: 0.9519
Epoch: 014/130 | Batch 0100/0313 | Loss: 0.8441
Epoch: 014/130 | Batch 0150/0313 | Loss: 1.1013
Epoch: 014/130 | Batch 0200/0313 | Loss: 0.7098
Epoch: 014/130 | Batch 0250/0313 | Loss: 0.8405
Epoch: 014/130 | Batch 0300/0313 | Loss: 0.6689
**Epoch: 014/130 | Train. Acc.: 65.353% | Loss: 0.9771
**Epoch: 014/130 | Valid. Acc.: 68.740% | Loss: 0.8810
**Validation loss decreased (0.937159 --> 0.880956). Saving model ...
Time elapsed: 24.85 min
Epoch: 015/130 | Current Learning Rate: 0.100000
Epoch: 015/130 | Batch 0000/0313 | Loss: 1.0086
Epoch: 015/130 | Batch 0050/0313 | Loss: 0.9840
Epoch: 015/130 | Batch 0100/0313 | Loss: 0.7705
Epoch: 015/130 | Batch 0150/0313 | Loss: 1.1600
Epoch: 015/130 | Batch 0200/0313 | Loss: 0.9617
Epoch: 015/130 | Batch 0250/0313 | Loss: 0.9389
Epoch: 015/130 | Batch 0300/0313 | Loss: 0.8492
**Epoch: 015/130 | Train. Acc.: 62.475% | Loss: 1.0680
**Epoch: 015/130 | Valid. Acc.: 68.050% | Loss: 0.9342
Time elapsed: 26.62 min
Epoch: 016/130 | Current Learning Rate: 0.100000
Epoch: 016/130 | Batch 0000/0313 | Loss: 0.7619
Epoch: 016/130 | Batch 0050/0313 | Loss: 0.9001
Epoch: 016/130 | Batch 0100/0313 | Loss: 0.9218
Epoch: 016/130 | Batch 0150/0313 | Loss: 0.9784
Epoch: 016/130 | Batch 0200/0313 | Loss: 0.8919
Epoch: 016/130 | Batch 0250/0313 | Loss: 0.8935
Epoch: 016/130 | Batch 0300/0313 | Loss: 0.6600
**Epoch: 016/130 | Train. Acc.: 65.148% | Loss: 0.9709
**Epoch: 016/130 | Valid. Acc.: 69.280% | Loss: 0.8928
Time elapsed: 28.38 min
Epoch: 017/130 | Current Learning Rate: 0.100000
Epoch: 017/130 | Batch 0000/0313 | Loss: 0.9689
Epoch: 017/130 | Batch 0050/0313 | Loss: 0.9903
Epoch: 017/130 | Batch 0100/0313 | Loss: 0.7357
Epoch: 017/130 | Batch 0150/0313 | Loss: 0.9793
Epoch: 017/130 | Batch 0200/0313 | Loss: 0.8300
Epoch: 017/130 | Batch 0250/0313 | Loss: 0.7405
Epoch: 017/130 | Batch 0300/0313 | Loss: 0.7978
**Epoch: 017/130 | Train. Acc.: 64.585% | Loss: 1.0471
**Epoch: 017/130 | Valid. Acc.: 68.270% | Loss: 0.9487
Time elapsed: 30.15 min
Epoch: 018/130 | Current Learning Rate: 0.100000
Epoch: 018/130 | Batch 0000/0313 | Loss: 0.7978
Epoch: 018/130 | Batch 0050/0313 | Loss: 0.8238
Epoch: 018/130 | Batch 0100/0313 | Loss: 0.8277
Epoch: 018/130 | Batch 0150/0313 | Loss: 0.6364
Epoch: 018/130 | Batch 0200/0313 | Loss: 0.6217
Epoch: 018/130 | Batch 0250/0313 | Loss: 0.7598
Epoch: 018/130 | Batch 0300/0313 | Loss: 0.6756
**Epoch: 018/130 | Train. Acc.: 64.087% | Loss: 1.1303
**Epoch: 018/130 | Valid. Acc.: 68.660% | Loss: 1.0225
Time elapsed: 31.92 min
Epoch: 019/130 | Current Learning Rate: 0.100000
Epoch: 019/130 | Batch 0000/0313 | Loss: 0.7661
Epoch: 019/130 | Batch 0050/0313 | Loss: 0.8297
Epoch: 019/130 | Batch 0100/0313 | Loss: 0.9096
Epoch: 019/130 | Batch 0150/0313 | Loss: 0.5880
Epoch: 019/130 | Batch 0200/0313 | Loss: 0.8577
Epoch: 019/130 | Batch 0250/0313 | Loss: 0.6532
Epoch: 019/130 | Batch 0300/0313 | Loss: 0.7804
**Epoch: 019/130 | Train. Acc.: 59.420% | Loss: 1.2609
**Epoch: 019/130 | Valid. Acc.: 64.470% | Loss: 1.1717
Time elapsed: 33.69 min
Epoch: 020/130 | Current Learning Rate: 0.100000
Epoch: 020/130 | Batch 0000/0313 | Loss: 0.6720
Epoch: 020/130 | Batch 0050/0313 | Loss: 0.6982
Epoch: 020/130 | Batch 0100/0313 | Loss: 0.7458
Epoch: 020/130 | Batch 0150/0313 | Loss: 0.6452
Epoch: 020/130 | Batch 0200/0313 | Loss: 0.8131
Epoch: 020/130 | Batch 0250/0313 | Loss: 0.8080
Epoch: 020/130 | Batch 0300/0313 | Loss: 0.7962
**Epoch: 020/130 | Train. Acc.: 64.725% | Loss: 1.2413
**Epoch: 020/130 | Valid. Acc.: 65.450% | Loss: 1.4959
Time elapsed: 35.46 min
Epoch: 021/130 | Current Learning Rate: 0.100000
Epoch: 021/130 | Batch 0000/0313 | Loss: 0.6639
Epoch: 021/130 | Batch 0050/0313 | Loss: 0.7491
Epoch: 021/130 | Batch 0100/0313 | Loss: 0.8083
Epoch: 021/130 | Batch 0150/0313 | Loss: 0.8976
Epoch: 021/130 | Batch 0200/0313 | Loss: 0.6991
Epoch: 021/130 | Batch 0250/0313 | Loss: 0.6691
Epoch: 021/130 | Batch 0300/0313 | Loss: 0.7897
**Epoch: 021/130 | Train. Acc.: 69.082% | Loss: 0.8839
**Epoch: 021/130 | Valid. Acc.: 72.310% | Loss: 0.8126
**Validation loss decreased (0.880956 --> 0.812570). Saving model ...
Time elapsed: 37.24 min
Epoch: 022/130 | Current Learning Rate: 0.100000
Epoch: 022/130 | Batch 0000/0313 | Loss: 0.6649
Epoch: 022/130 | Batch 0050/0313 | Loss: 0.8703
Epoch: 022/130 | Batch 0100/0313 | Loss: 0.6192
Epoch: 022/130 | Batch 0150/0313 | Loss: 0.7446
Epoch: 022/130 | Batch 0200/0313 | Loss: 0.6149
Epoch: 022/130 | Batch 0250/0313 | Loss: 0.7218
Epoch: 022/130 | Batch 0300/0313 | Loss: 0.9348
**Epoch: 022/130 | Train. Acc.: 64.540% | Loss: 1.0694
**Epoch: 022/130 | Valid. Acc.: 67.260% | Loss: 1.0565
Time elapsed: 39.01 min
Epoch: 023/130 | Current Learning Rate: 0.100000
Epoch: 023/130 | Batch 0000/0313 | Loss: 0.8673
Epoch: 023/130 | Batch 0050/0313 | Loss: 0.8399
Epoch: 023/130 | Batch 0100/0313 | Loss: 0.7572
Epoch: 023/130 | Batch 0150/0313 | Loss: 0.6815
Epoch: 023/130 | Batch 0200/0313 | Loss: 0.7926
Epoch: 023/130 | Batch 0250/0313 | Loss: 0.7798
Epoch: 023/130 | Batch 0300/0313 | Loss: 0.5730
**Epoch: 023/130 | Train. Acc.: 60.320% | Loss: 1.2523
**Epoch: 023/130 | Valid. Acc.: 64.350% | Loss: 1.1822
Time elapsed: 40.77 min
Epoch: 024/130 | Current Learning Rate: 0.100000
Epoch: 024/130 | Batch 0000/0313 | Loss: 0.6395
Epoch: 024/130 | Batch 0050/0313 | Loss: 0.5887
Epoch: 024/130 | Batch 0100/0313 | Loss: 0.8063
Epoch: 024/130 | Batch 0150/0313 | Loss: 0.6192
Epoch: 024/130 | Batch 0200/0313 | Loss: 0.7041
Epoch: 024/130 | Batch 0250/0313 | Loss: 0.7189
Epoch: 024/130 | Batch 0300/0313 | Loss: 0.6759
**Epoch: 024/130 | Train. Acc.: 70.780% | Loss: 0.8520
**Epoch: 024/130 | Valid. Acc.: 73.680% | Loss: 0.8193
Time elapsed: 42.54 min
Epoch: 025/130 | Current Learning Rate: 0.100000
Epoch: 025/130 | Batch 0000/0313 | Loss: 0.7515
Epoch: 025/130 | Batch 0050/0313 | Loss: 0.6412
Epoch: 025/130 | Batch 0100/0313 | Loss: 0.5383
Epoch: 025/130 | Batch 0150/0313 | Loss: 0.8317
Epoch: 025/130 | Batch 0200/0313 | Loss: 0.6829
Epoch: 025/130 | Batch 0250/0313 | Loss: 0.6709
Epoch: 025/130 | Batch 0300/0313 | Loss: 0.7064
**Epoch: 025/130 | Train. Acc.: 66.040% | Loss: 1.0348
**Epoch: 025/130 | Valid. Acc.: 72.520% | Loss: 0.8737
Time elapsed: 44.31 min
Epoch: 026/130 | Current Learning Rate: 0.100000
Epoch: 026/130 | Batch 0000/0313 | Loss: 0.7907
Epoch: 026/130 | Batch 0050/0313 | Loss: 0.6331
Epoch: 026/130 | Batch 0100/0313 | Loss: 0.8019
Epoch: 026/130 | Batch 0150/0313 | Loss: 0.6154
Epoch: 026/130 | Batch 0200/0313 | Loss: 0.8532
Epoch: 026/130 | Batch 0250/0313 | Loss: 0.6549
Epoch: 026/130 | Batch 0300/0313 | Loss: 0.6573
**Epoch: 026/130 | Train. Acc.: 73.765% | Loss: 0.7460
**Epoch: 026/130 | Valid. Acc.: 77.420% | Loss: 0.6632
**Validation loss decreased (0.812570 --> 0.663239). Saving model ...
Time elapsed: 46.08 min
Epoch: 027/130 | Current Learning Rate: 0.100000
Epoch: 027/130 | Batch 0000/0313 | Loss: 0.6779
Epoch: 027/130 | Batch 0050/0313 | Loss: 0.6622
Epoch: 027/130 | Batch 0100/0313 | Loss: 0.6730
Epoch: 027/130 | Batch 0150/0313 | Loss: 0.7369
Epoch: 027/130 | Batch 0200/0313 | Loss: 0.6101
Epoch: 027/130 | Batch 0250/0313 | Loss: 0.8741
Epoch: 027/130 | Batch 0300/0313 | Loss: 0.7808
**Epoch: 027/130 | Train. Acc.: 66.555% | Loss: 1.0702
**Epoch: 027/130 | Valid. Acc.: 72.350% | Loss: 0.9248
Time elapsed: 47.85 min
Epoch: 028/130 | Current Learning Rate: 0.100000
Epoch: 028/130 | Batch 0000/0313 | Loss: 0.6483
Epoch: 028/130 | Batch 0050/0313 | Loss: 0.6955
Epoch: 028/130 | Batch 0100/0313 | Loss: 0.7349
Epoch: 028/130 | Batch 0150/0313 | Loss: 0.7227
Epoch: 028/130 | Batch 0200/0313 | Loss: 0.6490
Epoch: 028/130 | Batch 0250/0313 | Loss: 0.6490
Epoch: 028/130 | Batch 0300/0313 | Loss: 0.6421
**Epoch: 028/130 | Train. Acc.: 72.820% | Loss: 0.7883
**Epoch: 028/130 | Valid. Acc.: 76.370% | Loss: 0.6974
Time elapsed: 49.61 min
Epoch: 029/130 | Current Learning Rate: 0.100000
Epoch: 029/130 | Batch 0000/0313 | Loss: 0.6676
Epoch: 029/130 | Batch 0050/0313 | Loss: 0.7802
Epoch: 029/130 | Batch 0100/0313 | Loss: 0.6429
Epoch: 029/130 | Batch 0150/0313 | Loss: 0.7120
Epoch: 029/130 | Batch 0200/0313 | Loss: 0.7326
Epoch: 029/130 | Batch 0250/0313 | Loss: 0.5915
Epoch: 029/130 | Batch 0300/0313 | Loss: 0.6534
**Epoch: 029/130 | Train. Acc.: 73.730% | Loss: 0.7894
**Epoch: 029/130 | Valid. Acc.: 77.030% | Loss: 0.7039
Time elapsed: 51.38 min
Epoch: 030/130 | Current Learning Rate: 0.100000
Epoch: 030/130 | Batch 0000/0313 | Loss: 0.5799
Epoch: 030/130 | Batch 0050/0313 | Loss: 0.6558
Epoch: 030/130 | Batch 0100/0313 | Loss: 0.6197
Epoch: 030/130 | Batch 0150/0313 | Loss: 0.7073
Epoch: 030/130 | Batch 0200/0313 | Loss: 0.5734
Epoch: 030/130 | Batch 0250/0313 | Loss: 0.8411
Epoch: 030/130 | Batch 0300/0313 | Loss: 0.6436
**Epoch: 030/130 | Train. Acc.: 72.215% | Loss: 0.7999
**Epoch: 030/130 | Valid. Acc.: 75.790% | Loss: 0.7233
Time elapsed: 53.15 min
Epoch: 031/130 | Current Learning Rate: 0.100000
Epoch: 031/130 | Batch 0000/0313 | Loss: 0.7031
Epoch: 031/130 | Batch 0050/0313 | Loss: 0.7760
Epoch: 031/130 | Batch 0100/0313 | Loss: 0.7618
Epoch: 031/130 | Batch 0150/0313 | Loss: 0.6962
Epoch: 031/130 | Batch 0200/0313 | Loss: 0.6851
Epoch: 031/130 | Batch 0250/0313 | Loss: 0.6243
Epoch: 031/130 | Batch 0300/0313 | Loss: 0.5430
**Epoch: 031/130 | Train. Acc.: 65.407% | Loss: 1.0685
**Epoch: 031/130 | Valid. Acc.: 69.660% | Loss: 0.9602
Time elapsed: 54.91 min
Epoch: 032/130 | Current Learning Rate: 0.100000
Epoch: 032/130 | Batch 0000/0313 | Loss: 0.7003
Epoch: 032/130 | Batch 0050/0313 | Loss: 0.5793
Epoch: 032/130 | Batch 0100/0313 | Loss: 0.5849
Epoch: 032/130 | Batch 0150/0313 | Loss: 0.6491
Epoch: 032/130 | Batch 0200/0313 | Loss: 0.7545
Epoch: 032/130 | Batch 0250/0313 | Loss: 0.4984
Epoch: 032/130 | Batch 0300/0313 | Loss: 0.6097
**Epoch: 032/130 | Train. Acc.: 75.902% | Loss: 0.6970
**Epoch: 032/130 | Valid. Acc.: 77.130% | Loss: 0.6886
Time elapsed: 56.68 min
Epoch: 033/130 | Current Learning Rate: 0.100000
Epoch: 033/130 | Batch 0000/0313 | Loss: 0.6876
Epoch: 033/130 | Batch 0050/0313 | Loss: 0.5711
Epoch: 033/130 | Batch 0100/0313 | Loss: 0.5057
Epoch: 033/130 | Batch 0150/0313 | Loss: 0.5919
Epoch: 033/130 | Batch 0200/0313 | Loss: 0.8617
Epoch: 033/130 | Batch 0250/0313 | Loss: 0.6069
Epoch: 033/130 | Batch 0300/0313 | Loss: 0.7207
**Epoch: 033/130 | Train. Acc.: 71.945% | Loss: 0.8466
**Epoch: 033/130 | Valid. Acc.: 74.680% | Loss: 0.7987
Epoch 00033: reducing learning rate of group 0 to 5.0000e-02.
Time elapsed: 58.44 min
Epoch: 034/130 | Current Learning Rate: 0.050000
Epoch: 034/130 | Batch 0000/0313 | Loss: 0.5521
Epoch: 034/130 | Batch 0050/0313 | Loss: 0.5467
Epoch: 034/130 | Batch 0100/0313 | Loss: 0.4728
Epoch: 034/130 | Batch 0150/0313 | Loss: 0.4712
Epoch: 034/130 | Batch 0200/0313 | Loss: 0.4115
Epoch: 034/130 | Batch 0250/0313 | Loss: 0.4561
Epoch: 034/130 | Batch 0300/0313 | Loss: 0.5030
**Epoch: 034/130 | Train. Acc.: 81.593% | Loss: 0.5303
**Epoch: 034/130 | Valid. Acc.: 82.910% | Loss: 0.5119
**Validation loss decreased (0.663239 --> 0.511874). Saving model ...
Time elapsed: 60.21 min
Epoch: 035/130 | Current Learning Rate: 0.050000
Epoch: 035/130 | Batch 0000/0313 | Loss: 0.4290
Epoch: 035/130 | Batch 0050/0313 | Loss: 0.4033
Epoch: 035/130 | Batch 0100/0313 | Loss: 0.5508
Epoch: 035/130 | Batch 0150/0313 | Loss: 0.5186
Epoch: 035/130 | Batch 0200/0313 | Loss: 0.4845
Epoch: 035/130 | Batch 0250/0313 | Loss: 0.4398
Epoch: 035/130 | Batch 0300/0313 | Loss: 0.5341
**Epoch: 035/130 | Train. Acc.: 80.883% | Loss: 0.5558
**Epoch: 035/130 | Valid. Acc.: 82.410% | Loss: 0.5320
Time elapsed: 61.98 min
Epoch: 036/130 | Current Learning Rate: 0.050000
Epoch: 036/130 | Batch 0000/0313 | Loss: 0.5681
Epoch: 036/130 | Batch 0050/0313 | Loss: 0.4205
Epoch: 036/130 | Batch 0100/0313 | Loss: 0.5454
Epoch: 036/130 | Batch 0150/0313 | Loss: 0.4289
Epoch: 036/130 | Batch 0200/0313 | Loss: 0.3031
Epoch: 036/130 | Batch 0250/0313 | Loss: 0.6326
Epoch: 036/130 | Batch 0300/0313 | Loss: 0.5777
**Epoch: 036/130 | Train. Acc.: 80.258% | Loss: 0.5734
**Epoch: 036/130 | Valid. Acc.: 81.510% | Loss: 0.5566
Time elapsed: 63.75 min
Epoch: 037/130 | Current Learning Rate: 0.050000
Epoch: 037/130 | Batch 0000/0313 | Loss: 0.3684
Epoch: 037/130 | Batch 0050/0313 | Loss: 0.4315
Epoch: 037/130 | Batch 0100/0313 | Loss: 0.5470
Epoch: 037/130 | Batch 0150/0313 | Loss: 0.6444
Epoch: 037/130 | Batch 0200/0313 | Loss: 0.4255
Epoch: 037/130 | Batch 0250/0313 | Loss: 0.4775
Epoch: 037/130 | Batch 0300/0313 | Loss: 0.6185
**Epoch: 037/130 | Train. Acc.: 76.465% | Loss: 0.6940
**Epoch: 037/130 | Valid. Acc.: 79.490% | Loss: 0.6383
Time elapsed: 65.51 min
Epoch: 038/130 | Current Learning Rate: 0.050000
Epoch: 038/130 | Batch 0000/0313 | Loss: 0.5108
Epoch: 038/130 | Batch 0050/0313 | Loss: 0.4552
Epoch: 038/130 | Batch 0100/0313 | Loss: 0.4193
Epoch: 038/130 | Batch 0150/0313 | Loss: 0.5315
Epoch: 038/130 | Batch 0200/0313 | Loss: 0.5056
Epoch: 038/130 | Batch 0250/0313 | Loss: 0.4805
Epoch: 038/130 | Batch 0300/0313 | Loss: 0.4207
**Epoch: 038/130 | Train. Acc.: 84.665% | Loss: 0.4477
**Epoch: 038/130 | Valid. Acc.: 85.240% | Loss: 0.4275
**Validation loss decreased (0.511874 --> 0.427525). Saving model ...
Time elapsed: 67.29 min
Epoch: 039/130 | Current Learning Rate: 0.050000
Epoch: 039/130 | Batch 0000/0313 | Loss: 0.3930
Epoch: 039/130 | Batch 0050/0313 | Loss: 0.4472
Epoch: 039/130 | Batch 0100/0313 | Loss: 0.4899
Epoch: 039/130 | Batch 0150/0313 | Loss: 0.4354
Epoch: 039/130 | Batch 0200/0313 | Loss: 0.4832
Epoch: 039/130 | Batch 0250/0313 | Loss: 0.4636
Epoch: 039/130 | Batch 0300/0313 | Loss: 0.3697
**Epoch: 039/130 | Train. Acc.: 78.790% | Loss: 0.6169
**Epoch: 039/130 | Valid. Acc.: 79.760% | Loss: 0.6375
Time elapsed: 69.06 min
Epoch: 040/130 | Current Learning Rate: 0.050000
Epoch: 040/130 | Batch 0000/0313 | Loss: 0.4134
Epoch: 040/130 | Batch 0050/0313 | Loss: 0.3212
Epoch: 040/130 | Batch 0100/0313 | Loss: 0.3935
Epoch: 040/130 | Batch 0150/0313 | Loss: 0.4019
Epoch: 040/130 | Batch 0200/0313 | Loss: 0.5167
Epoch: 040/130 | Batch 0250/0313 | Loss: 0.4893
Epoch: 040/130 | Batch 0300/0313 | Loss: 0.5098
**Epoch: 040/130 | Train. Acc.: 81.183% | Loss: 0.5438
**Epoch: 040/130 | Valid. Acc.: 83.780% | Loss: 0.4804
Time elapsed: 70.83 min
Epoch: 041/130 | Current Learning Rate: 0.050000
Epoch: 041/130 | Batch 0000/0313 | Loss: 0.4310
Epoch: 041/130 | Batch 0050/0313 | Loss: 0.3845
Epoch: 041/130 | Batch 0100/0313 | Loss: 0.4708
Epoch: 041/130 | Batch 0150/0313 | Loss: 0.3861
Epoch: 041/130 | Batch 0200/0313 | Loss: 0.4106
Epoch: 041/130 | Batch 0250/0313 | Loss: 0.4873
Epoch: 041/130 | Batch 0300/0313 | Loss: 0.4024
**Epoch: 041/130 | Train. Acc.: 78.668% | Loss: 0.6248
**Epoch: 041/130 | Valid. Acc.: 81.550% | Loss: 0.5411
Time elapsed: 72.60 min
Epoch: 042/130 | Current Learning Rate: 0.050000
Epoch: 042/130 | Batch 0000/0313 | Loss: 0.3881
Epoch: 042/130 | Batch 0050/0313 | Loss: 0.4128
Epoch: 042/130 | Batch 0100/0313 | Loss: 0.4647
Epoch: 042/130 | Batch 0150/0313 | Loss: 0.7809
Epoch: 042/130 | Batch 0200/0313 | Loss: 0.4137
Epoch: 042/130 | Batch 0250/0313 | Loss: 0.4708
Epoch: 042/130 | Batch 0300/0313 | Loss: 0.6315
**Epoch: 042/130 | Train. Acc.: 77.840% | Loss: 0.6792
**Epoch: 042/130 | Valid. Acc.: 80.370% | Loss: 0.6355
Time elapsed: 74.37 min
Epoch: 043/130 | Current Learning Rate: 0.050000
Epoch: 043/130 | Batch 0000/0313 | Loss: 0.6410
Epoch: 043/130 | Batch 0050/0313 | Loss: 0.3480
Epoch: 043/130 | Batch 0100/0313 | Loss: 0.4594
Epoch: 043/130 | Batch 0150/0313 | Loss: 0.3921
Epoch: 043/130 | Batch 0200/0313 | Loss: 0.4834
Epoch: 043/130 | Batch 0250/0313 | Loss: 0.6331
Epoch: 043/130 | Batch 0300/0313 | Loss: 0.4111
**Epoch: 043/130 | Train. Acc.: 83.017% | Loss: 0.4837
**Epoch: 043/130 | Valid. Acc.: 83.520% | Loss: 0.5003
Time elapsed: 76.15 min
Epoch: 044/130 | Current Learning Rate: 0.050000
Epoch: 044/130 | Batch 0000/0313 | Loss: 0.3037
Epoch: 044/130 | Batch 0050/0313 | Loss: 0.5606
Epoch: 044/130 | Batch 0100/0313 | Loss: 0.5443
Epoch: 044/130 | Batch 0150/0313 | Loss: 0.5310
Epoch: 044/130 | Batch 0200/0313 | Loss: 0.4391
Epoch: 044/130 | Batch 0250/0313 | Loss: 0.5625
Epoch: 044/130 | Batch 0300/0313 | Loss: 0.6685
**Epoch: 044/130 | Train. Acc.: 80.595% | Loss: 0.5678
**Epoch: 044/130 | Valid. Acc.: 81.000% | Loss: 0.5675
Time elapsed: 77.91 min
Epoch: 045/130 | Current Learning Rate: 0.050000
Epoch: 045/130 | Batch 0000/0313 | Loss: 0.5118
Epoch: 045/130 | Batch 0050/0313 | Loss: 0.5474
Epoch: 045/130 | Batch 0100/0313 | Loss: 0.3918
Epoch: 045/130 | Batch 0150/0313 | Loss: 0.3694
Epoch: 045/130 | Batch 0200/0313 | Loss: 0.5283
Epoch: 045/130 | Batch 0250/0313 | Loss: 0.4189
Epoch: 045/130 | Batch 0300/0313 | Loss: 0.5633
**Epoch: 045/130 | Train. Acc.: 81.837% | Loss: 0.5247
**Epoch: 045/130 | Valid. Acc.: 82.990% | Loss: 0.5031
Epoch 00045: reducing learning rate of group 0 to 2.5000e-02.
Time elapsed: 79.68 min
Epoch: 046/130 | Current Learning Rate: 0.025000
Epoch: 046/130 | Batch 0000/0313 | Loss: 0.3965
Epoch: 046/130 | Batch 0050/0313 | Loss: 0.2574
Epoch: 046/130 | Batch 0100/0313 | Loss: 0.2554
Epoch: 046/130 | Batch 0150/0313 | Loss: 0.2189
Epoch: 046/130 | Batch 0200/0313 | Loss: 0.3660
Epoch: 046/130 | Batch 0250/0313 | Loss: 0.2920
Epoch: 046/130 | Batch 0300/0313 | Loss: 0.3969
**Epoch: 046/130 | Train. Acc.: 85.230% | Loss: 0.4248
**Epoch: 046/130 | Valid. Acc.: 85.570% | Loss: 0.4475
Time elapsed: 81.44 min
Epoch: 047/130 | Current Learning Rate: 0.025000
Epoch: 047/130 | Batch 0000/0313 | Loss: 0.2875
Epoch: 047/130 | Batch 0050/0313 | Loss: 0.2611
Epoch: 047/130 | Batch 0100/0313 | Loss: 0.3226
Epoch: 047/130 | Batch 0150/0313 | Loss: 0.2778
Epoch: 047/130 | Batch 0200/0313 | Loss: 0.4512
Epoch: 047/130 | Batch 0250/0313 | Loss: 0.3531
Epoch: 047/130 | Batch 0300/0313 | Loss: 0.2218
**Epoch: 047/130 | Train. Acc.: 88.942% | Loss: 0.3220
**Epoch: 047/130 | Valid. Acc.: 88.380% | Loss: 0.3570
**Validation loss decreased (0.427525 --> 0.357047). Saving model ...
Time elapsed: 83.22 min
Epoch: 048/130 | Current Learning Rate: 0.025000
Epoch: 048/130 | Batch 0000/0313 | Loss: 0.4065
Epoch: 048/130 | Batch 0050/0313 | Loss: 0.2562
Epoch: 048/130 | Batch 0100/0313 | Loss: 0.4558
Epoch: 048/130 | Batch 0150/0313 | Loss: 0.3011
Epoch: 048/130 | Batch 0200/0313 | Loss: 0.4339
Epoch: 048/130 | Batch 0250/0313 | Loss: 0.3189
Epoch: 048/130 | Batch 0300/0313 | Loss: 0.3147
**Epoch: 048/130 | Train. Acc.: 87.672% | Loss: 0.3545
**Epoch: 048/130 | Valid. Acc.: 87.770% | Loss: 0.3691
Time elapsed: 84.98 min
Epoch: 049/130 | Current Learning Rate: 0.025000
Epoch: 049/130 | Batch 0000/0313 | Loss: 0.2318
Epoch: 049/130 | Batch 0050/0313 | Loss: 0.3010
Epoch: 049/130 | Batch 0100/0313 | Loss: 0.2734
Epoch: 049/130 | Batch 0150/0313 | Loss: 0.2539
Epoch: 049/130 | Batch 0200/0313 | Loss: 0.2665
Epoch: 049/130 | Batch 0250/0313 | Loss: 0.3648
Epoch: 049/130 | Batch 0300/0313 | Loss: 0.3938
**Epoch: 049/130 | Train. Acc.: 88.287% | Loss: 0.3419
**Epoch: 049/130 | Valid. Acc.: 87.930% | Loss: 0.3609
Time elapsed: 86.75 min
Epoch: 050/130 | Current Learning Rate: 0.025000
Epoch: 050/130 | Batch 0000/0313 | Loss: 0.3543
Epoch: 050/130 | Batch 0050/0313 | Loss: 0.3383
Epoch: 050/130 | Batch 0100/0313 | Loss: 0.2697
Epoch: 050/130 | Batch 0150/0313 | Loss: 0.2517
Epoch: 050/130 | Batch 0200/0313 | Loss: 0.1685
Epoch: 050/130 | Batch 0250/0313 | Loss: 0.3327
Epoch: 050/130 | Batch 0300/0313 | Loss: 0.3238
**Epoch: 050/130 | Train. Acc.: 85.840% | Loss: 0.4042
**Epoch: 050/130 | Valid. Acc.: 85.670% | Loss: 0.4581
Time elapsed: 88.52 min
Epoch: 051/130 | Current Learning Rate: 0.025000
Epoch: 051/130 | Batch 0000/0313 | Loss: 0.2599
Epoch: 051/130 | Batch 0050/0313 | Loss: 0.3540
Epoch: 051/130 | Batch 0100/0313 | Loss: 0.2600
Epoch: 051/130 | Batch 0150/0313 | Loss: 0.3060
Epoch: 051/130 | Batch 0200/0313 | Loss: 0.3316
Epoch: 051/130 | Batch 0250/0313 | Loss: 0.3762
Epoch: 051/130 | Batch 0300/0313 | Loss: 0.2446
**Epoch: 051/130 | Train. Acc.: 88.415% | Loss: 0.3340
**Epoch: 051/130 | Valid. Acc.: 88.220% | Loss: 0.3640
Time elapsed: 90.28 min
Epoch: 052/130 | Current Learning Rate: 0.025000
Epoch: 052/130 | Batch 0000/0313 | Loss: 0.3648
Epoch: 052/130 | Batch 0050/0313 | Loss: 0.3342
Epoch: 052/130 | Batch 0100/0313 | Loss: 0.3628
Epoch: 052/130 | Batch 0150/0313 | Loss: 0.3916
Epoch: 052/130 | Batch 0200/0313 | Loss: 0.3169
Epoch: 052/130 | Batch 0250/0313 | Loss: 0.3946
Epoch: 052/130 | Batch 0300/0313 | Loss: 0.2786
**Epoch: 052/130 | Train. Acc.: 88.695% | Loss: 0.3293
**Epoch: 052/130 | Valid. Acc.: 88.650% | Loss: 0.3466
**Validation loss decreased (0.357047 --> 0.346650). Saving model ...
Time elapsed: 92.06 min
Epoch: 053/130 | Current Learning Rate: 0.025000
Epoch: 053/130 | Batch 0000/0313 | Loss: 0.3157
Epoch: 053/130 | Batch 0050/0313 | Loss: 0.2752
Epoch: 053/130 | Batch 0100/0313 | Loss: 0.2883
Epoch: 053/130 | Batch 0150/0313 | Loss: 0.3830
Epoch: 053/130 | Batch 0200/0313 | Loss: 0.3817
Epoch: 053/130 | Batch 0250/0313 | Loss: 0.2694
Epoch: 053/130 | Batch 0300/0313 | Loss: 0.2790
**Epoch: 053/130 | Train. Acc.: 87.218% | Loss: 0.3711
**Epoch: 053/130 | Valid. Acc.: 87.190% | Loss: 0.3898
Time elapsed: 93.82 min
Epoch: 054/130 | Current Learning Rate: 0.025000
Epoch: 054/130 | Batch 0000/0313 | Loss: 0.3242
Epoch: 054/130 | Batch 0050/0313 | Loss: 0.2881
Epoch: 054/130 | Batch 0100/0313 | Loss: 0.2721
Epoch: 054/130 | Batch 0150/0313 | Loss: 0.2046
Epoch: 054/130 | Batch 0200/0313 | Loss: 0.2711
Epoch: 054/130 | Batch 0250/0313 | Loss: 0.4302
Epoch: 054/130 | Batch 0300/0313 | Loss: 0.2790
**Epoch: 054/130 | Train. Acc.: 87.460% | Loss: 0.3686
**Epoch: 054/130 | Valid. Acc.: 88.100% | Loss: 0.3582
Time elapsed: 95.59 min
Epoch: 055/130 | Current Learning Rate: 0.025000
Epoch: 055/130 | Batch 0000/0313 | Loss: 0.2627
Epoch: 055/130 | Batch 0050/0313 | Loss: 0.3016
Epoch: 055/130 | Batch 0100/0313 | Loss: 0.3117
Epoch: 055/130 | Batch 0150/0313 | Loss: 0.4660
Epoch: 055/130 | Batch 0200/0313 | Loss: 0.2561
Epoch: 055/130 | Batch 0250/0313 | Loss: 0.4975
Epoch: 055/130 | Batch 0300/0313 | Loss: 0.2670
**Epoch: 055/130 | Train. Acc.: 88.528% | Loss: 0.3297
**Epoch: 055/130 | Valid. Acc.: 88.070% | Loss: 0.3593
Time elapsed: 97.35 min
Epoch: 056/130 | Current Learning Rate: 0.025000
Epoch: 056/130 | Batch 0000/0313 | Loss: 0.2799
Epoch: 056/130 | Batch 0050/0313 | Loss: 0.2723
Epoch: 056/130 | Batch 0100/0313 | Loss: 0.3359
Epoch: 056/130 | Batch 0150/0313 | Loss: 0.2269
Epoch: 056/130 | Batch 0200/0313 | Loss: 0.3940
Epoch: 056/130 | Batch 0250/0313 | Loss: 0.3042
Epoch: 056/130 | Batch 0300/0313 | Loss: 0.2884
**Epoch: 056/130 | Train. Acc.: 88.392% | Loss: 0.3313
**Epoch: 056/130 | Valid. Acc.: 88.610% | Loss: 0.3544
Time elapsed: 99.12 min
Epoch: 057/130 | Current Learning Rate: 0.025000
Epoch: 057/130 | Batch 0000/0313 | Loss: 0.3189
Epoch: 057/130 | Batch 0050/0313 | Loss: 0.3133
Epoch: 057/130 | Batch 0100/0313 | Loss: 0.3711
Epoch: 057/130 | Batch 0150/0313 | Loss: 0.3105
Epoch: 057/130 | Batch 0200/0313 | Loss: 0.2877
Epoch: 057/130 | Batch 0250/0313 | Loss: 0.3199
Epoch: 057/130 | Batch 0300/0313 | Loss: 0.2817
**Epoch: 057/130 | Train. Acc.: 88.263% | Loss: 0.3321
**Epoch: 057/130 | Valid. Acc.: 88.170% | Loss: 0.3519
Time elapsed: 100.88 min
Epoch: 058/130 | Current Learning Rate: 0.025000
Epoch: 058/130 | Batch 0000/0313 | Loss: 0.4455
Epoch: 058/130 | Batch 0050/0313 | Loss: 0.2474
Epoch: 058/130 | Batch 0100/0313 | Loss: 0.2519
Epoch: 058/130 | Batch 0150/0313 | Loss: 0.3343
Epoch: 058/130 | Batch 0200/0313 | Loss: 0.2754
Epoch: 058/130 | Batch 0250/0313 | Loss: 0.3232
Epoch: 058/130 | Batch 0300/0313 | Loss: 0.3582
**Epoch: 058/130 | Train. Acc.: 87.385% | Loss: 0.3682
**Epoch: 058/130 | Valid. Acc.: 87.570% | Loss: 0.3840
Time elapsed: 102.65 min
Epoch: 059/130 | Current Learning Rate: 0.025000
Epoch: 059/130 | Batch 0000/0313 | Loss: 0.2655
Epoch: 059/130 | Batch 0050/0313 | Loss: 0.1948
Epoch: 059/130 | Batch 0100/0313 | Loss: 0.3241
Epoch: 059/130 | Batch 0150/0313 | Loss: 0.3390
Epoch: 059/130 | Batch 0200/0313 | Loss: 0.3496
Epoch: 059/130 | Batch 0250/0313 | Loss: 0.3144
Epoch: 059/130 | Batch 0300/0313 | Loss: 0.3423
**Epoch: 059/130 | Train. Acc.: 86.690% | Loss: 0.3812
**Epoch: 059/130 | Valid. Acc.: 86.170% | Loss: 0.4267
Epoch 00059: reducing learning rate of group 0 to 1.2500e-02.
Time elapsed: 104.41 min
Epoch: 060/130 | Current Learning Rate: 0.012500
Epoch: 060/130 | Batch 0000/0313 | Loss: 0.3143
Epoch: 060/130 | Batch 0050/0313 | Loss: 0.2478
Epoch: 060/130 | Batch 0100/0313 | Loss: 0.2217
Epoch: 060/130 | Batch 0150/0313 | Loss: 0.1804
Epoch: 060/130 | Batch 0200/0313 | Loss: 0.2450
Epoch: 060/130 | Batch 0250/0313 | Loss: 0.3518
Epoch: 060/130 | Batch 0300/0313 | Loss: 0.2363
**Epoch: 060/130 | Train. Acc.: 93.392% | Loss: 0.1917
**Epoch: 060/130 | Valid. Acc.: 91.620% | Loss: 0.2614
**Validation loss decreased (0.346650 --> 0.261411). Saving model ...
Time elapsed: 106.20 min
Epoch: 061/130 | Current Learning Rate: 0.012500
Epoch: 061/130 | Batch 0000/0313 | Loss: 0.1687
Epoch: 061/130 | Batch 0050/0313 | Loss: 0.2633
Epoch: 061/130 | Batch 0100/0313 | Loss: 0.2324
Epoch: 061/130 | Batch 0150/0313 | Loss: 0.1812
Epoch: 061/130 | Batch 0200/0313 | Loss: 0.1218
Epoch: 061/130 | Batch 0250/0313 | Loss: 0.1417
Epoch: 061/130 | Batch 0300/0313 | Loss: 0.1999
**Epoch: 061/130 | Train. Acc.: 92.802% | Loss: 0.2023
**Epoch: 061/130 | Valid. Acc.: 90.680% | Loss: 0.2857
Time elapsed: 107.97 min
Epoch: 062/130 | Current Learning Rate: 0.012500
Epoch: 062/130 | Batch 0000/0313 | Loss: 0.2342
Epoch: 062/130 | Batch 0050/0313 | Loss: 0.1399
Epoch: 062/130 | Batch 0100/0313 | Loss: 0.1409
Epoch: 062/130 | Batch 0150/0313 | Loss: 0.2848
Epoch: 062/130 | Batch 0200/0313 | Loss: 0.1405
Epoch: 062/130 | Batch 0250/0313 | Loss: 0.2876
Epoch: 062/130 | Batch 0300/0313 | Loss: 0.1713
**Epoch: 062/130 | Train. Acc.: 93.705% | Loss: 0.1826
**Epoch: 062/130 | Valid. Acc.: 91.840% | Loss: 0.2494
**Validation loss decreased (0.261411 --> 0.249364). Saving model ...
Time elapsed: 109.75 min
Epoch: 063/130 | Current Learning Rate: 0.012500
Epoch: 063/130 | Batch 0000/0313 | Loss: 0.3032
Epoch: 063/130 | Batch 0050/0313 | Loss: 0.1928
Epoch: 063/130 | Batch 0100/0313 | Loss: 0.2126
Epoch: 063/130 | Batch 0150/0313 | Loss: 0.1174
Epoch: 063/130 | Batch 0200/0313 | Loss: 0.1310
Epoch: 063/130 | Batch 0250/0313 | Loss: 0.2186
Epoch: 063/130 | Batch 0300/0313 | Loss: 0.1866
**Epoch: 063/130 | Train. Acc.: 92.680% | Loss: 0.2086
**Epoch: 063/130 | Valid. Acc.: 90.540% | Loss: 0.2925
Time elapsed: 111.52 min
Epoch: 064/130 | Current Learning Rate: 0.012500
Epoch: 064/130 | Batch 0000/0313 | Loss: 0.1397
Epoch: 064/130 | Batch 0050/0313 | Loss: 0.1694
Epoch: 064/130 | Batch 0100/0313 | Loss: 0.1832
Epoch: 064/130 | Batch 0150/0313 | Loss: 0.0837
Epoch: 064/130 | Batch 0200/0313 | Loss: 0.1682
Epoch: 064/130 | Batch 0250/0313 | Loss: 0.3730
Epoch: 064/130 | Batch 0300/0313 | Loss: 0.1877
**Epoch: 064/130 | Train. Acc.: 93.377% | Loss: 0.1896
**Epoch: 064/130 | Valid. Acc.: 91.010% | Loss: 0.2822
Time elapsed: 113.29 min
Epoch: 065/130 | Current Learning Rate: 0.012500
Epoch: 065/130 | Batch 0000/0313 | Loss: 0.1408
Epoch: 065/130 | Batch 0050/0313 | Loss: 0.2543
Epoch: 065/130 | Batch 0100/0313 | Loss: 0.2309
Epoch: 065/130 | Batch 0150/0313 | Loss: 0.2852
Epoch: 065/130 | Batch 0200/0313 | Loss: 0.1788
Epoch: 065/130 | Batch 0250/0313 | Loss: 0.1727
Epoch: 065/130 | Batch 0300/0313 | Loss: 0.2554
**Epoch: 065/130 | Train. Acc.: 91.375% | Loss: 0.2449
**Epoch: 065/130 | Valid. Acc.: 89.260% | Loss: 0.3424
Time elapsed: 115.06 min
Epoch: 066/130 | Current Learning Rate: 0.012500
Epoch: 066/130 | Batch 0000/0313 | Loss: 0.1371
Epoch: 066/130 | Batch 0050/0313 | Loss: 0.1391
Epoch: 066/130 | Batch 0100/0313 | Loss: 0.1635
Epoch: 066/130 | Batch 0150/0313 | Loss: 0.1622
Epoch: 066/130 | Batch 0200/0313 | Loss: 0.2656
Epoch: 066/130 | Batch 0250/0313 | Loss: 0.3294
Epoch: 066/130 | Batch 0300/0313 | Loss: 0.2354
**Epoch: 066/130 | Train. Acc.: 93.210% | Loss: 0.1955
**Epoch: 066/130 | Valid. Acc.: 90.270% | Loss: 0.2907
Time elapsed: 116.82 min
Epoch: 067/130 | Current Learning Rate: 0.012500
Epoch: 067/130 | Batch 0000/0313 | Loss: 0.1945
Epoch: 067/130 | Batch 0050/0313 | Loss: 0.2582
Epoch: 067/130 | Batch 0100/0313 | Loss: 0.1448
Epoch: 067/130 | Batch 0150/0313 | Loss: 0.1432
Epoch: 067/130 | Batch 0200/0313 | Loss: 0.1420
Epoch: 067/130 | Batch 0250/0313 | Loss: 0.1033
Epoch: 067/130 | Batch 0300/0313 | Loss: 0.1629
**Epoch: 067/130 | Train. Acc.: 92.515% | Loss: 0.2092
**Epoch: 067/130 | Valid. Acc.: 90.860% | Loss: 0.2980
Time elapsed: 118.59 min
Epoch: 068/130 | Current Learning Rate: 0.012500
Epoch: 068/130 | Batch 0000/0313 | Loss: 0.1288
Epoch: 068/130 | Batch 0050/0313 | Loss: 0.1678
Epoch: 068/130 | Batch 0100/0313 | Loss: 0.2559
Epoch: 068/130 | Batch 0150/0313 | Loss: 0.1664
Epoch: 068/130 | Batch 0200/0313 | Loss: 0.1759
Epoch: 068/130 | Batch 0250/0313 | Loss: 0.2000
Epoch: 068/130 | Batch 0300/0313 | Loss: 0.1439
**Epoch: 068/130 | Train. Acc.: 93.470% | Loss: 0.1879
**Epoch: 068/130 | Valid. Acc.: 90.900% | Loss: 0.2860
Time elapsed: 120.35 min
Epoch: 069/130 | Current Learning Rate: 0.012500
Epoch: 069/130 | Batch 0000/0313 | Loss: 0.1930
Epoch: 069/130 | Batch 0050/0313 | Loss: 0.1790
Epoch: 069/130 | Batch 0100/0313 | Loss: 0.1641
Epoch: 069/130 | Batch 0150/0313 | Loss: 0.1820
Epoch: 069/130 | Batch 0200/0313 | Loss: 0.1870
Epoch: 069/130 | Batch 0250/0313 | Loss: 0.2620
Epoch: 069/130 | Batch 0300/0313 | Loss: 0.2561
**Epoch: 069/130 | Train. Acc.: 92.965% | Loss: 0.2010
**Epoch: 069/130 | Valid. Acc.: 90.470% | Loss: 0.2918
Epoch 00069: reducing learning rate of group 0 to 6.2500e-03.
Time elapsed: 122.12 min
Epoch: 070/130 | Current Learning Rate: 0.006250
Epoch: 070/130 | Batch 0000/0313 | Loss: 0.1171
Epoch: 070/130 | Batch 0050/0313 | Loss: 0.1051
Epoch: 070/130 | Batch 0100/0313 | Loss: 0.0655
Epoch: 070/130 | Batch 0150/0313 | Loss: 0.2263
Epoch: 070/130 | Batch 0200/0313 | Loss: 0.1584
Epoch: 070/130 | Batch 0250/0313 | Loss: 0.1386
Epoch: 070/130 | Batch 0300/0313 | Loss: 0.0989
**Epoch: 070/130 | Train. Acc.: 96.198% | Loss: 0.1127
**Epoch: 070/130 | Valid. Acc.: 92.360% | Loss: 0.2429
**Validation loss decreased (0.249364 --> 0.242912). Saving model ...
Time elapsed: 123.90 min
Epoch: 071/130 | Current Learning Rate: 0.006250
Epoch: 071/130 | Batch 0000/0313 | Loss: 0.0825
Epoch: 071/130 | Batch 0050/0313 | Loss: 0.0603
Epoch: 071/130 | Batch 0100/0313 | Loss: 0.0807
Epoch: 071/130 | Batch 0150/0313 | Loss: 0.1399
Epoch: 071/130 | Batch 0200/0313 | Loss: 0.1136
Epoch: 071/130 | Batch 0250/0313 | Loss: 0.1088
Epoch: 071/130 | Batch 0300/0313 | Loss: 0.1010
**Epoch: 071/130 | Train. Acc.: 96.352% | Loss: 0.1101
**Epoch: 071/130 | Valid. Acc.: 92.400% | Loss: 0.2469
Time elapsed: 125.67 min
Epoch: 072/130 | Current Learning Rate: 0.006250
Epoch: 072/130 | Batch 0000/0313 | Loss: 0.1143
Epoch: 072/130 | Batch 0050/0313 | Loss: 0.1205
Epoch: 072/130 | Batch 0100/0313 | Loss: 0.1003
Epoch: 072/130 | Batch 0150/0313 | Loss: 0.0603
Epoch: 072/130 | Batch 0200/0313 | Loss: 0.1142
Epoch: 072/130 | Batch 0250/0313 | Loss: 0.0855
Epoch: 072/130 | Batch 0300/0313 | Loss: 0.1455
**Epoch: 072/130 | Train. Acc.: 96.330% | Loss: 0.1065
**Epoch: 072/130 | Valid. Acc.: 92.260% | Loss: 0.2567
Time elapsed: 127.44 min
Epoch: 073/130 | Current Learning Rate: 0.006250
Epoch: 073/130 | Batch 0000/0313 | Loss: 0.1021
Epoch: 073/130 | Batch 0050/0313 | Loss: 0.1671
Epoch: 073/130 | Batch 0100/0313 | Loss: 0.1493
Epoch: 073/130 | Batch 0150/0313 | Loss: 0.0958
Epoch: 073/130 | Batch 0200/0313 | Loss: 0.1122
Epoch: 073/130 | Batch 0250/0313 | Loss: 0.1222
Epoch: 073/130 | Batch 0300/0313 | Loss: 0.1207
**Epoch: 073/130 | Train. Acc.: 96.470% | Loss: 0.1028
**Epoch: 073/130 | Valid. Acc.: 92.330% | Loss: 0.2535
Time elapsed: 129.21 min
Epoch: 074/130 | Current Learning Rate: 0.006250
Epoch: 074/130 | Batch 0000/0313 | Loss: 0.1253
Epoch: 074/130 | Batch 0050/0313 | Loss: 0.0651
Epoch: 074/130 | Batch 0100/0313 | Loss: 0.1215
Epoch: 074/130 | Batch 0150/0313 | Loss: 0.1087
Epoch: 074/130 | Batch 0200/0313 | Loss: 0.1261
Epoch: 074/130 | Batch 0250/0313 | Loss: 0.1602
Epoch: 074/130 | Batch 0300/0313 | Loss: 0.0837
**Epoch: 074/130 | Train. Acc.: 96.132% | Loss: 0.1109
**Epoch: 074/130 | Valid. Acc.: 92.160% | Loss: 0.2661
Time elapsed: 130.98 min
Epoch: 075/130 | Current Learning Rate: 0.006250
Epoch: 075/130 | Batch 0000/0313 | Loss: 0.1437
Epoch: 075/130 | Batch 0050/0313 | Loss: 0.1174
Epoch: 075/130 | Batch 0100/0313 | Loss: 0.0861
Epoch: 075/130 | Batch 0150/0313 | Loss: 0.1315
Epoch: 075/130 | Batch 0200/0313 | Loss: 0.1238
Epoch: 075/130 | Batch 0250/0313 | Loss: 0.1094
Epoch: 075/130 | Batch 0300/0313 | Loss: 0.1002
**Epoch: 075/130 | Train. Acc.: 96.810% | Loss: 0.0941
**Epoch: 075/130 | Valid. Acc.: 92.550% | Loss: 0.2453
Time elapsed: 132.76 min
Epoch: 076/130 | Current Learning Rate: 0.006250
Epoch: 076/130 | Batch 0000/0313 | Loss: 0.0946
Epoch: 076/130 | Batch 0050/0313 | Loss: 0.0720
Epoch: 076/130 | Batch 0100/0313 | Loss: 0.1594
Epoch: 076/130 | Batch 0150/0313 | Loss: 0.0507
Epoch: 076/130 | Batch 0200/0313 | Loss: 0.0851
Epoch: 076/130 | Batch 0250/0313 | Loss: 0.0435
Epoch: 076/130 | Batch 0300/0313 | Loss: 0.1957
**Epoch: 076/130 | Train. Acc.: 96.690% | Loss: 0.0959
**Epoch: 076/130 | Valid. Acc.: 92.330% | Loss: 0.2553
Time elapsed: 134.53 min
Epoch: 077/130 | Current Learning Rate: 0.006250
Epoch: 077/130 | Batch 0000/0313 | Loss: 0.0671
Epoch: 077/130 | Batch 0050/0313 | Loss: 0.0750
Epoch: 077/130 | Batch 0100/0313 | Loss: 0.1483
Epoch: 077/130 | Batch 0150/0313 | Loss: 0.0908
Epoch: 077/130 | Batch 0200/0313 | Loss: 0.1819
Epoch: 077/130 | Batch 0250/0313 | Loss: 0.1056
Epoch: 077/130 | Batch 0300/0313 | Loss: 0.2202
**Epoch: 077/130 | Train. Acc.: 96.373% | Loss: 0.1026
**Epoch: 077/130 | Valid. Acc.: 92.130% | Loss: 0.2677
Epoch 00077: reducing learning rate of group 0 to 3.1250e-03.
Time elapsed: 136.30 min
Epoch: 078/130 | Current Learning Rate: 0.003125
Epoch: 078/130 | Batch 0000/0313 | Loss: 0.1019
Epoch: 078/130 | Batch 0050/0313 | Loss: 0.0894
Epoch: 078/130 | Batch 0100/0313 | Loss: 0.0330
Epoch: 078/130 | Batch 0150/0313 | Loss: 0.0703
Epoch: 078/130 | Batch 0200/0313 | Loss: 0.0591
Epoch: 078/130 | Batch 0250/0313 | Loss: 0.0580
Epoch: 078/130 | Batch 0300/0313 | Loss: 0.0601
**Epoch: 078/130 | Train. Acc.: 97.910% | Loss: 0.0638
**Epoch: 078/130 | Valid. Acc.: 93.120% | Loss: 0.2248
**Validation loss decreased (0.242912 --> 0.224754). Saving model ...
Time elapsed: 138.09 min
Epoch: 079/130 | Current Learning Rate: 0.003125
Epoch: 079/130 | Batch 0000/0313 | Loss: 0.0578
Epoch: 079/130 | Batch 0050/0313 | Loss: 0.0917
Epoch: 079/130 | Batch 0100/0313 | Loss: 0.0505
Epoch: 079/130 | Batch 0150/0313 | Loss: 0.0842
Epoch: 079/130 | Batch 0200/0313 | Loss: 0.0596
Epoch: 079/130 | Batch 0250/0313 | Loss: 0.0644
Epoch: 079/130 | Batch 0300/0313 | Loss: 0.1023
**Epoch: 079/130 | Train. Acc.: 98.030% | Loss: 0.0594
**Epoch: 079/130 | Valid. Acc.: 92.980% | Loss: 0.2431
Time elapsed: 139.86 min
Epoch: 080/130 | Current Learning Rate: 0.003125
Epoch: 080/130 | Batch 0000/0313 | Loss: 0.0487
Epoch: 080/130 | Batch 0050/0313 | Loss: 0.1045
Epoch: 080/130 | Batch 0100/0313 | Loss: 0.0888
Epoch: 080/130 | Batch 0150/0313 | Loss: 0.0610
Epoch: 080/130 | Batch 0200/0313 | Loss: 0.1616
Epoch: 080/130 | Batch 0250/0313 | Loss: 0.0444
Epoch: 080/130 | Batch 0300/0313 | Loss: 0.0617
**Epoch: 080/130 | Train. Acc.: 98.060% | Loss: 0.0579
**Epoch: 080/130 | Valid. Acc.: 93.170% | Loss: 0.2408
Time elapsed: 141.63 min
Epoch: 081/130 | Current Learning Rate: 0.003125
Epoch: 081/130 | Batch 0000/0313 | Loss: 0.0670
Epoch: 081/130 | Batch 0050/0313 | Loss: 0.0338
Epoch: 081/130 | Batch 0100/0313 | Loss: 0.0413
Epoch: 081/130 | Batch 0150/0313 | Loss: 0.0582
Epoch: 081/130 | Batch 0200/0313 | Loss: 0.0539
Epoch: 081/130 | Batch 0250/0313 | Loss: 0.0334
Epoch: 081/130 | Batch 0300/0313 | Loss: 0.0535
**Epoch: 081/130 | Train. Acc.: 98.065% | Loss: 0.0582
**Epoch: 081/130 | Valid. Acc.: 93.300% | Loss: 0.2336
Time elapsed: 143.40 min
Epoch: 082/130 | Current Learning Rate: 0.003125
Epoch: 082/130 | Batch 0000/0313 | Loss: 0.0318
Epoch: 082/130 | Batch 0050/0313 | Loss: 0.0713
Epoch: 082/130 | Batch 0100/0313 | Loss: 0.0676
Epoch: 082/130 | Batch 0150/0313 | Loss: 0.0271
Epoch: 082/130 | Batch 0200/0313 | Loss: 0.0675
Epoch: 082/130 | Batch 0250/0313 | Loss: 0.0275
Epoch: 082/130 | Batch 0300/0313 | Loss: 0.0962
**Epoch: 082/130 | Train. Acc.: 98.365% | Loss: 0.0498
**Epoch: 082/130 | Valid. Acc.: 93.420% | Loss: 0.2332
Time elapsed: 145.18 min
Epoch: 083/130 | Current Learning Rate: 0.003125
Epoch: 083/130 | Batch 0000/0313 | Loss: 0.0468
Epoch: 083/130 | Batch 0050/0313 | Loss: 0.0204
Epoch: 083/130 | Batch 0100/0313 | Loss: 0.0872
Epoch: 083/130 | Batch 0150/0313 | Loss: 0.0756
Epoch: 083/130 | Batch 0200/0313 | Loss: 0.0764
Epoch: 083/130 | Batch 0250/0313 | Loss: 0.1574
Epoch: 083/130 | Batch 0300/0313 | Loss: 0.0296
**Epoch: 083/130 | Train. Acc.: 98.360% | Loss: 0.0506
**Epoch: 083/130 | Valid. Acc.: 93.300% | Loss: 0.2404
Time elapsed: 146.95 min
Epoch: 084/130 | Current Learning Rate: 0.003125
Epoch: 084/130 | Batch 0000/0313 | Loss: 0.0308
Epoch: 084/130 | Batch 0050/0313 | Loss: 0.0386
Epoch: 084/130 | Batch 0100/0313 | Loss: 0.0698
Epoch: 084/130 | Batch 0150/0313 | Loss: 0.0397
Epoch: 084/130 | Batch 0200/0313 | Loss: 0.0380
Epoch: 084/130 | Batch 0250/0313 | Loss: 0.0670
Epoch: 084/130 | Batch 0300/0313 | Loss: 0.1095
**Epoch: 084/130 | Train. Acc.: 98.395% | Loss: 0.0467
**Epoch: 084/130 | Valid. Acc.: 92.970% | Loss: 0.2417
Time elapsed: 148.73 min
Epoch: 085/130 | Current Learning Rate: 0.003125
Epoch: 085/130 | Batch 0000/0313 | Loss: 0.0189
Epoch: 085/130 | Batch 0050/0313 | Loss: 0.0952
Epoch: 085/130 | Batch 0100/0313 | Loss: 0.0286
Epoch: 085/130 | Batch 0150/0313 | Loss: 0.0492
Epoch: 085/130 | Batch 0200/0313 | Loss: 0.0341
Epoch: 085/130 | Batch 0250/0313 | Loss: 0.0433
Epoch: 085/130 | Batch 0300/0313 | Loss: 0.0473
**Epoch: 085/130 | Train. Acc.: 98.250% | Loss: 0.0508
**Epoch: 085/130 | Valid. Acc.: 93.330% | Loss: 0.2424
Epoch 00085: reducing learning rate of group 0 to 1.5625e-03.
Time elapsed: 150.50 min
Epoch: 086/130 | Current Learning Rate: 0.001563
Epoch: 086/130 | Batch 0000/0313 | Loss: 0.0372
Epoch: 086/130 | Batch 0050/0313 | Loss: 0.0609
Epoch: 086/130 | Batch 0100/0313 | Loss: 0.0686
Epoch: 086/130 | Batch 0150/0313 | Loss: 0.0545
Epoch: 086/130 | Batch 0200/0313 | Loss: 0.0387
Epoch: 086/130 | Batch 0250/0313 | Loss: 0.0755
Epoch: 086/130 | Batch 0300/0313 | Loss: 0.0554
**Epoch: 086/130 | Train. Acc.: 98.885% | Loss: 0.0358
**Epoch: 086/130 | Valid. Acc.: 93.460% | Loss: 0.2329
Time elapsed: 152.26 min
Epoch: 087/130 | Current Learning Rate: 0.001563
Epoch: 087/130 | Batch 0000/0313 | Loss: 0.0548
Epoch: 087/130 | Batch 0050/0313 | Loss: 0.0632
Epoch: 087/130 | Batch 0100/0313 | Loss: 0.0726
Epoch: 087/130 | Batch 0150/0313 | Loss: 0.0737
Epoch: 087/130 | Batch 0200/0313 | Loss: 0.0363
Epoch: 087/130 | Batch 0250/0313 | Loss: 0.0403
Epoch: 087/130 | Batch 0300/0313 | Loss: 0.0444
**Epoch: 087/130 | Train. Acc.: 99.082% | Loss: 0.0312
**Epoch: 087/130 | Valid. Acc.: 93.590% | Loss: 0.2261
Time elapsed: 154.04 min
Epoch: 088/130 | Current Learning Rate: 0.001563
Epoch: 088/130 | Batch 0000/0313 | Loss: 0.0241
Epoch: 088/130 | Batch 0050/0313 | Loss: 0.0277
Epoch: 088/130 | Batch 0100/0313 | Loss: 0.1143
Epoch: 088/130 | Batch 0150/0313 | Loss: 0.0470
Epoch: 088/130 | Batch 0200/0313 | Loss: 0.0534
Epoch: 088/130 | Batch 0250/0313 | Loss: 0.0583
Epoch: 088/130 | Batch 0300/0313 | Loss: 0.0485
**Epoch: 088/130 | Train. Acc.: 99.017% | Loss: 0.0319
**Epoch: 088/130 | Valid. Acc.: 93.600% | Loss: 0.2280
Time elapsed: 155.81 min
Epoch: 089/130 | Current Learning Rate: 0.001563
Epoch: 089/130 | Batch 0000/0313 | Loss: 0.0401
Epoch: 089/130 | Batch 0050/0313 | Loss: 0.0099
Epoch: 089/130 | Batch 0100/0313 | Loss: 0.0320
Epoch: 089/130 | Batch 0150/0313 | Loss: 0.0625
Epoch: 089/130 | Batch 0200/0313 | Loss: 0.0203
Epoch: 089/130 | Batch 0250/0313 | Loss: 0.0254
Epoch: 089/130 | Batch 0300/0313 | Loss: 0.0654
**Epoch: 089/130 | Train. Acc.: 99.032% | Loss: 0.0311
**Epoch: 089/130 | Valid. Acc.: 93.600% | Loss: 0.2312
Time elapsed: 157.58 min
Epoch: 090/130 | Current Learning Rate: 0.001563
Epoch: 090/130 | Batch 0000/0313 | Loss: 0.0367
Epoch: 090/130 | Batch 0050/0313 | Loss: 0.0375
Epoch: 090/130 | Batch 0100/0313 | Loss: 0.0409
Epoch: 090/130 | Batch 0150/0313 | Loss: 0.0337
Epoch: 090/130 | Batch 0200/0313 | Loss: 0.0425
Epoch: 090/130 | Batch 0250/0313 | Loss: 0.0134
Epoch: 090/130 | Batch 0300/0313 | Loss: 0.0437
**Epoch: 090/130 | Train. Acc.: 99.050% | Loss: 0.0309
**Epoch: 090/130 | Valid. Acc.: 93.330% | Loss: 0.2350
Time elapsed: 159.35 min
Epoch: 091/130 | Current Learning Rate: 0.001563
Epoch: 091/130 | Batch 0000/0313 | Loss: 0.0374
Epoch: 091/130 | Batch 0050/0313 | Loss: 0.0252
Epoch: 091/130 | Batch 0100/0313 | Loss: 0.0718
Epoch: 091/130 | Batch 0150/0313 | Loss: 0.0502
Epoch: 091/130 | Batch 0200/0313 | Loss: 0.0321
Epoch: 091/130 | Batch 0250/0313 | Loss: 0.0359
Epoch: 091/130 | Batch 0300/0313 | Loss: 0.0591
**Epoch: 091/130 | Train. Acc.: 99.157% | Loss: 0.0285
**Epoch: 091/130 | Valid. Acc.: 93.410% | Loss: 0.2377
Time elapsed: 161.13 min
Epoch: 092/130 | Current Learning Rate: 0.001563
Epoch: 092/130 | Batch 0000/0313 | Loss: 0.0514
Epoch: 092/130 | Batch 0050/0313 | Loss: 0.0330
Epoch: 092/130 | Batch 0100/0313 | Loss: 0.0377
Epoch: 092/130 | Batch 0150/0313 | Loss: 0.0464
Epoch: 092/130 | Batch 0200/0313 | Loss: 0.0671
Epoch: 092/130 | Batch 0250/0313 | Loss: 0.0401
Epoch: 092/130 | Batch 0300/0313 | Loss: 0.0788
**Epoch: 092/130 | Train. Acc.: 99.213% | Loss: 0.0251
**Epoch: 092/130 | Valid. Acc.: 93.480% | Loss: 0.2358
Epoch 00092: reducing learning rate of group 0 to 7.8125e-04.
Time elapsed: 162.90 min
Epoch: 093/130 | Current Learning Rate: 0.000781
Epoch: 093/130 | Batch 0000/0313 | Loss: 0.0445
Epoch: 093/130 | Batch 0050/0313 | Loss: 0.0505
Epoch: 093/130 | Batch 0100/0313 | Loss: 0.0266
Epoch: 093/130 | Batch 0150/0313 | Loss: 0.0081
Epoch: 093/130 | Batch 0200/0313 | Loss: 0.0074
Epoch: 093/130 | Batch 0250/0313 | Loss: 0.0493
Epoch: 093/130 | Batch 0300/0313 | Loss: 0.0289
**Epoch: 093/130 | Train. Acc.: 99.308% | Loss: 0.0231
**Epoch: 093/130 | Valid. Acc.: 93.650% | Loss: 0.2308
Time elapsed: 164.67 min
Epoch: 094/130 | Current Learning Rate: 0.000781
Epoch: 094/130 | Batch 0000/0313 | Loss: 0.0270
Epoch: 094/130 | Batch 0050/0313 | Loss: 0.0093
Epoch: 094/130 | Batch 0100/0313 | Loss: 0.0204
Epoch: 094/130 | Batch 0150/0313 | Loss: 0.0326
Epoch: 094/130 | Batch 0200/0313 | Loss: 0.0103
Epoch: 094/130 | Batch 0250/0313 | Loss: 0.0854
Epoch: 094/130 | Batch 0300/0313 | Loss: 0.0168
**Epoch: 094/130 | Train. Acc.: 99.303% | Loss: 0.0229
**Epoch: 094/130 | Valid. Acc.: 93.780% | Loss: 0.2273
Time elapsed: 166.44 min
Epoch: 095/130 | Current Learning Rate: 0.000781
Epoch: 095/130 | Batch 0000/0313 | Loss: 0.0151
Epoch: 095/130 | Batch 0050/0313 | Loss: 0.0462
Epoch: 095/130 | Batch 0100/0313 | Loss: 0.0183
Epoch: 095/130 | Batch 0150/0313 | Loss: 0.0248
Epoch: 095/130 | Batch 0200/0313 | Loss: 0.0129
Epoch: 095/130 | Batch 0250/0313 | Loss: 0.0468
Epoch: 095/130 | Batch 0300/0313 | Loss: 0.0185
**Epoch: 095/130 | Train. Acc.: 99.373% | Loss: 0.0211
**Epoch: 095/130 | Valid. Acc.: 93.780% | Loss: 0.2286
Time elapsed: 168.21 min
Epoch: 096/130 | Current Learning Rate: 0.000781
Epoch: 096/130 | Batch 0000/0313 | Loss: 0.0269
Epoch: 096/130 | Batch 0050/0313 | Loss: 0.0382
Epoch: 096/130 | Batch 0100/0313 | Loss: 0.0173
Epoch: 096/130 | Batch 0150/0313 | Loss: 0.0667
Epoch: 096/130 | Batch 0200/0313 | Loss: 0.0318
Epoch: 096/130 | Batch 0250/0313 | Loss: 0.0091
Epoch: 096/130 | Batch 0300/0313 | Loss: 0.0586
**Epoch: 096/130 | Train. Acc.: 99.340% | Loss: 0.0219
**Epoch: 096/130 | Valid. Acc.: 93.750% | Loss: 0.2300
Time elapsed: 169.98 min
Epoch: 097/130 | Current Learning Rate: 0.000781
Epoch: 097/130 | Batch 0000/0313 | Loss: 0.0639
Epoch: 097/130 | Batch 0050/0313 | Loss: 0.0310
Epoch: 097/130 | Batch 0100/0313 | Loss: 0.0236
Epoch: 097/130 | Batch 0150/0313 | Loss: 0.0211
Epoch: 097/130 | Batch 0200/0313 | Loss: 0.0456
Epoch: 097/130 | Batch 0250/0313 | Loss: 0.0223
Epoch: 097/130 | Batch 0300/0313 | Loss: 0.0291
**Epoch: 097/130 | Train. Acc.: 99.355% | Loss: 0.0210
**Epoch: 097/130 | Valid. Acc.: 94.030% | Loss: 0.2282
Time elapsed: 171.76 min
Epoch: 098/130 | Current Learning Rate: 0.000781
Epoch: 098/130 | Batch 0000/0313 | Loss: 0.0194
Epoch: 098/130 | Batch 0050/0313 | Loss: 0.0217
Epoch: 098/130 | Batch 0100/0313 | Loss: 0.0226
Epoch: 098/130 | Batch 0150/0313 | Loss: 0.0452
Epoch: 098/130 | Batch 0200/0313 | Loss: 0.0412
Epoch: 098/130 | Batch 0250/0313 | Loss: 0.0171
Epoch: 098/130 | Batch 0300/0313 | Loss: 0.1023
**Epoch: 098/130 | Train. Acc.: 99.370% | Loss: 0.0208
**Epoch: 098/130 | Valid. Acc.: 93.810% | Loss: 0.2306
Time elapsed: 173.52 min
Epoch: 099/130 | Current Learning Rate: 0.000781
Epoch: 099/130 | Batch 0000/0313 | Loss: 0.0115
Epoch: 099/130 | Batch 0050/0313 | Loss: 0.0272
Epoch: 099/130 | Batch 0100/0313 | Loss: 0.0179
Epoch: 099/130 | Batch 0150/0313 | Loss: 0.0295
Epoch: 099/130 | Batch 0200/0313 | Loss: 0.0660
Epoch: 099/130 | Batch 0250/0313 | Loss: 0.0457
Epoch: 099/130 | Batch 0300/0313 | Loss: 0.0118
**Epoch: 099/130 | Train. Acc.: 99.462% | Loss: 0.0192
**Epoch: 099/130 | Valid. Acc.: 93.720% | Loss: 0.2346
Epoch 00099: reducing learning rate of group 0 to 3.9063e-04.
Time elapsed: 175.29 min
Epoch: 100/130 | Current Learning Rate: 0.000391
Epoch: 100/130 | Batch 0000/0313 | Loss: 0.0138
Epoch: 100/130 | Batch 0050/0313 | Loss: 0.0379
Epoch: 100/130 | Batch 0100/0313 | Loss: 0.0296
Epoch: 100/130 | Batch 0150/0313 | Loss: 0.0209
Epoch: 100/130 | Batch 0200/0313 | Loss: 0.0312
Epoch: 100/130 | Batch 0250/0313 | Loss: 0.0533
Epoch: 100/130 | Batch 0300/0313 | Loss: 0.0095
**Epoch: 100/130 | Train. Acc.: 99.492% | Loss: 0.0187
**Epoch: 100/130 | Valid. Acc.: 93.650% | Loss: 0.2317
Time elapsed: 177.05 min
Epoch: 101/130 | Current Learning Rate: 0.000391
Epoch: 101/130 | Batch 0000/0313 | Loss: 0.0559
Epoch: 101/130 | Batch 0050/0313 | Loss: 0.0289
Epoch: 101/130 | Batch 0100/0313 | Loss: 0.0651
Epoch: 101/130 | Batch 0150/0313 | Loss: 0.0160
Epoch: 101/130 | Batch 0200/0313 | Loss: 0.0544
Epoch: 101/130 | Batch 0250/0313 | Loss: 0.0330
Epoch: 101/130 | Batch 0300/0313 | Loss: 0.0065
**Epoch: 101/130 | Train. Acc.: 99.462% | Loss: 0.0176
**Epoch: 101/130 | Valid. Acc.: 93.810% | Loss: 0.2296
Time elapsed: 178.82 min
Epoch: 102/130 | Current Learning Rate: 0.000391
Epoch: 102/130 | Batch 0000/0313 | Loss: 0.0308
Epoch: 102/130 | Batch 0050/0313 | Loss: 0.0345
Epoch: 102/130 | Batch 0100/0313 | Loss: 0.0130
Epoch: 102/130 | Batch 0150/0313 | Loss: 0.0199
Epoch: 102/130 | Batch 0200/0313 | Loss: 0.0260
Epoch: 102/130 | Batch 0250/0313 | Loss: 0.0143
Epoch: 102/130 | Batch 0300/0313 | Loss: 0.0502
**Epoch: 102/130 | Train. Acc.: 99.475% | Loss: 0.0179
**Epoch: 102/130 | Valid. Acc.: 93.810% | Loss: 0.2345
Time elapsed: 180.58 min
Epoch: 103/130 | Current Learning Rate: 0.000391
Epoch: 103/130 | Batch 0000/0313 | Loss: 0.0172
Epoch: 103/130 | Batch 0050/0313 | Loss: 0.0316
Epoch: 103/130 | Batch 0100/0313 | Loss: 0.0405
Epoch: 103/130 | Batch 0150/0313 | Loss: 0.0205
Epoch: 103/130 | Batch 0200/0313 | Loss: 0.0032
Epoch: 103/130 | Batch 0250/0313 | Loss: 0.0251
Epoch: 103/130 | Batch 0300/0313 | Loss: 0.0237
**Epoch: 103/130 | Train. Acc.: 99.480% | Loss: 0.0173
**Epoch: 103/130 | Valid. Acc.: 93.880% | Loss: 0.2314
Time elapsed: 182.35 min
Epoch: 104/130 | Current Learning Rate: 0.000391
Epoch: 104/130 | Batch 0000/0313 | Loss: 0.0283
Epoch: 104/130 | Batch 0050/0313 | Loss: 0.0480
Epoch: 104/130 | Batch 0100/0313 | Loss: 0.0296
Epoch: 104/130 | Batch 0150/0313 | Loss: 0.0075
Epoch: 104/130 | Batch 0200/0313 | Loss: 0.0167
Epoch: 104/130 | Batch 0250/0313 | Loss: 0.0293
Epoch: 104/130 | Batch 0300/0313 | Loss: 0.0187
**Epoch: 104/130 | Train. Acc.: 99.510% | Loss: 0.0167
**Epoch: 104/130 | Valid. Acc.: 93.850% | Loss: 0.2346
Time elapsed: 184.12 min
Epoch: 105/130 | Current Learning Rate: 0.000391
Epoch: 105/130 | Batch 0000/0313 | Loss: 0.0064
Epoch: 105/130 | Batch 0050/0313 | Loss: 0.0243
Epoch: 105/130 | Batch 0100/0313 | Loss: 0.0158
Epoch: 105/130 | Batch 0150/0313 | Loss: 0.0174
Epoch: 105/130 | Batch 0200/0313 | Loss: 0.0424
Epoch: 105/130 | Batch 0250/0313 | Loss: 0.0541
Epoch: 105/130 | Batch 0300/0313 | Loss: 0.0138
**Epoch: 105/130 | Train. Acc.: 99.487% | Loss: 0.0174
**Epoch: 105/130 | Valid. Acc.: 93.690% | Loss: 0.2331
Time elapsed: 185.90 min
Epoch: 106/130 | Current Learning Rate: 0.000391
Epoch: 106/130 | Batch 0000/0313 | Loss: 0.0200
Epoch: 106/130 | Batch 0050/0313 | Loss: 0.0200
Epoch: 106/130 | Batch 0100/0313 | Loss: 0.0177
Epoch: 106/130 | Batch 0150/0313 | Loss: 0.0160
Epoch: 106/130 | Batch 0200/0313 | Loss: 0.0262
Epoch: 106/130 | Batch 0250/0313 | Loss: 0.0130
Epoch: 106/130 | Batch 0300/0313 | Loss: 0.0210
**Epoch: 106/130 | Train. Acc.: 99.537% | Loss: 0.0167
**Epoch: 106/130 | Valid. Acc.: 93.810% | Loss: 0.2347
Epoch 00106: reducing learning rate of group 0 to 1.9531e-04.
Time elapsed: 187.66 min
Epoch: 107/130 | Current Learning Rate: 0.000195
Epoch: 107/130 | Batch 0000/0313 | Loss: 0.0234
Epoch: 107/130 | Batch 0050/0313 | Loss: 0.0385
Epoch: 107/130 | Batch 0100/0313 | Loss: 0.0383
Epoch: 107/130 | Batch 0150/0313 | Loss: 0.0141
Epoch: 107/130 | Batch 0200/0313 | Loss: 0.0210
Epoch: 107/130 | Batch 0250/0313 | Loss: 0.0372
Epoch: 107/130 | Batch 0300/0313 | Loss: 0.0130
**Epoch: 107/130 | Train. Acc.: 99.545% | Loss: 0.0164
**Epoch: 107/130 | Valid. Acc.: 93.820% | Loss: 0.2338
Time elapsed: 189.43 min
Epoch: 108/130 | Current Learning Rate: 0.000195
Epoch: 108/130 | Batch 0000/0313 | Loss: 0.0102
Epoch: 108/130 | Batch 0050/0313 | Loss: 0.0486
Epoch: 108/130 | Batch 0100/0313 | Loss: 0.0450
Epoch: 108/130 | Batch 0150/0313 | Loss: 0.0181
Epoch: 108/130 | Batch 0200/0313 | Loss: 0.0095
Epoch: 108/130 | Batch 0250/0313 | Loss: 0.0143
Epoch: 108/130 | Batch 0300/0313 | Loss: 0.0447
**Epoch: 108/130 | Train. Acc.: 99.540% | Loss: 0.0168
**Epoch: 108/130 | Valid. Acc.: 93.880% | Loss: 0.2324
Time elapsed: 191.20 min
Epoch: 109/130 | Current Learning Rate: 0.000195
Epoch: 109/130 | Batch 0000/0313 | Loss: 0.0248
Epoch: 109/130 | Batch 0050/0313 | Loss: 0.0173
Epoch: 109/130 | Batch 0100/0313 | Loss: 0.0086
Epoch: 109/130 | Batch 0150/0313 | Loss: 0.0208
Epoch: 109/130 | Batch 0200/0313 | Loss: 0.0142
Epoch: 109/130 | Batch 0250/0313 | Loss: 0.0169
Epoch: 109/130 | Batch 0300/0313 | Loss: 0.0034
**Epoch: 109/130 | Train. Acc.: 99.530% | Loss: 0.0166
**Epoch: 109/130 | Valid. Acc.: 93.800% | Loss: 0.2329
Time elapsed: 192.97 min
Epoch: 110/130 | Current Learning Rate: 0.000195
Epoch: 110/130 | Batch 0000/0313 | Loss: 0.0243
Epoch: 110/130 | Batch 0050/0313 | Loss: 0.0389
Epoch: 110/130 | Batch 0100/0313 | Loss: 0.0109
Epoch: 110/130 | Batch 0150/0313 | Loss: 0.0127
Epoch: 110/130 | Batch 0200/0313 | Loss: 0.0146
Epoch: 110/130 | Batch 0250/0313 | Loss: 0.0339
Epoch: 110/130 | Batch 0300/0313 | Loss: 0.0235
**Epoch: 110/130 | Train. Acc.: 99.525% | Loss: 0.0165
**Epoch: 110/130 | Valid. Acc.: 93.790% | Loss: 0.2351
Time elapsed: 194.74 min
Epoch: 111/130 | Current Learning Rate: 0.000195
Epoch: 111/130 | Batch 0000/0313 | Loss: 0.0238
Epoch: 111/130 | Batch 0050/0313 | Loss: 0.0107
Epoch: 111/130 | Batch 0100/0313 | Loss: 0.0082
Epoch: 111/130 | Batch 0150/0313 | Loss: 0.0067
Epoch: 111/130 | Batch 0200/0313 | Loss: 0.0328
Epoch: 111/130 | Batch 0250/0313 | Loss: 0.0210
Epoch: 111/130 | Batch 0300/0313 | Loss: 0.0075
**Epoch: 111/130 | Train. Acc.: 99.540% | Loss: 0.0157
**Epoch: 111/130 | Valid. Acc.: 93.710% | Loss: 0.2356
Time elapsed: 196.51 min
Epoch: 112/130 | Current Learning Rate: 0.000195
Epoch: 112/130 | Batch 0000/0313 | Loss: 0.0231
Epoch: 112/130 | Batch 0050/0313 | Loss: 0.0106
Epoch: 112/130 | Batch 0100/0313 | Loss: 0.0358
Epoch: 112/130 | Batch 0150/0313 | Loss: 0.0381
Epoch: 112/130 | Batch 0200/0313 | Loss: 0.0156
Epoch: 112/130 | Batch 0250/0313 | Loss: 0.0186
Epoch: 112/130 | Batch 0300/0313 | Loss: 0.0190
**Epoch: 112/130 | Train. Acc.: 99.550% | Loss: 0.0157
**Epoch: 112/130 | Valid. Acc.: 93.790% | Loss: 0.2339
Time elapsed: 198.27 min
Epoch: 113/130 | Current Learning Rate: 0.000195
Epoch: 113/130 | Batch 0000/0313 | Loss: 0.0296
Epoch: 113/130 | Batch 0050/0313 | Loss: 0.0055
Epoch: 113/130 | Batch 0100/0313 | Loss: 0.0078
Epoch: 113/130 | Batch 0150/0313 | Loss: 0.0078
Epoch: 113/130 | Batch 0200/0313 | Loss: 0.0072
Epoch: 113/130 | Batch 0250/0313 | Loss: 0.0228
Epoch: 113/130 | Batch 0300/0313 | Loss: 0.0562
**Epoch: 113/130 | Train. Acc.: 99.552% | Loss: 0.0156
**Epoch: 113/130 | Valid. Acc.: 93.670% | Loss: 0.2351
Epoch 00113: reducing learning rate of group 0 to 9.7656e-05.
Time elapsed: 200.04 min
Epoch: 114/130 | Current Learning Rate: 0.000098
Epoch: 114/130 | Batch 0000/0313 | Loss: 0.0343
Epoch: 114/130 | Batch 0050/0313 | Loss: 0.0059
Epoch: 114/130 | Batch 0100/0313 | Loss: 0.0046
Epoch: 114/130 | Batch 0150/0313 | Loss: 0.0117
Epoch: 114/130 | Batch 0200/0313 | Loss: 0.0245
Epoch: 114/130 | Batch 0250/0313 | Loss: 0.0152
Epoch: 114/130 | Batch 0300/0313 | Loss: 0.0053
**Epoch: 114/130 | Train. Acc.: 99.510% | Loss: 0.0169
**Epoch: 114/130 | Valid. Acc.: 93.680% | Loss: 0.2358
Time elapsed: 201.80 min
Epoch: 115/130 | Current Learning Rate: 0.000098
Epoch: 115/130 | Batch 0000/0313 | Loss: 0.0352
Epoch: 115/130 | Batch 0050/0313 | Loss: 0.0416
Epoch: 115/130 | Batch 0100/0313 | Loss: 0.0315
Epoch: 115/130 | Batch 0150/0313 | Loss: 0.0106
Epoch: 115/130 | Batch 0200/0313 | Loss: 0.0058
Epoch: 115/130 | Batch 0250/0313 | Loss: 0.0386
Epoch: 115/130 | Batch 0300/0313 | Loss: 0.0105
**Epoch: 115/130 | Train. Acc.: 99.547% | Loss: 0.0152
**Epoch: 115/130 | Valid. Acc.: 93.890% | Loss: 0.2339
Time elapsed: 203.57 min
Epoch: 116/130 | Current Learning Rate: 0.000098
Epoch: 116/130 | Batch 0000/0313 | Loss: 0.0158
Epoch: 116/130 | Batch 0050/0313 | Loss: 0.0120
Epoch: 116/130 | Batch 0100/0313 | Loss: 0.0293
Epoch: 116/130 | Batch 0150/0313 | Loss: 0.0212
Epoch: 116/130 | Batch 0200/0313 | Loss: 0.0223
Epoch: 116/130 | Batch 0250/0313 | Loss: 0.0234
Epoch: 116/130 | Batch 0300/0313 | Loss: 0.0424
**Epoch: 116/130 | Train. Acc.: 99.595% | Loss: 0.0148
**Epoch: 116/130 | Valid. Acc.: 93.660% | Loss: 0.2365
Time elapsed: 205.33 min
Epoch: 117/130 | Current Learning Rate: 0.000098
Epoch: 117/130 | Batch 0000/0313 | Loss: 0.0065
Epoch: 117/130 | Batch 0050/0313 | Loss: 0.0077
Epoch: 117/130 | Batch 0100/0313 | Loss: 0.0132
Epoch: 117/130 | Batch 0150/0313 | Loss: 0.0958
Epoch: 117/130 | Batch 0200/0313 | Loss: 0.0216
Epoch: 117/130 | Batch 0250/0313 | Loss: 0.0235
Epoch: 117/130 | Batch 0300/0313 | Loss: 0.0087
**Epoch: 117/130 | Train. Acc.: 99.585% | Loss: 0.0150
**Epoch: 117/130 | Valid. Acc.: 93.840% | Loss: 0.2366
Time elapsed: 207.10 min
Epoch: 118/130 | Current Learning Rate: 0.000098
Epoch: 118/130 | Batch 0000/0313 | Loss: 0.0202
Epoch: 118/130 | Batch 0050/0313 | Loss: 0.0200
Epoch: 118/130 | Batch 0100/0313 | Loss: 0.0085
Epoch: 118/130 | Batch 0150/0313 | Loss: 0.0085
Epoch: 118/130 | Batch 0200/0313 | Loss: 0.0109
Epoch: 118/130 | Batch 0250/0313 | Loss: 0.0157
Epoch: 118/130 | Batch 0300/0313 | Loss: 0.0326
**Epoch: 118/130 | Train. Acc.: 99.545% | Loss: 0.0158
**Epoch: 118/130 | Valid. Acc.: 93.830% | Loss: 0.2340
Time elapsed: 208.86 min
Epoch: 119/130 | Current Learning Rate: 0.000098
Epoch: 119/130 | Batch 0000/0313 | Loss: 0.0067
Epoch: 119/130 | Batch 0050/0313 | Loss: 0.0120
Epoch: 119/130 | Batch 0100/0313 | Loss: 0.0153
Epoch: 119/130 | Batch 0150/0313 | Loss: 0.0188
Epoch: 119/130 | Batch 0200/0313 | Loss: 0.0146
Epoch: 119/130 | Batch 0250/0313 | Loss: 0.0193
Epoch: 119/130 | Batch 0300/0313 | Loss: 0.0071
**Epoch: 119/130 | Train. Acc.: 99.578% | Loss: 0.0150
**Epoch: 119/130 | Valid. Acc.: 93.750% | Loss: 0.2351
Time elapsed: 210.63 min
Epoch: 120/130 | Current Learning Rate: 0.000098
Epoch: 120/130 | Batch 0000/0313 | Loss: 0.0204
Epoch: 120/130 | Batch 0050/0313 | Loss: 0.0286
Epoch: 120/130 | Batch 0100/0313 | Loss: 0.0466
Epoch: 120/130 | Batch 0150/0313 | Loss: 0.0041
Epoch: 120/130 | Batch 0200/0313 | Loss: 0.0201
Epoch: 120/130 | Batch 0250/0313 | Loss: 0.0162
Epoch: 120/130 | Batch 0300/0313 | Loss: 0.0136
**Epoch: 120/130 | Train. Acc.: 99.547% | Loss: 0.0156
**Epoch: 120/130 | Valid. Acc.: 93.800% | Loss: 0.2360
Epoch 00120: reducing learning rate of group 0 to 4.8828e-05.
Time elapsed: 212.39 min
Epoch: 121/130 | Current Learning Rate: 0.000049
Epoch: 121/130 | Batch 0000/0313 | Loss: 0.0225
Epoch: 121/130 | Batch 0050/0313 | Loss: 0.0405
Epoch: 121/130 | Batch 0100/0313 | Loss: 0.0378
Epoch: 121/130 | Batch 0150/0313 | Loss: 0.0521
Epoch: 121/130 | Batch 0200/0313 | Loss: 0.0423
Epoch: 121/130 | Batch 0250/0313 | Loss: 0.0074
Epoch: 121/130 | Batch 0300/0313 | Loss: 0.0202
**Epoch: 121/130 | Train. Acc.: 99.620% | Loss: 0.0145
**Epoch: 121/130 | Valid. Acc.: 93.730% | Loss: 0.2364
Time elapsed: 214.16 min
Epoch: 122/130 | Current Learning Rate: 0.000049
Epoch: 122/130 | Batch 0000/0313 | Loss: 0.0261
Epoch: 122/130 | Batch 0050/0313 | Loss: 0.0460
Epoch: 122/130 | Batch 0100/0313 | Loss: 0.0256
Epoch: 122/130 | Batch 0150/0313 | Loss: 0.0083
Epoch: 122/130 | Batch 0200/0313 | Loss: 0.0114
Epoch: 122/130 | Batch 0250/0313 | Loss: 0.0056
Epoch: 122/130 | Batch 0300/0313 | Loss: 0.0295
**Epoch: 122/130 | Train. Acc.: 99.582% | Loss: 0.0146
**Epoch: 122/130 | Valid. Acc.: 93.710% | Loss: 0.2356
Time elapsed: 215.92 min
Epoch: 123/130 | Current Learning Rate: 0.000049
Epoch: 123/130 | Batch 0000/0313 | Loss: 0.0121
Epoch: 123/130 | Batch 0050/0313 | Loss: 0.0081
Epoch: 123/130 | Batch 0100/0313 | Loss: 0.0094
Epoch: 123/130 | Batch 0150/0313 | Loss: 0.0168
Epoch: 123/130 | Batch 0200/0313 | Loss: 0.0317
Epoch: 123/130 | Batch 0250/0313 | Loss: 0.0178
Epoch: 123/130 | Batch 0300/0313 | Loss: 0.0386
**Epoch: 123/130 | Train. Acc.: 99.562% | Loss: 0.0151
**Epoch: 123/130 | Valid. Acc.: 93.730% | Loss: 0.2354
Time elapsed: 217.69 min
Epoch: 124/130 | Current Learning Rate: 0.000049
Epoch: 124/130 | Batch 0000/0313 | Loss: 0.0157
Epoch: 124/130 | Batch 0050/0313 | Loss: 0.0138
Epoch: 124/130 | Batch 0100/0313 | Loss: 0.0314
Epoch: 124/130 | Batch 0150/0313 | Loss: 0.0438
Epoch: 124/130 | Batch 0200/0313 | Loss: 0.0192
Epoch: 124/130 | Batch 0250/0313 | Loss: 0.0675
Epoch: 124/130 | Batch 0300/0313 | Loss: 0.0140
**Epoch: 124/130 | Train. Acc.: 99.572% | Loss: 0.0150
**Epoch: 124/130 | Valid. Acc.: 93.810% | Loss: 0.2347
Time elapsed: 219.45 min
Epoch: 125/130 | Current Learning Rate: 0.000049
Epoch: 125/130 | Batch 0000/0313 | Loss: 0.0343
Epoch: 125/130 | Batch 0050/0313 | Loss: 0.0378
Epoch: 125/130 | Batch 0100/0313 | Loss: 0.0174
Epoch: 125/130 | Batch 0150/0313 | Loss: 0.0284
Epoch: 125/130 | Batch 0200/0313 | Loss: 0.0143
Epoch: 125/130 | Batch 0250/0313 | Loss: 0.0141
Epoch: 125/130 | Batch 0300/0313 | Loss: 0.0618
**Epoch: 125/130 | Train. Acc.: 99.595% | Loss: 0.0143
**Epoch: 125/130 | Valid. Acc.: 93.770% | Loss: 0.2366
Time elapsed: 221.22 min
Epoch: 126/130 | Current Learning Rate: 0.000049
Epoch: 126/130 | Batch 0000/0313 | Loss: 0.0104
Epoch: 126/130 | Batch 0050/0313 | Loss: 0.0091
Epoch: 126/130 | Batch 0100/0313 | Loss: 0.0108
Epoch: 126/130 | Batch 0150/0313 | Loss: 0.0052
Epoch: 126/130 | Batch 0200/0313 | Loss: 0.0119
Epoch: 126/130 | Batch 0250/0313 | Loss: 0.0145
Epoch: 126/130 | Batch 0300/0313 | Loss: 0.0281
**Epoch: 126/130 | Train. Acc.: 99.635% | Loss: 0.0142
**Epoch: 126/130 | Valid. Acc.: 93.800% | Loss: 0.2338
Time elapsed: 222.98 min
Epoch: 127/130 | Current Learning Rate: 0.000049
Epoch: 127/130 | Batch 0000/0313 | Loss: 0.0138
Epoch: 127/130 | Batch 0050/0313 | Loss: 0.0523
Epoch: 127/130 | Batch 0100/0313 | Loss: 0.0168
Epoch: 127/130 | Batch 0150/0313 | Loss: 0.0294
Epoch: 127/130 | Batch 0200/0313 | Loss: 0.0075
Epoch: 127/130 | Batch 0250/0313 | Loss: 0.0117
Epoch: 127/130 | Batch 0300/0313 | Loss: 0.0246
**Epoch: 127/130 | Train. Acc.: 99.595% | Loss: 0.0142
**Epoch: 127/130 | Valid. Acc.: 93.770% | Loss: 0.2349
Epoch 00127: reducing learning rate of group 0 to 2.4414e-05.
Time elapsed: 224.75 min
Epoch: 128/130 | Current Learning Rate: 0.000024
Epoch: 128/130 | Batch 0000/0313 | Loss: 0.0191
Epoch: 128/130 | Batch 0050/0313 | Loss: 0.0210
Epoch: 128/130 | Batch 0100/0313 | Loss: 0.0171
Epoch: 128/130 | Batch 0150/0313 | Loss: 0.0274
Epoch: 128/130 | Batch 0200/0313 | Loss: 0.0143
Epoch: 128/130 | Batch 0250/0313 | Loss: 0.0077
Epoch: 128/130 | Batch 0300/0313 | Loss: 0.0484
**Epoch: 128/130 | Train. Acc.: 99.585% | Loss: 0.0150
**Epoch: 128/130 | Valid. Acc.: 93.780% | Loss: 0.2374
Time elapsed: 226.52 min
Epoch: 129/130 | Current Learning Rate: 0.000024
Epoch: 129/130 | Batch 0000/0313 | Loss: 0.0547
Epoch: 129/130 | Batch 0050/0313 | Loss: 0.0094
Epoch: 129/130 | Batch 0100/0313 | Loss: 0.0157
Epoch: 129/130 | Batch 0150/0313 | Loss: 0.0084
Epoch: 129/130 | Batch 0200/0313 | Loss: 0.0459
Epoch: 129/130 | Batch 0250/0313 | Loss: 0.0129
Epoch: 129/130 | Batch 0300/0313 | Loss: 0.0100
**Epoch: 129/130 | Train. Acc.: 99.585% | Loss: 0.0147
**Epoch: 129/130 | Valid. Acc.: 93.850% | Loss: 0.2338
Time elapsed: 228.29 min
Epoch: 130/130 | Current Learning Rate: 0.000024
Epoch: 130/130 | Batch 0000/0313 | Loss: 0.0491
Epoch: 130/130 | Batch 0050/0313 | Loss: 0.0251
Epoch: 130/130 | Batch 0100/0313 | Loss: 0.0062
Epoch: 130/130 | Batch 0150/0313 | Loss: 0.0311
Epoch: 130/130 | Batch 0200/0313 | Loss: 0.0355
Epoch: 130/130 | Batch 0250/0313 | Loss: 0.0279
Epoch: 130/130 | Batch 0300/0313 | Loss: 0.0302
**Epoch: 130/130 | Train. Acc.: 99.650% | Loss: 0.0133
**Epoch: 130/130 | Valid. Acc.: 93.770% | Loss: 0.2358
Time elapsed: 230.06 min
Total Training Time: 230.06 min
Model: ResNet152
Test Loss: 0.2346
Test Accuracy (Overall): 93.17%

Test Accuracy of Airplane: 95% (952/1000)
Test Accuracy of      Car: 97% (979/1000)
Test Accuracy of     Bird: 90% (902/1000)
Test Accuracy of      Cat: 85% (856/1000)
Test Accuracy of     Deer: 94% (942/1000)
Test Accuracy of      Dog: 88% (884/1000)
Test Accuracy of     Frog: 94% (941/1000)
Test Accuracy of    Horse: 96% (960/1000)
Test Accuracy of     Ship: 95% (956/1000)
Test Accuracy of    Truck: 94% (945/1000)
