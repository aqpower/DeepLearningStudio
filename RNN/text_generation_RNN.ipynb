{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvKWR3W5bhYI"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "Projext Objectives:\n",
        "\n",
        "- Generate text using RNN.\n",
        "- Creat a training examples and targets for text generation.\n",
        "- Build a RNN model for serquence generation using Keras Subclassing.\n",
        "- Create a text generator and evaluate the output.\n",
        "\n",
        "\n",
        "\n",
        "This projext demonstrates how to generate text using character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Bellow is a sample output when the model in this tutorial trained for 30 epochs and started with prompt \"Q\"\n",
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "\n",
        "While some of the sentences are grammatical, most odo not make sense. The model has not learned the meaning of words, but there are some things to consider.\n",
        "\n",
        "  * The  model is charcter-based. When training started the model did not know how to spell an English-word, or that words were even a unit of text.\n",
        "\n",
        "  * The strucuter of the outputresembles a play-blocks of text generally begin with a speaker name, in all the capital letters similar to the dataset.\n",
        "\n",
        "  * As demonstrated below, the model is trained on small batches of text (100 cahracters each), and is stil able to generate a longer sequence of text with coherent strucutre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pbaxk-QjR_r"
      },
      "source": [
        "## Setup\n",
        "Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeJDXIbwifEo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKaebvTzke-G"
      },
      "source": [
        "### Download the Shakesoeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oU8tk4TxjpIP"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_to_file \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshakespeare.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78gjtFMilg13"
      },
      "source": [
        "### Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWxzttpTkYCJ",
        "outputId": "c50d8bf9-7c97-4d43-f81f-0d50de6dd23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenght of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Lenght of text: {len(text)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QF5qmu4l31O"
      },
      "source": [
        "Let's take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfylHBE4lanl",
        "outputId": "44d85c1e-56d0-4db6-9898-55199e804d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa915edKohNS"
      },
      "source": [
        "Let's see how many unique characters are in our documnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvHz2xTFpAId",
        "outputId": "c9ae7390-b150-4508-f548-4f982cad7123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArtvLS3Rpgx-"
      },
      "source": [
        "This number represents the tokens that the Neural Network will consume during training, and will be generating during use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3V7dXcGpNZV"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY_8YQ5Jp1wr"
      },
      "source": [
        "#### Vectorize the text\n",
        "Before training, we need to convert the strings to a numerical representation.\n",
        "\n",
        "Using `tf.keras.layers.StringLookup` layer can convert each character into an numeric ID, it just needs the text to be split into tokens first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdWEjJtpU0e",
        "outputId": "89cf80d6-2f66-4226-e363-27d83a1a58d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
              " [b'n', b'a', b'f', b'e', b't', b's']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_text = ['stefan', 'nafets']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_text, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TGz_GkOqyMS"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layers, this will help us to tranform the character into numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wpwzsuzrc0_"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW0aeYVEr0WY"
      },
      "source": [
        "it converts from tokens to character ID's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBEOBgSvr7dd",
        "outputId": "dfed7edb-b388-49f0-bf39-6ac3a33b54e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[58, 59, 44, 45, 40, 53],\n",
              " [53, 40, 45, 44, 59, 58]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzCQR2U6sAbK"
      },
      "source": [
        "Since the goal of this project is to generate text, it will be important to invert this representation and recover human-readable strings from it. To achieve this we can use `tf.keras.layers.StringLookup(......, invert=True)`.\n",
        "\n",
        "NOTE: Here instade of pasing the original vocabulary henerated with `sorted(set(list))` use the `get_vocabulary()` method so the `tf.keras.layers.StringLookup` layer so that the `[UNK]` are set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aczLcFbZ1g8X"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1Q_d027y99"
      },
      "source": [
        "This layer recovers the cahracters from the vectors of ID's, and returns them as a `tf.RaggedTensor` of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4zY3Qy62EkA",
        "outputId": "6a5a2682-dc9e-4008-92b1-034d8f03f1ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
              " [b'n', b'a', b'f', b'e', b't', b's']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR22YfoJ3mys"
      },
      "source": [
        "You can use `tf.strings.reduce_join()` to join the characters back into strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOHv84_82K6x",
        "outputId": "93632687-e722-4173-fa51-1579ec0e7cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'stefan', b'nafets'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT3Euh2yDMtT"
      },
      "outputs": [],
      "source": [
        "# lets create a function that we can call later\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-WJ97BEDNPD"
      },
      "source": [
        "## The prediciton task\n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "This is the task that we training the model to perform. The input to the model will be sequence of characters, and you train the model to predict the output-the following character at each time step.\n",
        "\n",
        "Since RNN's maintain an internal state that depends on the prevours seen elements, given all the characters computed untill this moment, what is the next character?\n",
        "\n",
        "## Create training examples and targets\n",
        "\n",
        "Next devide the text into examples sequences. Each input sequence will containe `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets containe the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is 'Hello'. The input sequence would be 'Hell', and the target sequnce 'ello'.\n",
        "\n",
        "First use the `tf.data.Dataset.from_tensor_slices` fucntion to convert text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Dp6ZxJDNTP",
        "outputId": "68d2b431-5808-4f77-832b-e54b2b1e81db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVFduXRrR9te"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EXquCTSSRFs",
        "outputId": "1183aeee-1a62-4f66-f237-a9be89a7b004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56zpdaBJSfkZ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIPEFVaTXqz"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwB-JKsSTp1n",
        "outputId": "0402a03c-586f-4e62-8aab-fc316ca288b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjCgTZ4KUJCM"
      },
      "source": [
        "Is easier to see what this is doing if you join the tokens beack into strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFsBhlVCUWVs",
        "outputId": "4ce913a7-b578-46f1-db3b-598ac239398d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zezk9OoYUphl"
      },
      "source": [
        "For training we will need a dataset of `(input, label)` pairs. Where `input` and `label` are sequences. At each time stap the inpout is the current character and the label is the next character\n",
        "\n",
        "The following function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestamp:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXCaYaeoW_NJ"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsBNfmi7YIOL",
        "outputId": "9ab1e1d6-c5a1-4265-89e6-9dede5075ae0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list('Tensorflow'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwSpQ7u7YSFZ"
      },
      "source": [
        "We can see that in the frist row we have the input without the target and in the secound row we have the target without the label, which makes input label pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNIMOA8aYqaW"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7HjIBPZ98O",
        "outputId": "8e9acf83-edbf-4e93-d36c-a8aea96d78f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input  : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input  :', text_from_ids(input_example).numpy())\n",
        "  print('Target :', text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuis5J_chWg"
      },
      "source": [
        "#### Create training batches\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzbXjv_Gc2Ww",
        "outputId": "d176e1e3-3768-4b5f-98c1-1823e3b09a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(63, 100), dtype=tf.int64, name=None), TensorSpec(shape=(63, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 63\n",
        "\n",
        "\"\"\"Buffer size to shuffle the dataset\n",
        "(TF data is designed to work wiht possobly infinite sequences,\n",
        "so it dosen't attempt to shuffle the entire sequence in memory. Instadfe\n",
        "it maintaines a buffer is which it shuffles elements)\"\"\"\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE) \\\n",
        "    .batch(BATCH_SIZE, drop_remainder=True) \\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYBu3sEAdp13"
      },
      "source": [
        "## Build The Model\n",
        "\n",
        "This section defines the model as `keras.Model` sublcass.\n",
        "\n",
        "The model have the following layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup label that will map each character-ID to a vector with `embedding_dim` dimensions.\n",
        "* `tf.keras.layers.GRU`: A type of RNN with seize `units=rnn_units` (We can also use LSTM layer here)\n",
        "* `tf.keras.layers.Debse`: The output layer, with `vocab_size` outputs. it outputs one logit for each character in vocabulary. There are the log-likehood of each character accoriding to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr3WxA5eahmR"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulart in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebPdL-nfL0z6"
      },
      "source": [
        "The class bellow does the following::\n",
        "  - We derive a class from the keras.Model\n",
        "  - The constructor is used to define the layers of the model\n",
        "  - We define the pass forward using the layers defined in the constructor\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFplKbBcMjeQ"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        \"\"\"since we are training a text generation model,\n",
        "        we use the previous state, in training. If there is no state,\n",
        "        then we initialize the state \"\"\"\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziyS14rrMjzP"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIlCxkcgMj3P"
      },
      "source": [
        "For each character the model looks up the embedding, runs GRU one timestamp with the embedding as input, and applies the dnese layer to generate logits predicting the top-likelihhood of the next character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMXiBgWMj6E"
      },
      "source": [
        "### Try The Model\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "Frist check the shape of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5WPE9fNMkEH",
        "outputId": "eaf58f98-385f-42b5-dde8-2e6f1283e4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(63, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx0Cc3YLlR0f"
      },
      "source": [
        "In the above example for sequence length of the input is `100` but the model can be run on inputs of any length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REmzH00Wluzd",
        "outputId": "9c060022-11d3-4ba1-8543-5003058dd0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKBVI1e2lyd5"
      },
      "source": [
        "To get the actual prediction from the model we need to sample from the output distribution, to get actual character indicies. The distribution is defined by the logits over hte character vocabulary\n",
        "\n",
        "Note: It is important to sample from the distribution as taking the argmax of the distribution can easily get the model stuck in a loop\n",
        "\n",
        "Try it for the first example in the batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_IS_mYTlyb0"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0],\n",
        "    num_samples = 1\n",
        ")\n",
        "\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwFY1SLHlyZ_"
      },
      "source": [
        "This gives us, at each timestep of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-RsjLk6lyWk",
        "outputId": "28f1a1c4-82bc-4730-e686-976e4b23eb66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([14, 34, 22, 37, 14,  8, 48, 25, 49, 30, 36, 37, 38, 26, 43, 53, 60,\n",
              "       49, 24,  8, 25,  4,  6, 43, 35, 33, 12, 26, 62, 53, 30, 38,  2,  8,\n",
              "        8, 25, 26, 46, 57,  5, 23, 39, 19,  8, 40,  9, 24, 47, 64, 65, 34,\n",
              "       46, 46, 22, 32, 51, 53, 32, 26, 23, 23, 52, 50, 54, 44, 34, 52, 43,\n",
              "        6, 44, 48, 52, 24, 29,  7, 23, 59, 58, 29, 11, 62, 16, 18, 27, 32,\n",
              "       22, 28, 19, 12, 25, 49, 62, 17, 47, 57, 58, 19, 61, 18, 65])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll68rpuGlySv"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLr3FP3JnoPi",
        "outputId": "b967f035-b743-4c2b-b3c9-09c9e3a716f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            "b'rd and a villain:\\nWhich to maintain I would allow him odds,\\nAnd meet him, were I tied to run afoot\\nE'\n",
            "Next Char Predictions:\n",
            "b\"AUIXA-iLjQWXYMdnujK-L$'dVT;MwnQY --LMgr&JZF-a.KhyzUggISlnSMJJmkoeUmd'eimKP,JtsP:wCENSIOF;LjwDhrsFvEz\"\n"
          ]
        }
      ],
      "source": [
        "print('Input:', text_from_ids(input_example_batch[0]).numpy(), sep='\\n')\n",
        "print('Next Char Predictions:', text_from_ids(sampled_indices).numpy(), sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA-_jtexua2S"
      },
      "source": [
        "From this output of the we can see that the model is untrained and don't understand the text and can't make accurate predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiXP97nqog9G"
      },
      "source": [
        "### Train The Model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN statem and the input this time step, predict the class of the next character\n",
        "\n",
        "#### Attach an optimizer, and loss function\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss funciton works in this case because it is applied across the last dimension of the predictions\n",
        "\n",
        "Beacuse the model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn54YyjBpQ-X"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giUC8DTasMmE",
        "outputId": "e569e73d-60e9-4aab-c719-ec1c71c00557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediciton shape: \n",
            "(63, 100, 66)\n",
            "# (batch_size, sequence_length, vocab_size\n",
            "New loss:         tf.Tensor(4.18988, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    'Prediciton shape: ',\n",
        "    example_batch_predictions.shape,\n",
        "    '# (batch_size, sequence_length, vocab_size',\n",
        "    sep='\\n'\n",
        ")\n",
        "print('New loss:        ', example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlUK-djCvi4_"
      },
      "source": [
        "A newly initialized model should't be to sure of itself, the output logits should all have similar magnitudes. To confirm this you can chekc that the exmponential of the mean loss is approximatly equal to the vocabulary size. A much higher loss means the model is sure of it's wrong answers and is badly initialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHoMcOISwsnx",
        "outputId": "24de1fe7-3c57-46ad-93ce-51b5f3703ee4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66.01486"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFJZb8NUxC2O"
      },
      "source": [
        "Note: This value can from the examples it was evaluated on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiYqEwy7w9SE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwNogo9dxoj4"
      },
      "source": [
        "#### Configure checkpoints\n",
        "We will use `tf.keras.callbacks.ModelChekpoint` to ensure that checkpoints are saved during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTZdbPuoxtsR"
      },
      "outputs": [],
      "source": [
        "# Path where tha checkpoints will be saved\n",
        "checkpoint_path = './training_checkpoints'\n",
        "# Name of the checkpoint file\n",
        "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt_(epoch)\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix, save_weight_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eer326nAy1GB"
      },
      "source": [
        "## Fit the Model\n",
        "To keep training time resonable, uses 10 epochs to train the model.\n",
        "\n",
        "Note: Set the runtime to GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOeO1vlzznTy",
        "outputId": "4c489f76-b5b6-4f10-cc87-4959480122f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "174/175 [============================>.] - ETA: 0s - loss: 2.6899"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 26s 88ms/step - loss: 2.6868\n",
            "Epoch 2/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.9593"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 17s 83ms/step - loss: 1.9593\n",
            "Epoch 3/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.6845"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 19s 87ms/step - loss: 1.6845\n",
            "Epoch 4/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.5293"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 16s 76ms/step - loss: 1.5293\n",
            "Epoch 5/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.4338"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 15s 74ms/step - loss: 1.4338\n",
            "Epoch 6/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.3672"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 15s 76ms/step - loss: 1.3672\n",
            "Epoch 7/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.3156"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 20s 104ms/step - loss: 1.3156\n",
            "Epoch 8/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.2699"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 16s 78ms/step - loss: 1.2699\n",
            "Epoch 9/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.2289"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 15s 75ms/step - loss: 1.2289\n",
            "Epoch 10/10\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.1870"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r175/175 [==============================] - 16s 81ms/step - loss: 1.1870\n",
            "CPU times: user 2min 36s, sys: 4.81 s, total: 2min 41s\n",
            "Wall time: 2min 56s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8kUHGyP2yqb"
      },
      "source": [
        "## Generate Text\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as we execute it.\n",
        "\n",
        "Each time we call the model we pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to countinue generating text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqjeqSlhZ4Zd"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbEpE7PJavZg"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybPv6YTka-1l"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the genereaated text, you'll see that model knows when to capitalize, make paragraps and imitates a Shakespare-like writing vocabulary. With the small number of training epochs, it has not yet learne to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHCxvpp1bynd",
        "outputId": "19910db4-9dcb-4a1c-c95c-36aa4f726a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "After our maintage of fair walls, not that they\n",
            "Batt, possess the remorwhal,\n",
            "Denied the Bolingbroke, do leave upon me A begganant\n",
            "May hence; and wetch of yestend\n",
            "But the most speech of mine oraward!\n",
            "\n",
            "CORIOLANUS:\n",
            "Because to keep your lady's doubhent.\n",
            "\n",
            "POLIXENSE:\n",
            "Who thou that bid me asswer mine inteem sprin,\n",
            "Or in my love behind thy beast with thee?\n",
            "\n",
            "KING RICHARD III:\n",
            "And learn their sue that please your grace:\n",
            "The perilloth was the valley, thither in half\n",
            "To straw thy will. Gentlemen, call Hall!\n",
            "\n",
            "Third Citizen:\n",
            "Not boin: and Montague,\n",
            "You dance what thou dost calm Cavil withmit thee:\n",
            "The headsine royal elpowers come?\n",
            "Ha! you be juiced in our master Antigonue,\n",
            "That hath woundshmance: be cross'd privately, you\n",
            "hither to her fairer with the feast wails Lecembard.\n",
            "\n",
            "LEONTES:\n",
            "The dure groos sidely be Mesel'd,\n",
            "And all thy earl of Heruly, and I think I am more ancher.\n",
            "\n",
            "CORIOLANUS:\n",
            "For this, I came both\n",
            "To go in our turn; we'll divide this old:\n",
            "The field with me both made the false policy,\n",
            "Ro  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.1108763217926025\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkRXL_FJdOe_"
      },
      "source": [
        "The easies thing you can do to imporve the results is to trian it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "We can also experiment with diffenret start stirng, try adding another RNN layer to imporve the model's accuracy, or adjust the temperature parameter to generate more or less random predictons.\n",
        "\n",
        "If we want the model to generate text faster the easiest thing we can do is batch the next generation in the example below the model generates 5 output in about the same time it took to generate 1 above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBAZDqcQePDy",
        "outputId": "95ee6c2a-a15a-4c92-ff63-4f54182f7c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nNot to the County Pire here, sir, if Wert me not\\nAnd let your brother's worse.\\n\\nLUCENTIO:\\nWhat is't not the country: we changed spite!\\nCome to your weeping, and my name be plainment\\nAnd dear sons weally: back him Romeo in brother,\\nAnother drownting against the earth;\\nA troophood any monaward of Madgeal spirit,\\nAnd creph in so secrew'd him, she doubt her unbeg,\\nThe want of England stives are we remeth\\nWith which and every godness from deep in them;\\nAnd pity her town what't in haste,\\nThat our king great kins without your hands,\\nMay chanced in your noteous to our people.\\n\\nANGELO:\\nClaudio; for Gerries and be bliss.\\nBut let me let embrech's thy deadife!\\n\\nBENVOLIO:\\nWhat is the haste for while we doad, like a food?\\n\\nHASTINGS:\\nWhy none of the maid, my weven wants\\nThat then us does me; for tell the presence challen\\nCall home.\\n\\nBUCKINGHAM:\\nYour grace is mady to be nor wrick'd in coronation.\\nDurpe it is to earnesh; cold son, you have some crown,\\nDore envying early me more injured;\\nFor here shal\"\n",
            " b\"ROMEO:\\nThis worthy gates, contracious part of sevenjey's ass'?\\n\\nTYBULTIO:\\nFor she hath done shown the case, if thou canst grow him\\nAs thou have lost, which be spoken'd without counsel?\\n\\nISABELLA:\\nThy ghostly common will.\\n\\nDUKE OF AUMERLE:\\nFellow, this Kate? why look you not to see this dayful ready?\\nSee, have I went what, thou hast won\\nMade a cause, thruct I call'd.\\n\\nPOLIXENES:\\nI will. know you!\\n\\nDUKE OF YORK:\\nThis is as Lould and Have I do?\\n\\nTYBALT:\\nAy, sir; and be I tell you; Warwick to deliver\\nYou have confident to the her forth of you\\nwell seem to bear her pitied that; but yet,\\nMy name is common us: 'twas a happy counsel,\\nWhere behold those course of a thousand wicked hands\\nBut to day she straith, to the king have a looker\\nberet: and as is my heart, as you sleep!\\n\\nGEORGE:\\nO, with the work the aight you seems,\\nAnd then; it nather tooch the rain of work,\\nWhy accuped in Rosemband, and God perchunt in scorton\\nWill entreat your pulloss breathed a house blood.\\n\\nAUTOLYCUS:\\nNot to say to put un\"\n",
            " b\"ROMEO:\\nThat thou devisciaves.\\nThus, and so you'll you\\nchange died.\\n\\nPOMPHY:\\nGod in renowned with thy land.\\nThe lands'd ones let me sit, and go many best\\nThan the drops of such criese, shall know\\nThe Volsces hearing thee, have weather\\nThat thus to lancishmen to the abemblances\\nThus have I enter'd: with!\\n\\nJULIET:\\nHow! Villa, I learn the sanctuay?\\n\\nTYor:\\nWell, you no whom it;\\nAnd Bolingbroke hath made for our pectress he\\nThe maid of Marret; and is. I have done:\\nThe nums of Marcius, grantageneth or twill prove\\nAll in grace: fair wanthour in thy life! A sight\\nof whop: here;\\n'Tis very well; but so dream'st true wonder'd.\\nTurn you till metch her!\\n\\nVOLUMNIA:\\nLet me hear me talk.\\nBut she is worthy men: 'tis a wood doke out again,\\nand after it is.' What fearful Diccoster shall\\nI were alike: Marcius wise!\\n\\nDUKE VINCENTIO:\\nYou, ay, Withal we beaz\\nThe grandfy thousand sweet and scorn as gries,\\nMyself dust, young sweet lady, and her above hate,\\nAnd help to see, and contrise thy cold crown,\\nThere is my ha\"\n",
            " b\"ROMEO:\\nHair tells, set down.\\n\\nDUKE VINCENTIO:\\nThat 'twere I cannot, she will keep thee,\\nWe'll meet thee sight.\\n\\nWillom:\\nYet I'll prophecy my love: 'I'll pluck\\nnot to be housedou, Warwick was here:\\nDo on by him, though not been Dorious, God, to young hither.\\nThe consuasors of you might still apparent\\nTo the maiden of my life, or weaton.\\n\\nROTES:\\n\\nGLOUCESTER:\\nAmong your joyfully:\\nSwe'll let; sweet Claudio, Toming, our late took as go:\\nWhich yet hath made my news upon myself\\nWhen she durse my spreech upon the king.\\n\\nPETRUCHIO:\\nNay, be rother thy overmustice\\npursed it with the commoss shalt see their worms.\\n\\nKATHARINA:\\nWhat, sister, whither do you give\\nAgainst her wear-bed. They are not;\\nAnd we have conceived you marry your mistress,--\\n\\nClifford:\\nFor strength and fears and crumbers comes here\\nI cannot be another's look'd--\\nAs come be made:' conjacent: he hat haste hase?\\nDuke, maiden, peace to your ends most grain,\\nLet thereou--mark'd o'er-range to the world,\\nAnd hell wherein your father and not \"\n",
            " b\"ROMEO:\\nNeeds might here left him all;\\nAnd he's a crown, to dry to a cense;\\nMy fresh any butterformine store,\\nWho percent in question, it gentleman,\\nIn that thy pleaturies to serve a leant have any\\nthirtt and open marriage; so us too much,\\nFor one claporous Vailon husband.\\n\\nMENENIUS:\\nHe cannot be curved on.\\nWhat cradural comfort much would proceed agree and to prince?\\n\\nKING RICHARD II:\\nHow she kill'd soul's name?\\n\\nNeTARUS:\\nBe it to defend me duke to the Tower.\\n\\nThird Servant:\\nAnd Signior Lord Have I have the world, and not our pest\\ndestrudy; we'll have I protecton'd doad,\\nMaster in the strest, and beding too;\\nMade parted in the sant and tired.\\n\\nHENRY BOLINGBROKE:\\nMy liege is, sir: but eyes bear thy strong frian; they were\\nbut by the preciped of your master enter'd; thus doth devise,\\nWill'd played here, and Pompey with him what; I\\ngnaw'd it like to let this rebelies;\\nAnd take my brother's shows were woman's sovereign's,\\nOr, in reprieve of her bearing with it,\\nFor I were a dear life sels? \\nMIS\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time 4.116111516952515\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO', 'ROMEO', 'ROMEO', 'ROMEO', 'ROMEO' ])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + \"_\" * 80)\n",
        "print(\"\\nRun time\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-W2aUwafLQF"
      },
      "source": [
        "## Export the generator\n",
        "This signal-step nodel can easily be saved and restored, allowing you to use it anywhere a `tf.saved_model` is accepted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1VfBnmYfo-Z",
        "outputId": "9eee1a48-aa75-4853-c33f-4654e7a0dc99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7a4c88555fc0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# save the one step model\n",
        "tf.saved_model.save(one_step_model, 'one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R72cmUybfph3"
      },
      "outputs": [],
      "source": [
        "# reload the model\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfQLT-xwrLcf",
        "outputId": "c940559c-aa18-4823-96a0-9de9d58fefe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Dogs match, and 'maritom'd you, if my sole duke the world,\n",
            "That she your promises: come upon that \n"
          ]
        }
      ],
      "source": [
        "states =  None\n",
        "next_char = tf.constant(['ROMEO'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIxQh3kVr3da"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but it does not give us much control, it uses teacher-forcing which prevents bad predicitons from being fed back to the model, so the model never learns to recover form mistakes.\n",
        "\n",
        "So now that we've seen how to run the model manualy next we can implement the tarining loop. This gives a starting point if, for example, we want to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function\n",
        "\n",
        "We use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/basics).\n",
        "\n",
        "The basic procedure is\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and aply them to the model using optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyqgLJAXr3Zi"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAkW6KkI1Nrt"
      },
      "source": [
        "The above implementation of the `train_step` method follows `Keras train_step conventions`. This is optional, but it allows us to change the begavior of the train step and still use keras `Model.cmpile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gkMieOT2ZXx"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcCkKXiU2yN9"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzIy9lLq3OET",
        "outputId": "001f2a4d-ad13-4f0b-c1f7-4aef8b84ad1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "175/175 [==============================] - 15s 57ms/step - loss: 2.7085\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a4c606ddde0>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9Pbaxk-QjR_r",
        "78gjtFMilg13",
        "W3V7dXcGpNZV",
        "SY_8YQ5Jp1wr",
        "V-WJ97BEDNPD",
        "rNuis5J_chWg",
        "zYBu3sEAdp13",
        "VgMXiBgWMj6E",
        "qiXP97nqog9G",
        "XwNogo9dxoj4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
