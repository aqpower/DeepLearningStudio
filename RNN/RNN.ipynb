{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import opencc\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "max_length = 99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            paragraphs = poetry.get(\"paragraphs\")\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    src_path = Path(\"./data/chinese-poetry-master/全唐诗/\")\n",
    "    for file_path in src_path.glob(\"poet.tang*\"):\n",
    "        data.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "        # data.extend(handleJson(file_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")  # All if author=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3514\n",
      "data_size 1206\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    # Find the maximum target length\n",
    "    max_target_len = max([t.size(0) for t in targets])\n",
    "    # Pad targets to the maximum length\n",
    "    padded_targets = torch.stack(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                t, (0, max_target_len - t.size(0)), \"constant\", word_to_index[\"<START>\"]\n",
    "            )\n",
    "            for t in targets\n",
    "        ]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )  # Enable batch_first\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # Adjusted for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Adjusted view for batch processing, removing hard-coded lengths\n",
    "        output = self.linear1(F.relu(lstm_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        # output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        # Reshape output to (batch_size, seq_len, vocab_size) for compatibility\n",
    "        output = output.view(input.size(0), input.size(1), -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoetryModel(len(word_to_index), 256, 256)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.NLLLoss(\n",
    "    ignore_index=word_to_index[\"<START>\"], reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 00000/00302 | Loss: 8.1615\n",
      "Epoch: 001/050 | Batch 00100/00302 | Loss: 5.7644\n",
      "Epoch: 001/050 | Batch 00200/00302 | Loss: 6.0068\n",
      "Epoch: 001/050 | Batch 00300/00302 | Loss: 6.0886\n",
      "Epoch: 002/050 | Batch 00000/00302 | Loss: 5.8192\n",
      "Epoch: 002/050 | Batch 00100/00302 | Loss: 5.6160\n",
      "Epoch: 002/050 | Batch 00200/00302 | Loss: 5.6990\n",
      "Epoch: 002/050 | Batch 00300/00302 | Loss: 5.6606\n",
      "Epoch: 003/050 | Batch 00000/00302 | Loss: 5.4652\n",
      "Epoch: 003/050 | Batch 00100/00302 | Loss: 5.3944\n",
      "Epoch: 003/050 | Batch 00200/00302 | Loss: 5.3430\n",
      "Epoch: 003/050 | Batch 00300/00302 | Loss: 5.4390\n",
      "Epoch: 004/050 | Batch 00000/00302 | Loss: 5.0503\n",
      "Epoch: 004/050 | Batch 00100/00302 | Loss: 5.1681\n",
      "Epoch: 004/050 | Batch 00200/00302 | Loss: 5.4184\n",
      "Epoch: 004/050 | Batch 00300/00302 | Loss: 5.2579\n",
      "Epoch: 005/050 | Batch 00000/00302 | Loss: 5.2254\n",
      "Epoch: 005/050 | Batch 00100/00302 | Loss: 5.0910\n",
      "Epoch: 005/050 | Batch 00200/00302 | Loss: 5.0171\n",
      "Epoch: 005/050 | Batch 00300/00302 | Loss: 5.0612\n",
      "Epoch: 006/050 | Batch 00000/00302 | Loss: 4.8769\n",
      "Epoch: 006/050 | Batch 00100/00302 | Loss: 4.8542\n",
      "Epoch: 006/050 | Batch 00200/00302 | Loss: 5.2087\n",
      "Epoch: 006/050 | Batch 00300/00302 | Loss: 5.0762\n",
      "Epoch: 007/050 | Batch 00000/00302 | Loss: 4.9845\n",
      "Epoch: 007/050 | Batch 00100/00302 | Loss: 5.0564\n",
      "Epoch: 007/050 | Batch 00200/00302 | Loss: 4.9912\n",
      "Epoch: 007/050 | Batch 00300/00302 | Loss: 4.9981\n",
      "Epoch: 008/050 | Batch 00000/00302 | Loss: 4.6432\n",
      "Epoch: 008/050 | Batch 00100/00302 | Loss: 4.6798\n",
      "Epoch: 008/050 | Batch 00200/00302 | Loss: 4.6568\n",
      "Epoch: 008/050 | Batch 00300/00302 | Loss: 4.6239\n",
      "Epoch: 009/050 | Batch 00000/00302 | Loss: 4.7914\n",
      "Epoch: 009/050 | Batch 00100/00302 | Loss: 4.5560\n",
      "Epoch: 009/050 | Batch 00200/00302 | Loss: 4.2895\n",
      "Epoch: 009/050 | Batch 00300/00302 | Loss: 4.9631\n",
      "Epoch: 010/050 | Batch 00000/00302 | Loss: 4.9856\n",
      "Epoch: 010/050 | Batch 00100/00302 | Loss: 4.9304\n",
      "Epoch: 010/050 | Batch 00200/00302 | Loss: 5.0783\n",
      "Epoch: 010/050 | Batch 00300/00302 | Loss: 4.9443\n",
      "Epoch: 011/050 | Batch 00000/00302 | Loss: 4.5965\n",
      "Epoch: 011/050 | Batch 00100/00302 | Loss: 4.6888\n",
      "Epoch: 011/050 | Batch 00200/00302 | Loss: 4.4581\n",
      "Epoch: 011/050 | Batch 00300/00302 | Loss: 4.0538\n",
      "Epoch: 012/050 | Batch 00000/00302 | Loss: 5.2394\n",
      "Epoch: 012/050 | Batch 00100/00302 | Loss: 4.0803\n",
      "Epoch: 012/050 | Batch 00200/00302 | Loss: 4.3822\n",
      "Epoch: 012/050 | Batch 00300/00302 | Loss: 4.6325\n",
      "Epoch: 013/050 | Batch 00000/00302 | Loss: 4.1364\n",
      "Epoch: 013/050 | Batch 00100/00302 | Loss: 4.4609\n",
      "Epoch: 013/050 | Batch 00200/00302 | Loss: 4.7101\n",
      "Epoch: 013/050 | Batch 00300/00302 | Loss: 4.9544\n",
      "Epoch: 014/050 | Batch 00000/00302 | Loss: 4.2733\n",
      "Epoch: 014/050 | Batch 00100/00302 | Loss: 4.5812\n",
      "Epoch: 014/050 | Batch 00200/00302 | Loss: 4.0057\n",
      "Epoch: 014/050 | Batch 00300/00302 | Loss: 4.6744\n",
      "Epoch: 015/050 | Batch 00000/00302 | Loss: 4.2182\n",
      "Epoch: 015/050 | Batch 00100/00302 | Loss: 4.6126\n",
      "Epoch: 015/050 | Batch 00200/00302 | Loss: 4.3814\n",
      "Epoch: 015/050 | Batch 00300/00302 | Loss: 4.2860\n",
      "Epoch: 016/050 | Batch 00000/00302 | Loss: 4.7773\n",
      "Epoch: 016/050 | Batch 00100/00302 | Loss: 3.8445\n",
      "Epoch: 016/050 | Batch 00200/00302 | Loss: 4.0823\n",
      "Epoch: 016/050 | Batch 00300/00302 | Loss: 4.4497\n",
      "Epoch: 017/050 | Batch 00000/00302 | Loss: 3.9608\n",
      "Epoch: 017/050 | Batch 00100/00302 | Loss: 4.3466\n",
      "Epoch: 017/050 | Batch 00200/00302 | Loss: 4.3170\n",
      "Epoch: 017/050 | Batch 00300/00302 | Loss: 4.5521\n",
      "Epoch: 018/050 | Batch 00000/00302 | Loss: 4.1091\n",
      "Epoch: 018/050 | Batch 00100/00302 | Loss: 4.6329\n",
      "Epoch: 018/050 | Batch 00200/00302 | Loss: 3.9891\n",
      "Epoch: 018/050 | Batch 00300/00302 | Loss: 4.1830\n",
      "Epoch: 019/050 | Batch 00000/00302 | Loss: 4.2527\n",
      "Epoch: 019/050 | Batch 00100/00302 | Loss: 4.2265\n",
      "Epoch: 019/050 | Batch 00200/00302 | Loss: 3.9236\n",
      "Epoch: 019/050 | Batch 00300/00302 | Loss: 3.6934\n",
      "Epoch: 020/050 | Batch 00000/00302 | Loss: 3.9742\n",
      "Epoch: 020/050 | Batch 00100/00302 | Loss: 3.9888\n",
      "Epoch: 020/050 | Batch 00200/00302 | Loss: 4.2562\n",
      "Epoch: 020/050 | Batch 00300/00302 | Loss: 3.9285\n",
      "Epoch: 021/050 | Batch 00000/00302 | Loss: 4.1067\n",
      "Epoch: 021/050 | Batch 00100/00302 | Loss: 3.6258\n",
      "Epoch: 021/050 | Batch 00200/00302 | Loss: 3.8610\n",
      "Epoch: 021/050 | Batch 00300/00302 | Loss: 3.8572\n",
      "Epoch: 022/050 | Batch 00000/00302 | Loss: 4.1417\n",
      "Epoch: 022/050 | Batch 00100/00302 | Loss: 3.7614\n",
      "Epoch: 022/050 | Batch 00200/00302 | Loss: 4.2043\n",
      "Epoch: 022/050 | Batch 00300/00302 | Loss: 4.2976\n",
      "Epoch: 023/050 | Batch 00000/00302 | Loss: 3.9968\n",
      "Epoch: 023/050 | Batch 00100/00302 | Loss: 3.8405\n",
      "Epoch: 023/050 | Batch 00200/00302 | Loss: 3.9123\n",
      "Epoch: 023/050 | Batch 00300/00302 | Loss: 3.5959\n",
      "Epoch: 024/050 | Batch 00000/00302 | Loss: 3.6569\n",
      "Epoch: 024/050 | Batch 00100/00302 | Loss: 3.9736\n",
      "Epoch: 024/050 | Batch 00200/00302 | Loss: 3.3572\n",
      "Epoch: 024/050 | Batch 00300/00302 | Loss: 4.8478\n",
      "Epoch: 025/050 | Batch 00000/00302 | Loss: 3.9137\n",
      "Epoch: 025/050 | Batch 00100/00302 | Loss: 3.5970\n",
      "Epoch: 025/050 | Batch 00200/00302 | Loss: 3.8751\n",
      "Epoch: 025/050 | Batch 00300/00302 | Loss: 4.7295\n",
      "Epoch: 026/050 | Batch 00000/00302 | Loss: 3.7993\n",
      "Epoch: 026/050 | Batch 00100/00302 | Loss: 3.9592\n",
      "Epoch: 026/050 | Batch 00200/00302 | Loss: 4.7068\n",
      "Epoch: 026/050 | Batch 00300/00302 | Loss: 3.2481\n",
      "Epoch: 027/050 | Batch 00000/00302 | Loss: 4.3676\n",
      "Epoch: 027/050 | Batch 00100/00302 | Loss: 3.4088\n",
      "Epoch: 027/050 | Batch 00200/00302 | Loss: 4.0291\n",
      "Epoch: 027/050 | Batch 00300/00302 | Loss: 4.3301\n",
      "Epoch: 028/050 | Batch 00000/00302 | Loss: 3.4422\n",
      "Epoch: 028/050 | Batch 00100/00302 | Loss: 3.0654\n",
      "Epoch: 028/050 | Batch 00200/00302 | Loss: 3.9029\n",
      "Epoch: 028/050 | Batch 00300/00302 | Loss: 3.9853\n",
      "Epoch: 029/050 | Batch 00000/00302 | Loss: 3.6778\n",
      "Epoch: 029/050 | Batch 00100/00302 | Loss: 4.1378\n",
      "Epoch: 029/050 | Batch 00200/00302 | Loss: 3.4424\n",
      "Epoch: 029/050 | Batch 00300/00302 | Loss: 4.0392\n",
      "Epoch: 030/050 | Batch 00000/00302 | Loss: 3.4047\n",
      "Epoch: 030/050 | Batch 00100/00302 | Loss: 4.3382\n",
      "Epoch: 030/050 | Batch 00200/00302 | Loss: 3.5707\n",
      "Epoch: 030/050 | Batch 00300/00302 | Loss: 4.0096\n",
      "Epoch: 031/050 | Batch 00000/00302 | Loss: 3.5741\n",
      "Epoch: 031/050 | Batch 00100/00302 | Loss: 3.1118\n",
      "Epoch: 031/050 | Batch 00200/00302 | Loss: 3.5867\n",
      "Epoch: 031/050 | Batch 00300/00302 | Loss: 3.4837\n",
      "Epoch: 032/050 | Batch 00000/00302 | Loss: 3.2502\n",
      "Epoch: 032/050 | Batch 00100/00302 | Loss: 3.6624\n",
      "Epoch: 032/050 | Batch 00200/00302 | Loss: 3.4628\n",
      "Epoch: 032/050 | Batch 00300/00302 | Loss: 3.6767\n",
      "Epoch: 033/050 | Batch 00000/00302 | Loss: 3.8145\n",
      "Epoch: 033/050 | Batch 00100/00302 | Loss: 3.5061\n",
      "Epoch: 033/050 | Batch 00200/00302 | Loss: 3.6653\n",
      "Epoch: 033/050 | Batch 00300/00302 | Loss: 3.9636\n",
      "Epoch: 034/050 | Batch 00000/00302 | Loss: 3.1280\n",
      "Epoch: 034/050 | Batch 00100/00302 | Loss: 3.1915\n",
      "Epoch: 034/050 | Batch 00200/00302 | Loss: 2.7924\n",
      "Epoch: 034/050 | Batch 00300/00302 | Loss: 3.1978\n",
      "Epoch: 035/050 | Batch 00000/00302 | Loss: 2.6878\n",
      "Epoch: 035/050 | Batch 00100/00302 | Loss: 3.1334\n",
      "Epoch: 035/050 | Batch 00200/00302 | Loss: 3.8334\n",
      "Epoch: 035/050 | Batch 00300/00302 | Loss: 3.9363\n",
      "Epoch: 036/050 | Batch 00000/00302 | Loss: 3.4132\n",
      "Epoch: 036/050 | Batch 00100/00302 | Loss: 3.0882\n",
      "Epoch: 036/050 | Batch 00200/00302 | Loss: 2.9128\n",
      "Epoch: 036/050 | Batch 00300/00302 | Loss: 3.2046\n",
      "Epoch: 037/050 | Batch 00000/00302 | Loss: 2.7907\n",
      "Epoch: 037/050 | Batch 00100/00302 | Loss: 2.8096\n",
      "Epoch: 037/050 | Batch 00200/00302 | Loss: 3.8060\n",
      "Epoch: 037/050 | Batch 00300/00302 | Loss: 3.2864\n",
      "Epoch: 038/050 | Batch 00000/00302 | Loss: 3.3303\n",
      "Epoch: 038/050 | Batch 00100/00302 | Loss: 3.3558\n",
      "Epoch: 038/050 | Batch 00200/00302 | Loss: 3.0226\n",
      "Epoch: 038/050 | Batch 00300/00302 | Loss: 3.2464\n",
      "Epoch: 039/050 | Batch 00000/00302 | Loss: 3.1734\n",
      "Epoch: 039/050 | Batch 00100/00302 | Loss: 3.1394\n",
      "Epoch: 039/050 | Batch 00200/00302 | Loss: 2.9915\n",
      "Epoch: 039/050 | Batch 00300/00302 | Loss: 3.7274\n",
      "Epoch: 040/050 | Batch 00000/00302 | Loss: 2.5902\n",
      "Epoch: 040/050 | Batch 00100/00302 | Loss: 2.8569\n",
      "Epoch: 040/050 | Batch 00200/00302 | Loss: 3.7372\n",
      "Epoch: 040/050 | Batch 00300/00302 | Loss: 3.6494\n",
      "Epoch: 041/050 | Batch 00000/00302 | Loss: 3.1853\n",
      "Epoch: 041/050 | Batch 00100/00302 | Loss: 2.8688\n",
      "Epoch: 041/050 | Batch 00200/00302 | Loss: 3.9175\n",
      "Epoch: 041/050 | Batch 00300/00302 | Loss: 3.3332\n",
      "Epoch: 042/050 | Batch 00000/00302 | Loss: 2.8020\n",
      "Epoch: 042/050 | Batch 00100/00302 | Loss: 3.0471\n",
      "Epoch: 042/050 | Batch 00200/00302 | Loss: 3.2869\n",
      "Epoch: 042/050 | Batch 00300/00302 | Loss: 3.0187\n",
      "Epoch: 043/050 | Batch 00000/00302 | Loss: 3.5259\n",
      "Epoch: 043/050 | Batch 00100/00302 | Loss: 3.2314\n",
      "Epoch: 043/050 | Batch 00200/00302 | Loss: 2.9069\n",
      "Epoch: 043/050 | Batch 00300/00302 | Loss: 3.1053\n",
      "Epoch: 044/050 | Batch 00000/00302 | Loss: 2.0794\n",
      "Epoch: 044/050 | Batch 00100/00302 | Loss: 2.6955\n",
      "Epoch: 044/050 | Batch 00200/00302 | Loss: 3.6030\n",
      "Epoch: 044/050 | Batch 00300/00302 | Loss: 2.9048\n",
      "Epoch: 045/050 | Batch 00000/00302 | Loss: 3.4238\n",
      "Epoch: 045/050 | Batch 00100/00302 | Loss: 3.6003\n",
      "Epoch: 045/050 | Batch 00200/00302 | Loss: 2.5218\n",
      "Epoch: 045/050 | Batch 00300/00302 | Loss: 3.4740\n",
      "Epoch: 046/050 | Batch 00000/00302 | Loss: 2.3224\n",
      "Epoch: 046/050 | Batch 00100/00302 | Loss: 2.3749\n",
      "Epoch: 046/050 | Batch 00200/00302 | Loss: 2.2950\n",
      "Epoch: 046/050 | Batch 00300/00302 | Loss: 3.1499\n",
      "Epoch: 047/050 | Batch 00000/00302 | Loss: 2.9664\n",
      "Epoch: 047/050 | Batch 00100/00302 | Loss: 2.8467\n",
      "Epoch: 047/050 | Batch 00200/00302 | Loss: 2.2224\n",
      "Epoch: 047/050 | Batch 00300/00302 | Loss: 2.5106\n",
      "Epoch: 048/050 | Batch 00000/00302 | Loss: 2.7242\n",
      "Epoch: 048/050 | Batch 00100/00302 | Loss: 3.1159\n",
      "Epoch: 048/050 | Batch 00200/00302 | Loss: 3.2449\n",
      "Epoch: 048/050 | Batch 00300/00302 | Loss: 3.1779\n",
      "Epoch: 049/050 | Batch 00000/00302 | Loss: 3.1606\n",
      "Epoch: 049/050 | Batch 00100/00302 | Loss: 3.0928\n",
      "Epoch: 049/050 | Batch 00200/00302 | Loss: 2.8801\n",
      "Epoch: 049/050 | Batch 00300/00302 | Loss: 3.0869\n",
      "Epoch: 050/050 | Batch 00000/00302 | Loss: 3.2901\n",
      "Epoch: 050/050 | Batch 00100/00302 | Loss: 3.0944\n",
      "Epoch: 050/050 | Batch 00200/00302 | Loss: 1.8710\n",
      "Epoch: 050/050 | Batch 00300/00302 | Loss: 3.2266\n"
     ]
    }
   ],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (t, o) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=t.size(0))\n",
    "            output, hidden = model(t.to(device), hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), o.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "    torch.save(model.state_dict(), \"poetry-gen.pth\")\n",
    "\n",
    "\n",
    "train(model, num_epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"poetry-gen.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['江']\n",
      "水: -3.2523\n",
      "山: -3.6810\n",
      "上: -3.6984\n",
      "江山\n",
      "一: -1.8605\n",
      "白: -2.3346\n",
      "东: -2.6775\n",
      "江山白\n",
      "日: -1.1718\n",
      "玉: -2.1024\n",
      "云: -2.7929\n",
      "江山白日\n",
      "月: -0.9610\n",
      "夜: -2.6997\n",
      "暮: -2.9791\n",
      "江山白日暮\n",
      "月: -2.0810\n",
      "高: -2.2924\n",
      "，: -2.4153\n",
      "江山白日暮，\n",
      "水: -1.5942\n",
      "明: -3.2785\n",
      "白: -3.3700\n",
      "江山白日暮，白\n",
      "云: -2.0098\n",
      "日: -2.4484\n",
      "马: -2.5727\n",
      "江山白日暮，白马\n",
      "愁: -3.5174\n",
      "空: -3.5805\n",
      "飞: -3.5959\n",
      "江山白日暮，白马愁\n",
      "寒: -2.0720\n",
      "飞: -2.5286\n",
      "边: -3.4779\n",
      "江山白日暮，白马愁飞\n",
      "天: -2.6075\n",
      "空: -2.7734\n",
      "飞: -3.0239\n",
      "江山白日暮，白马愁飞飞\n",
      "。: -0.0008\n",
      "，: -8.8518\n",
      "天: -9.7263\n",
      "江山白日暮，白马愁飞飞。\n",
      "<EOP>: -1.6649\n",
      "君: -3.3274\n",
      "白: -3.5796\n",
      "江山白日暮，白马愁飞飞。\n"
     ]
    }
   ],
   "source": [
    "def make_one_hot_vec_target(word, word_to_index):\n",
    "    rst = autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "    return rst\n",
    "\n",
    "\n",
    "def generate_text(start_word=\"<START>\", top_k=1):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for word in words:\n",
    "            input_vector = make_one_hot_vec_target(word, word_to_index).unsqueeze(0)\n",
    "            model(input_vector.to(device), hidden_state)\n",
    "            generated_text += word\n",
    "\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output, hidden_state = model(input_vector.to(device), hidden_state)\n",
    "            top_values, top_indices = output.data.topk(top_k)\n",
    "\n",
    "            if top_k == 1:\n",
    "                selected_index = top_indices.item()\n",
    "            else:\n",
    "                top_indices = top_indices.squeeze()\n",
    "                top_values = top_values.squeeze()\n",
    "                top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "                top_probs = top_values.tolist()\n",
    "                # For demonstration, print the top_k words and their probabilities\n",
    "                # print(f\"Top {top_k} words and their probabilities:\")\n",
    "                for word, prob in zip(top_words, top_probs):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "                \n",
    "                top_indices = top_indices.squeeze()\n",
    "                selected_index = top_indices[random.randint(0, top_k - 1)].item()\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            print(generated_text)\n",
    "            input_vector = make_one_hot_vec_target(next_word, word_to_index).unsqueeze(0)\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "# print(generate_text(\"月夜\", top_k=2))\n",
    "# print(generate_text(\"山\"))\n",
    "# print(generate_text(\"烟\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
