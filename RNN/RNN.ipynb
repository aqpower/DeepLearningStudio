{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 128\n",
    "max_length = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3482\n",
      "data_size 1287\n"
     ]
    }
   ],
   "source": [
    "with open(\"poems.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    poems = json.load(f)\n",
    "\n",
    "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "poems = [list(poem) + [\"<EOP>\"] for poem in poems]\n",
    "index_tensors = {\n",
    "    word: torch.LongTensor([word_to_index[word]]) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence):\n",
    "\n",
    "    inputs = [index_tensors[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [index_tensors[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_targets = nn.utils.rnn.pad_sequence(\n",
    "        targets, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 在循环神经网络（RNN）中\n",
    "        # 当前时刻的隐藏状态是由当前时刻的输入和上一个时刻的隐藏状态共同决定的。\n",
    "        self.input_to_hidden = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.tanh(self.input_to_hidden(combined))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = RNN(embedding_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        batch_size, seq_len, _ = embeds.size()\n",
    "        outputs = []\n",
    "        for i in range(seq_len):\n",
    "            hidden = self.rnn(embeds[:, i, :], hidden)\n",
    "            outputs.append(hidden)\n",
    "        rnn_out = torch.stack(outputs, dim=1)\n",
    "        output = self.linear1(F.relu(rnn_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        output = self.softmax(output)\n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return torch.zeros(batch_size, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, scheduler, vocab_size):\n",
    "    log_dict = {\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"train_perplexity_per_epoch\": [],\n",
    "    }\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=inputs.size(0))\n",
    "            output, hidden = model(inputs.to(device), hidden)\n",
    "\n",
    "            # print(output.shape, targets.shape)\n",
    "            # torch.Size([16, 120, 3482]) torch.Size([16, 120])\n",
    "            # print(output.view(-1, vocab_size).shape, targets.view(-1).shape)\n",
    "            # torch.Size([1920, 3482]) torch.Size([1920])\n",
    "            # 使用view函数调整输出和目标的形状以匹配损失函数的期望输入\n",
    "            # output的原始形状是[批次大小, 序列长度, 词汇表大小]，targets的原始形状是[批次大小, 序列长度]\n",
    "            # view(-1, vocab_size)将output重塑为[批次大小*序列长度, 词汇表大小]，以匹配每个时间步的预测\n",
    "            # targets通过view(-1)被重塑为[批次大小*序列长度]，这样每个预测都有一个对应的目标值\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx + 1:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        log_dict[\"train_loss_per_epoch\"].append(avg_loss)\n",
    "        log_dict[\"train_perplexity_per_epoch\"].append(perplexity)\n",
    "\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoetryModel(\n",
       "  (embeddings): Embedding(3482, 256)\n",
       "  (rnn): RNN(\n",
       "    (input_to_hidden): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=3482, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_training_stats(log_dict):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_dict[\"train_loss_per_epoch\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_dict[\"train_perplexity_per_epoch\"], label=\"Training Perplexity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training Perplexity\")\n",
    "    plt.savefig(\"training_stats.svg\")\n",
    "    plt.show()\n",
    "\n",
    "model = PoetryModel(vocab_size=len(word_to_index), embedding_dim=256, hidden_dim=512)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"], reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=9, verbose=True\n",
    ")\n",
    "# log_dict = train(\n",
    "#     model, num_epochs, data_loader, optimizer, criterion, scheduler, vocab_size\n",
    "# )\n",
    "# plot_training_stats(log_dict)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长安一片月，万户捣衣声。秋风吹不尽，总是玉关情。何日平胡虏，良人罢远征。\n",
      "江海多豪气，朝廷有直声。何言马蹄下，一旦是佳城。\n",
      "月皎昭阳殿，霜清长信宫。天行乘玉辇，飞燕与君同。\n",
      "泉眼不清光殿人来，绮中有酒声。一餐不平意，无情终与期。清景既为山，天地同沙石。虎当风日好，云外遶碧峰。至今还山上，黄叶落岩霜。路向高僧望，云绕万重风。\n",
      "日暮景太粉，花房春水流。天涯一挥桃，发我欲南襟。十二吟轻命，楚臣亦满阴。君王不可见，志宠有松列。吾将相交去，回首醉酒船。每不能老手，百下泪赏东。出入无花好，宛人来尚游。二年非玉手，白日夜月色。江风调醉日，流粉同归心。昔时亦无心，缅然皆乡天。一壶余阴起，万古\n",
      "烟: 0.6155\n",
      "吹: 0.2331\n",
      "露: 0.1514\n",
      "风\n",
      "纪: 0.8845\n",
      "里: 0.0661\n",
      "起: 0.0494\n",
      "风烟\n",
      "南: 0.9528\n",
      "江: 0.0261\n",
      "海: 0.0212\n",
      "风烟纪\n",
      "城: 0.9897\n",
      "山: 0.0065\n",
      "都: 0.0038\n",
      "风烟纪南\n",
      "，: 0.9954\n",
      "。: 0.0034\n",
      "头: 0.0012\n",
      "风烟纪南城\n",
      "尘: 0.9759\n",
      "水: 0.0173\n",
      "旌: 0.0069\n",
      "风烟纪南城，\n",
      "土: 0.9593\n",
      "水: 0.0221\n",
      "户: 0.0186\n",
      "风烟纪南城，尘\n",
      "荆: 0.9791\n",
      "青: 0.0123\n",
      "今: 0.0086\n",
      "风烟纪南城，尘土\n",
      "门: 0.9935\n",
      "青: 0.0041\n",
      "城: 0.0024\n",
      "风烟纪南城，尘土荆\n",
      "路: 0.9905\n",
      "城: 0.0054\n",
      "东: 0.0041\n",
      "风烟纪南城，尘土荆门\n",
      "。: 1.0000\n",
      "，: 0.0000\n",
      "劒: 0.0000\n",
      "风烟纪南城，尘土荆门路\n",
      "天: 0.9942\n",
      "相: 0.0031\n",
      "江: 0.0027\n",
      "风烟纪南城，尘土荆门路。\n",
      "寒: 0.9707\n",
      "山: 0.0196\n",
      "河: 0.0097\n",
      "风烟纪南城，尘土荆门路。天\n",
      "多: 0.5725\n",
      "猎: 0.3919\n",
      "不: 0.0356\n",
      "风烟纪南城，尘土荆门路。天寒\n",
      "猎: 0.8564\n",
      "兽: 0.0836\n",
      "谷: 0.0600\n",
      "风烟纪南城，尘土荆门路。天寒多\n",
      "骑: 0.9712\n",
      "字: 0.0154\n",
      "会: 0.0134\n",
      "风烟纪南城，尘土荆门路。天寒多猎\n",
      "，: 1.0000\n",
      "全: 0.0000\n",
      "门: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑\n",
      "走: 0.9347\n",
      "海: 0.0377\n",
      "桥: 0.0276\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，\n",
      "上: 0.9786\n",
      "鸟: 0.0176\n",
      "海: 0.0037\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走\n",
      "樊: 0.9143\n",
      "高: 0.0474\n",
      "马: 0.0383\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走上\n",
      "姬: 0.9542\n",
      "应: 0.0297\n",
      "三: 0.0160\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走上樊\n",
      "墓: 0.9058\n",
      "枝: 0.0629\n",
      "夫: 0.0313\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走上樊姬\n",
      "。: 1.0000\n",
      "大: 0.0000\n",
      "意: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走上樊姬墓\n",
      "<EOP>: 0.9994\n",
      "秦: 0.0003\n",
      "玉: 0.0003\n",
      "风烟纪南城，尘土荆门路。天寒多猎骑，走上樊姬墓。\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_word=\"<START>\", top_k=1, log=False):\n",
    "    generated_text = \"\"\n",
    "    index_tensors_list = []\n",
    "    for word in start_word:\n",
    "        index_tensors_list.append(index_tensors[word].unsqueeze(0))\n",
    "        generated_text += word\n",
    "\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_length - len(generated_text)):\n",
    "            input_tensor = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "\n",
    "            output, hidden_state = model(input_tensor.to(device), hidden_state)\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "            top_values, top_indices = last_word.data.topk(top_k)\n",
    "\n",
    "            probabilities = torch.exp(top_values)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "\n",
    "            probabilities_np = probabilities.cpu().detach().numpy()\n",
    "            probabilities_np = probabilities_np / probabilities_np.sum()\n",
    "            indices_np = top_indices.cpu().detach().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            # * 需要升一个维\n",
    "            index_tensors_list = [index_tensors[next_word]]\n",
    "            generated_text += next_word\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"长安一片月\", top_k=1))\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "print(generate_text(\"月\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"日\", top_k=30))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
