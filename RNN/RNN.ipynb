{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import opencc\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "max_length = 99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            paragraphs = poetry.get(\"paragraphs\")\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    src_path = Path(\"./data/chinese-poetry-master/全唐诗/\")\n",
    "    for file_path in src_path.glob(\"poet.tang*\"):\n",
    "        data.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "    # data.extend(handleJson(file_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")  # All if author=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3514\n",
      "data_size 1206\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    # Find the maximum target length\n",
    "    max_target_len = max([t.size(0) for t in targets])\n",
    "    # Pad targets to the maximum length\n",
    "    padded_targets = torch.stack(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                t, (0, max_target_len - t.size(0)), \"constant\", word_to_index[\"<START>\"]\n",
    "            )\n",
    "            for t in targets\n",
    "        ]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )  # Enable batch_first\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # Adjusted for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Adjusted view for batch processing, removing hard-coded lengths\n",
    "        output = self.linear1(F.relu(lstm_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        # output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        # Reshape output to (batch_size, seq_len, vocab_size) for compatibility\n",
    "        output = output.view(input.size(0), input.size(1), -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoetryModel(len(word_to_index), 256, 256)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.NLLLoss(ignore_index=word_to_index[\"<START>\"], reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 00000/00302 | Loss: 8.1615\n",
      "Epoch: 001/050 | Batch 00100/00302 | Loss: 5.7644\n",
      "Epoch: 001/050 | Batch 00200/00302 | Loss: 6.0068\n",
      "Epoch: 001/050 | Batch 00300/00302 | Loss: 6.0886\n",
      "Epoch: 002/050 | Batch 00000/00302 | Loss: 5.8192\n",
      "Epoch: 002/050 | Batch 00100/00302 | Loss: 5.6160\n",
      "Epoch: 002/050 | Batch 00200/00302 | Loss: 5.6990\n",
      "Epoch: 002/050 | Batch 00300/00302 | Loss: 5.6606\n",
      "Epoch: 003/050 | Batch 00000/00302 | Loss: 5.4652\n",
      "Epoch: 003/050 | Batch 00100/00302 | Loss: 5.3944\n",
      "Epoch: 003/050 | Batch 00200/00302 | Loss: 5.3430\n",
      "Epoch: 003/050 | Batch 00300/00302 | Loss: 5.4390\n",
      "Epoch: 004/050 | Batch 00000/00302 | Loss: 5.0503\n",
      "Epoch: 004/050 | Batch 00100/00302 | Loss: 5.1681\n",
      "Epoch: 004/050 | Batch 00200/00302 | Loss: 5.4184\n",
      "Epoch: 004/050 | Batch 00300/00302 | Loss: 5.2579\n",
      "Epoch: 005/050 | Batch 00000/00302 | Loss: 5.2254\n",
      "Epoch: 005/050 | Batch 00100/00302 | Loss: 5.0910\n",
      "Epoch: 005/050 | Batch 00200/00302 | Loss: 5.0171\n",
      "Epoch: 005/050 | Batch 00300/00302 | Loss: 5.0612\n",
      "Epoch: 006/050 | Batch 00000/00302 | Loss: 4.8769\n",
      "Epoch: 006/050 | Batch 00100/00302 | Loss: 4.8542\n",
      "Epoch: 006/050 | Batch 00200/00302 | Loss: 5.2087\n",
      "Epoch: 006/050 | Batch 00300/00302 | Loss: 5.0762\n",
      "Epoch: 007/050 | Batch 00000/00302 | Loss: 4.9845\n",
      "Epoch: 007/050 | Batch 00100/00302 | Loss: 5.0564\n",
      "Epoch: 007/050 | Batch 00200/00302 | Loss: 4.9912\n",
      "Epoch: 007/050 | Batch 00300/00302 | Loss: 4.9981\n",
      "Epoch: 008/050 | Batch 00000/00302 | Loss: 4.6432\n",
      "Epoch: 008/050 | Batch 00100/00302 | Loss: 4.6798\n",
      "Epoch: 008/050 | Batch 00200/00302 | Loss: 4.6568\n",
      "Epoch: 008/050 | Batch 00300/00302 | Loss: 4.6239\n",
      "Epoch: 009/050 | Batch 00000/00302 | Loss: 4.7914\n",
      "Epoch: 009/050 | Batch 00100/00302 | Loss: 4.5560\n",
      "Epoch: 009/050 | Batch 00200/00302 | Loss: 4.2895\n",
      "Epoch: 009/050 | Batch 00300/00302 | Loss: 4.9631\n",
      "Epoch: 010/050 | Batch 00000/00302 | Loss: 4.9856\n",
      "Epoch: 010/050 | Batch 00100/00302 | Loss: 4.9304\n",
      "Epoch: 010/050 | Batch 00200/00302 | Loss: 5.0783\n",
      "Epoch: 010/050 | Batch 00300/00302 | Loss: 4.9443\n",
      "Epoch: 011/050 | Batch 00000/00302 | Loss: 4.5965\n",
      "Epoch: 011/050 | Batch 00100/00302 | Loss: 4.6888\n",
      "Epoch: 011/050 | Batch 00200/00302 | Loss: 4.4581\n",
      "Epoch: 011/050 | Batch 00300/00302 | Loss: 4.0538\n",
      "Epoch: 012/050 | Batch 00000/00302 | Loss: 5.2394\n",
      "Epoch: 012/050 | Batch 00100/00302 | Loss: 4.0803\n",
      "Epoch: 012/050 | Batch 00200/00302 | Loss: 4.3822\n",
      "Epoch: 012/050 | Batch 00300/00302 | Loss: 4.6325\n",
      "Epoch: 013/050 | Batch 00000/00302 | Loss: 4.1364\n",
      "Epoch: 013/050 | Batch 00100/00302 | Loss: 4.4609\n",
      "Epoch: 013/050 | Batch 00200/00302 | Loss: 4.7101\n",
      "Epoch: 013/050 | Batch 00300/00302 | Loss: 4.9544\n",
      "Epoch: 014/050 | Batch 00000/00302 | Loss: 4.2733\n",
      "Epoch: 014/050 | Batch 00100/00302 | Loss: 4.5812\n",
      "Epoch: 014/050 | Batch 00200/00302 | Loss: 4.0057\n",
      "Epoch: 014/050 | Batch 00300/00302 | Loss: 4.6744\n",
      "Epoch: 015/050 | Batch 00000/00302 | Loss: 4.2182\n",
      "Epoch: 015/050 | Batch 00100/00302 | Loss: 4.6126\n",
      "Epoch: 015/050 | Batch 00200/00302 | Loss: 4.3814\n",
      "Epoch: 015/050 | Batch 00300/00302 | Loss: 4.2860\n",
      "Epoch: 016/050 | Batch 00000/00302 | Loss: 4.7773\n",
      "Epoch: 016/050 | Batch 00100/00302 | Loss: 3.8445\n",
      "Epoch: 016/050 | Batch 00200/00302 | Loss: 4.0823\n",
      "Epoch: 016/050 | Batch 00300/00302 | Loss: 4.4497\n",
      "Epoch: 017/050 | Batch 00000/00302 | Loss: 3.9608\n",
      "Epoch: 017/050 | Batch 00100/00302 | Loss: 4.3466\n",
      "Epoch: 017/050 | Batch 00200/00302 | Loss: 4.3170\n",
      "Epoch: 017/050 | Batch 00300/00302 | Loss: 4.5521\n",
      "Epoch: 018/050 | Batch 00000/00302 | Loss: 4.1091\n",
      "Epoch: 018/050 | Batch 00100/00302 | Loss: 4.6329\n",
      "Epoch: 018/050 | Batch 00200/00302 | Loss: 3.9891\n",
      "Epoch: 018/050 | Batch 00300/00302 | Loss: 4.1830\n",
      "Epoch: 019/050 | Batch 00000/00302 | Loss: 4.2527\n",
      "Epoch: 019/050 | Batch 00100/00302 | Loss: 4.2265\n",
      "Epoch: 019/050 | Batch 00200/00302 | Loss: 3.9236\n",
      "Epoch: 019/050 | Batch 00300/00302 | Loss: 3.6934\n",
      "Epoch: 020/050 | Batch 00000/00302 | Loss: 3.9742\n",
      "Epoch: 020/050 | Batch 00100/00302 | Loss: 3.9888\n",
      "Epoch: 020/050 | Batch 00200/00302 | Loss: 4.2562\n",
      "Epoch: 020/050 | Batch 00300/00302 | Loss: 3.9285\n",
      "Epoch: 021/050 | Batch 00000/00302 | Loss: 4.1067\n",
      "Epoch: 021/050 | Batch 00100/00302 | Loss: 3.6258\n",
      "Epoch: 021/050 | Batch 00200/00302 | Loss: 3.8610\n",
      "Epoch: 021/050 | Batch 00300/00302 | Loss: 3.8572\n",
      "Epoch: 022/050 | Batch 00000/00302 | Loss: 4.1417\n",
      "Epoch: 022/050 | Batch 00100/00302 | Loss: 3.7614\n",
      "Epoch: 022/050 | Batch 00200/00302 | Loss: 4.2043\n",
      "Epoch: 022/050 | Batch 00300/00302 | Loss: 4.2976\n",
      "Epoch: 023/050 | Batch 00000/00302 | Loss: 3.9968\n",
      "Epoch: 023/050 | Batch 00100/00302 | Loss: 3.8405\n",
      "Epoch: 023/050 | Batch 00200/00302 | Loss: 3.9123\n",
      "Epoch: 023/050 | Batch 00300/00302 | Loss: 3.5959\n",
      "Epoch: 024/050 | Batch 00000/00302 | Loss: 3.6569\n",
      "Epoch: 024/050 | Batch 00100/00302 | Loss: 3.9736\n",
      "Epoch: 024/050 | Batch 00200/00302 | Loss: 3.3572\n",
      "Epoch: 024/050 | Batch 00300/00302 | Loss: 4.8478\n",
      "Epoch: 025/050 | Batch 00000/00302 | Loss: 3.9137\n",
      "Epoch: 025/050 | Batch 00100/00302 | Loss: 3.5970\n",
      "Epoch: 025/050 | Batch 00200/00302 | Loss: 3.8751\n",
      "Epoch: 025/050 | Batch 00300/00302 | Loss: 4.7295\n",
      "Epoch: 026/050 | Batch 00000/00302 | Loss: 3.7993\n",
      "Epoch: 026/050 | Batch 00100/00302 | Loss: 3.9592\n",
      "Epoch: 026/050 | Batch 00200/00302 | Loss: 4.7068\n",
      "Epoch: 026/050 | Batch 00300/00302 | Loss: 3.2481\n",
      "Epoch: 027/050 | Batch 00000/00302 | Loss: 4.3676\n",
      "Epoch: 027/050 | Batch 00100/00302 | Loss: 3.4088\n",
      "Epoch: 027/050 | Batch 00200/00302 | Loss: 4.0291\n",
      "Epoch: 027/050 | Batch 00300/00302 | Loss: 4.3301\n",
      "Epoch: 028/050 | Batch 00000/00302 | Loss: 3.4422\n",
      "Epoch: 028/050 | Batch 00100/00302 | Loss: 3.0654\n",
      "Epoch: 028/050 | Batch 00200/00302 | Loss: 3.9029\n",
      "Epoch: 028/050 | Batch 00300/00302 | Loss: 3.9853\n",
      "Epoch: 029/050 | Batch 00000/00302 | Loss: 3.6778\n",
      "Epoch: 029/050 | Batch 00100/00302 | Loss: 4.1378\n",
      "Epoch: 029/050 | Batch 00200/00302 | Loss: 3.4424\n",
      "Epoch: 029/050 | Batch 00300/00302 | Loss: 4.0392\n",
      "Epoch: 030/050 | Batch 00000/00302 | Loss: 3.4047\n",
      "Epoch: 030/050 | Batch 00100/00302 | Loss: 4.3382\n",
      "Epoch: 030/050 | Batch 00200/00302 | Loss: 3.5707\n",
      "Epoch: 030/050 | Batch 00300/00302 | Loss: 4.0096\n",
      "Epoch: 031/050 | Batch 00000/00302 | Loss: 3.5741\n",
      "Epoch: 031/050 | Batch 00100/00302 | Loss: 3.1118\n",
      "Epoch: 031/050 | Batch 00200/00302 | Loss: 3.5867\n",
      "Epoch: 031/050 | Batch 00300/00302 | Loss: 3.4837\n",
      "Epoch: 032/050 | Batch 00000/00302 | Loss: 3.2502\n",
      "Epoch: 032/050 | Batch 00100/00302 | Loss: 3.6624\n",
      "Epoch: 032/050 | Batch 00200/00302 | Loss: 3.4628\n",
      "Epoch: 032/050 | Batch 00300/00302 | Loss: 3.6767\n",
      "Epoch: 033/050 | Batch 00000/00302 | Loss: 3.8145\n",
      "Epoch: 033/050 | Batch 00100/00302 | Loss: 3.5061\n",
      "Epoch: 033/050 | Batch 00200/00302 | Loss: 3.6653\n",
      "Epoch: 033/050 | Batch 00300/00302 | Loss: 3.9636\n",
      "Epoch: 034/050 | Batch 00000/00302 | Loss: 3.1280\n",
      "Epoch: 034/050 | Batch 00100/00302 | Loss: 3.1915\n",
      "Epoch: 034/050 | Batch 00200/00302 | Loss: 2.7924\n",
      "Epoch: 034/050 | Batch 00300/00302 | Loss: 3.1978\n",
      "Epoch: 035/050 | Batch 00000/00302 | Loss: 2.6878\n",
      "Epoch: 035/050 | Batch 00100/00302 | Loss: 3.1334\n",
      "Epoch: 035/050 | Batch 00200/00302 | Loss: 3.8334\n",
      "Epoch: 035/050 | Batch 00300/00302 | Loss: 3.9363\n",
      "Epoch: 036/050 | Batch 00000/00302 | Loss: 3.4132\n",
      "Epoch: 036/050 | Batch 00100/00302 | Loss: 3.0882\n",
      "Epoch: 036/050 | Batch 00200/00302 | Loss: 2.9128\n",
      "Epoch: 036/050 | Batch 00300/00302 | Loss: 3.2046\n",
      "Epoch: 037/050 | Batch 00000/00302 | Loss: 2.7907\n",
      "Epoch: 037/050 | Batch 00100/00302 | Loss: 2.8096\n",
      "Epoch: 037/050 | Batch 00200/00302 | Loss: 3.8060\n",
      "Epoch: 037/050 | Batch 00300/00302 | Loss: 3.2864\n",
      "Epoch: 038/050 | Batch 00000/00302 | Loss: 3.3303\n",
      "Epoch: 038/050 | Batch 00100/00302 | Loss: 3.3558\n",
      "Epoch: 038/050 | Batch 00200/00302 | Loss: 3.0226\n",
      "Epoch: 038/050 | Batch 00300/00302 | Loss: 3.2464\n",
      "Epoch: 039/050 | Batch 00000/00302 | Loss: 3.1734\n",
      "Epoch: 039/050 | Batch 00100/00302 | Loss: 3.1394\n",
      "Epoch: 039/050 | Batch 00200/00302 | Loss: 2.9915\n",
      "Epoch: 039/050 | Batch 00300/00302 | Loss: 3.7274\n",
      "Epoch: 040/050 | Batch 00000/00302 | Loss: 2.5902\n",
      "Epoch: 040/050 | Batch 00100/00302 | Loss: 2.8569\n",
      "Epoch: 040/050 | Batch 00200/00302 | Loss: 3.7372\n",
      "Epoch: 040/050 | Batch 00300/00302 | Loss: 3.6494\n",
      "Epoch: 041/050 | Batch 00000/00302 | Loss: 3.1853\n",
      "Epoch: 041/050 | Batch 00100/00302 | Loss: 2.8688\n",
      "Epoch: 041/050 | Batch 00200/00302 | Loss: 3.9175\n",
      "Epoch: 041/050 | Batch 00300/00302 | Loss: 3.3332\n",
      "Epoch: 042/050 | Batch 00000/00302 | Loss: 2.8020\n",
      "Epoch: 042/050 | Batch 00100/00302 | Loss: 3.0471\n",
      "Epoch: 042/050 | Batch 00200/00302 | Loss: 3.2869\n",
      "Epoch: 042/050 | Batch 00300/00302 | Loss: 3.0187\n",
      "Epoch: 043/050 | Batch 00000/00302 | Loss: 3.5259\n",
      "Epoch: 043/050 | Batch 00100/00302 | Loss: 3.2314\n",
      "Epoch: 043/050 | Batch 00200/00302 | Loss: 2.9069\n",
      "Epoch: 043/050 | Batch 00300/00302 | Loss: 3.1053\n",
      "Epoch: 044/050 | Batch 00000/00302 | Loss: 2.0794\n",
      "Epoch: 044/050 | Batch 00100/00302 | Loss: 2.6955\n",
      "Epoch: 044/050 | Batch 00200/00302 | Loss: 3.6030\n",
      "Epoch: 044/050 | Batch 00300/00302 | Loss: 2.9048\n",
      "Epoch: 045/050 | Batch 00000/00302 | Loss: 3.4238\n",
      "Epoch: 045/050 | Batch 00100/00302 | Loss: 3.6003\n",
      "Epoch: 045/050 | Batch 00200/00302 | Loss: 2.5218\n",
      "Epoch: 045/050 | Batch 00300/00302 | Loss: 3.4740\n",
      "Epoch: 046/050 | Batch 00000/00302 | Loss: 2.3224\n",
      "Epoch: 046/050 | Batch 00100/00302 | Loss: 2.3749\n",
      "Epoch: 046/050 | Batch 00200/00302 | Loss: 2.2950\n",
      "Epoch: 046/050 | Batch 00300/00302 | Loss: 3.1499\n",
      "Epoch: 047/050 | Batch 00000/00302 | Loss: 2.9664\n",
      "Epoch: 047/050 | Batch 00100/00302 | Loss: 2.8467\n",
      "Epoch: 047/050 | Batch 00200/00302 | Loss: 2.2224\n",
      "Epoch: 047/050 | Batch 00300/00302 | Loss: 2.5106\n",
      "Epoch: 048/050 | Batch 00000/00302 | Loss: 2.7242\n",
      "Epoch: 048/050 | Batch 00100/00302 | Loss: 3.1159\n",
      "Epoch: 048/050 | Batch 00200/00302 | Loss: 3.2449\n",
      "Epoch: 048/050 | Batch 00300/00302 | Loss: 3.1779\n",
      "Epoch: 049/050 | Batch 00000/00302 | Loss: 3.1606\n",
      "Epoch: 049/050 | Batch 00100/00302 | Loss: 3.0928\n",
      "Epoch: 049/050 | Batch 00200/00302 | Loss: 2.8801\n",
      "Epoch: 049/050 | Batch 00300/00302 | Loss: 3.0869\n",
      "Epoch: 050/050 | Batch 00000/00302 | Loss: 3.2901\n",
      "Epoch: 050/050 | Batch 00100/00302 | Loss: 3.0944\n",
      "Epoch: 050/050 | Batch 00200/00302 | Loss: 1.8710\n",
      "Epoch: 050/050 | Batch 00300/00302 | Loss: 3.2266\n"
     ]
    }
   ],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (t, o) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=t.size(0))\n",
    "            output, hidden = model(t.to(device), hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), o.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "    torch.save(model.state_dict(), \"poetry-gen.pth\")\n",
    "\n",
    "\n",
    "train(model, num_epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"poetry-gen.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['江']\n",
      "江山东海客，云帆一何处。黄河万里情，独坐远行亲。长啸千古树，云间望长安。一行无所处，远别离离情。相思若未得，吾去难未还。\n",
      "['泉']\n",
      "泉水东北流，波荡双鸳鸯。飞燕燕汉国，飞龙与天通。人生寒松草，草木不相连。坐思天上月，空余碧玉道。云行无知老，吾将何时？。\n",
      "['泉']\n",
      "泉水东南阳，花明玉窗枝。素手愁风色，独宿天门中。水中天上月，空长松里寒。长松一相访，空余道士征。何如见君情，为我一杯杯。酒后不可为，长安不可期。\n",
      "['泉']\n",
      "泉水如青天，云帆罗浮空。山花开景殿，五入丹阙柳。独有天门心，清溪入碧云。清月何寂悠，歌酒入松松。风清长安来，胡雁还归西。\n",
      "['风']\n",
      "吹: 0.6101\n",
      "日: 0.2246\n",
      "云: 0.1653\n",
      "风吹\n",
      "玉: 0.5230\n",
      "落: 0.3101\n",
      "花: 0.1669\n",
      "风吹落\n",
      "花: 0.8889\n",
      "日: 0.0960\n",
      "天: 0.0151\n",
      "风吹落花\n",
      "月: 0.3929\n",
      "满: 0.3712\n",
      "夜: 0.2359\n",
      "风吹落花满\n",
      "树: 0.6393\n",
      "水: 0.2313\n",
      "溪: 0.1295\n",
      "风吹落花满树\n",
      "，: 0.8248\n",
      "树: 0.1029\n",
      "枝: 0.0723\n",
      "风吹落花满树，\n",
      "春: 0.5094\n",
      "水: 0.3440\n",
      "西: 0.1467\n",
      "风吹落花满树，水\n",
      "明: 0.5360\n",
      "弄: 0.2375\n",
      "摇: 0.2265\n",
      "风吹落花满树，水明\n",
      "月: 0.6931\n",
      "白: 0.1730\n",
      "湖: 0.1339\n",
      "风吹落花满树，水明白\n",
      "日: 0.4349\n",
      "雪: 0.3277\n",
      "露: 0.2373\n",
      "风吹落花满树，水明白露\n",
      "华: 0.4100\n",
      "生: 0.4015\n",
      "露: 0.1884\n",
      "风吹落花满树，水明白露华\n",
      "枝: 0.3626\n",
      "池: 0.3541\n",
      "浓: 0.2833\n",
      "风吹落花满树，水明白露华浓\n",
      "。: 0.9949\n",
      "香: 0.0041\n",
      "枝: 0.0010\n",
      "风吹落花满树，水明白露华浓。\n",
      "若: 0.5106\n",
      "此: 0.3838\n",
      "坐: 0.1056\n",
      "风吹落花满树，水明白露华浓。坐\n",
      "愁: 0.4517\n",
      "看: 0.4299\n",
      "坐: 0.1184\n",
      "风吹落花满树，水明白露华浓。坐看\n",
      "秋: 0.7945\n",
      "寒: 0.1041\n",
      "飞: 0.1014\n",
      "风吹落花满树，水明白露华浓。坐看秋\n",
      "草: 0.4816\n",
      "色: 0.2845\n",
      "风: 0.2339\n",
      "风吹落花满树，水明白露华浓。坐看秋色\n",
      "黄: 0.3788\n",
      "夜: 0.3156\n",
      "碧: 0.3056\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄\n",
      "鹤: 0.4808\n",
      "叶: 0.2768\n",
      "河: 0.2424\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤\n",
      "里: 0.8056\n",
      "寒: 0.1165\n",
      "月: 0.0779\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里\n",
      "，: 0.9998\n",
      "。: 0.0001\n",
      "？: 0.0001\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，\n",
      "今: 0.6504\n",
      "独: 0.1855\n",
      "云: 0.1641\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今\n",
      "年: 0.4996\n",
      "来: 0.2679\n",
      "日: 0.2326\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来\n",
      "还: 0.5542\n",
      "见: 0.3054\n",
      "三: 0.1404\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还\n",
      "入: 0.4224\n",
      "见: 0.3407\n",
      "是: 0.2369\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见\n",
      "故: 0.3450\n",
      "愁: 0.3284\n",
      "春: 0.3266\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见故\n",
      "人: 0.9908\n",
      "家: 0.0061\n",
      "国: 0.0031\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见故人\n",
      "家: 0.6457\n",
      "间: 0.1948\n",
      "倾: 0.1595\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见故人家\n",
      "。: 0.9891\n",
      "？: 0.0099\n",
      "西: 0.0009\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见故人家。\n",
      "<EOP>: 0.9995\n",
      "今: 0.0003\n",
      "胡: 0.0002\n",
      "风吹落花满树，水明白露华浓。坐看秋色黄鹤里，今来还见故人家。\n"
     ]
    }
   ],
   "source": [
    "def make_one_hot_vec_target(word, word_to_index):\n",
    "    rst = autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "    return rst\n",
    "\n",
    "\n",
    "def generate_text(start_word=\"<START>\", top_k=1, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "    with torch.no_grad():\n",
    "        for word in words:\n",
    "            input_vector = make_one_hot_vec_target(word, word_to_index).unsqueeze(0)\n",
    "            model(input_vector.to(device), hidden_state)\n",
    "            generated_text += word\n",
    "\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output, hidden_state = model(input_vector.to(device), hidden_state)\n",
    "            top_values, top_indices = output.data.topk(top_k)\n",
    "\n",
    "            if top_k == 1:\n",
    "                selected_index = top_indices.item()\n",
    "            else:\n",
    "                top_indices = top_indices.squeeze()\n",
    "                top_values = top_values.squeeze()\n",
    "\n",
    "                probabilities = torch.exp(top_values)\n",
    "                top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "\n",
    "                probabilities_np = probabilities.cpu().detach().numpy()\n",
    "                probabilities_np = probabilities_np / probabilities_np.sum()\n",
    "                indices_np = top_indices.cpu().detach().numpy()\n",
    "                if log:\n",
    "                    for word, prob in zip(top_words, probabilities_np):\n",
    "                        print(f\"{word}: {prob:.4f}\")\n",
    "                selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            input_vector = make_one_hot_vec_target(next_word, word_to_index).unsqueeze(\n",
    "                0\n",
    "            )\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=1))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=30))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
