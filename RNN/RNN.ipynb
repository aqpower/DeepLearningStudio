{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import opencc\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "max_length = 99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            paragraphs = poetry.get(\"paragraphs\")\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    src_path = Path(\"./data/chinese-poetry-master/全唐诗/\")\n",
    "    for file_path in src_path.glob(\"poet.tang*\"):\n",
    "        data.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "    # data.extend(handleJson(file_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")  # All if author=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_targets = nn.utils.rnn.pad_sequence(\n",
    "        targets, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )  # Enable batch_first\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # Adjusted for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Adjusted view for batch processing, removing hard-coded lengths\n",
    "        output = self.linear1(F.relu(lstm_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        # output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        # Reshape output to (batch_size, seq_len, vocab_size) for compatibility\n",
    "        output = output.view(input.size(0), input.size(1), -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoetryModel(len(word_to_index), 256, 256)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.NLLLoss(ignore_index=word_to_index[\"<START>\"], reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (sequence, target) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=sequence.size(0))\n",
    "            output, hidden = model(sequence.to(device), hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), target.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "    torch.save(model.state_dict(), \"poetry-gen.pth\")\n",
    "\n",
    "\n",
    "# train(model, num_epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"poetry-gen.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot_vec_target(word, word_to_index):\n",
    "    rst = autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "    return rst\n",
    "\n",
    "\n",
    "def generate_text(start_word=\"<START>\", top_k=1, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "    with torch.no_grad():\n",
    "        vectors_list = []\n",
    "        for word in words:\n",
    "            word_vector = make_one_hot_vec_target(word, word_to_index).unsqueeze(0)\n",
    "            vectors_list.append(word_vector)\n",
    "            generated_text += word\n",
    "\n",
    "        input_vector = torch.cat(vectors_list, dim=1)\n",
    "        print(input_vector)\n",
    "        print(input_vector.size())\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output, hidden_state = model(input_vector.to(device), hidden_state)\n",
    "            last_word = output[:, -1, :]\n",
    "            top_values, top_indices = last_word.data.topk(top_k)\n",
    "\n",
    "            if top_k == 1:\n",
    "                selected_index = top_indices.item()\n",
    "            else:\n",
    "                top_indices = top_indices.squeeze()\n",
    "                top_values = top_values.squeeze()\n",
    "\n",
    "                probabilities = torch.exp(top_values)\n",
    "                top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "\n",
    "                probabilities_np = probabilities.cpu().detach().numpy()\n",
    "                probabilities_np = probabilities_np / probabilities_np.sum()\n",
    "                indices_np = top_indices.cpu().detach().numpy()\n",
    "                if log:\n",
    "                    for word, prob in zip(top_words, probabilities_np):\n",
    "                        print(f\"{word}: {prob:.4f}\")\n",
    "                selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            input_vector = make_one_hot_vec_target(next_word, word_to_index).unsqueeze(\n",
    "                0\n",
    "            )\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=1))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=30))\n",
    "print(generate_text(\"沾衣欲湿杏花雨\", top_k=3))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
