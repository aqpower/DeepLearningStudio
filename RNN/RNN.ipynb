{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import opencc\n",
    "import pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "max_length = 99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            paragraphs = poetry.get(\"paragraphs\")\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    src_path = Path(\"./data/chinese-poetry-master/全唐诗/\")\n",
    "    for file_path in src_path.glob(\"poet.tang*\"):\n",
    "        data.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "        # data.extend(handleJson(file_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")  # All if author=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3514\n",
      "data_size 1206\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    # Find the maximum target length\n",
    "    max_target_len = max([t.size(0) for t in targets])\n",
    "    # Pad targets to the maximum length\n",
    "    padded_targets = torch.stack(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                t, (0, max_target_len - t.size(0)), \"constant\", word_to_index[\"<START>\"]\n",
    "            )\n",
    "            for t in targets\n",
    "        ]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )  # Enable batch_first\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # Adjusted for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Adjusted view for batch processing, removing hard-coded lengths\n",
    "        output = self.linear1(F.relu(lstm_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        # output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        # Reshape output to (batch_size, seq_len, vocab_size) for compatibility\n",
    "        output = output.view(input.size(0), input.size(1), -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoetryModel(len(word_to_index), 256, 256)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    ignore_index=word_to_index[\"<START>\"], reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 00000/00302 | Loss: 8.1766\n",
      "Epoch: 001/050 | Batch 00100/00302 | Loss: 5.9518\n",
      "Epoch: 001/050 | Batch 00200/00302 | Loss: 5.2774\n",
      "Epoch: 001/050 | Batch 00300/00302 | Loss: 5.5833\n",
      "Epoch: 002/050 | Batch 00000/00302 | Loss: 5.2386\n",
      "Epoch: 002/050 | Batch 00100/00302 | Loss: 5.4950\n",
      "Epoch: 002/050 | Batch 00200/00302 | Loss: 5.5742\n",
      "Epoch: 002/050 | Batch 00300/00302 | Loss: 5.3981\n",
      "Epoch: 003/050 | Batch 00000/00302 | Loss: 5.5771\n",
      "Epoch: 003/050 | Batch 00100/00302 | Loss: 5.0366\n",
      "Epoch: 003/050 | Batch 00200/00302 | Loss: 5.3690\n",
      "Epoch: 003/050 | Batch 00300/00302 | Loss: 5.5693\n",
      "Epoch: 004/050 | Batch 00000/00302 | Loss: 4.9900\n",
      "Epoch: 004/050 | Batch 00100/00302 | Loss: 5.7209\n",
      "Epoch: 004/050 | Batch 00200/00302 | Loss: 5.0506\n",
      "Epoch: 004/050 | Batch 00300/00302 | Loss: 5.0493\n",
      "Epoch: 005/050 | Batch 00000/00302 | Loss: 4.9989\n",
      "Epoch: 005/050 | Batch 00100/00302 | Loss: 5.1177\n",
      "Epoch: 005/050 | Batch 00200/00302 | Loss: 5.1614\n",
      "Epoch: 005/050 | Batch 00300/00302 | Loss: 5.0541\n",
      "Epoch: 006/050 | Batch 00000/00302 | Loss: 4.9371\n",
      "Epoch: 006/050 | Batch 00100/00302 | Loss: 5.1387\n",
      "Epoch: 006/050 | Batch 00200/00302 | Loss: 4.5968\n",
      "Epoch: 006/050 | Batch 00300/00302 | Loss: 4.9599\n",
      "Epoch: 007/050 | Batch 00000/00302 | Loss: 4.6963\n",
      "Epoch: 007/050 | Batch 00100/00302 | Loss: 5.0097\n",
      "Epoch: 007/050 | Batch 00200/00302 | Loss: 4.8722\n",
      "Epoch: 007/050 | Batch 00300/00302 | Loss: 5.1256\n",
      "Epoch: 008/050 | Batch 00000/00302 | Loss: 4.6116\n",
      "Epoch: 008/050 | Batch 00100/00302 | Loss: 5.5670\n",
      "Epoch: 008/050 | Batch 00200/00302 | Loss: 4.7735\n",
      "Epoch: 008/050 | Batch 00300/00302 | Loss: 5.2249\n",
      "Epoch: 009/050 | Batch 00000/00302 | Loss: 5.0360\n",
      "Epoch: 009/050 | Batch 00100/00302 | Loss: 5.0558\n",
      "Epoch: 009/050 | Batch 00200/00302 | Loss: 4.4985\n",
      "Epoch: 009/050 | Batch 00300/00302 | Loss: 5.1769\n",
      "Epoch: 010/050 | Batch 00000/00302 | Loss: 4.3401\n",
      "Epoch: 010/050 | Batch 00100/00302 | Loss: 5.0234\n",
      "Epoch: 010/050 | Batch 00200/00302 | Loss: 5.3501\n",
      "Epoch: 010/050 | Batch 00300/00302 | Loss: 4.8856\n",
      "Epoch: 011/050 | Batch 00000/00302 | Loss: 4.7886\n",
      "Epoch: 011/050 | Batch 00100/00302 | Loss: 4.4824\n",
      "Epoch: 011/050 | Batch 00200/00302 | Loss: 5.1539\n",
      "Epoch: 011/050 | Batch 00300/00302 | Loss: 5.1281\n",
      "Epoch: 012/050 | Batch 00000/00302 | Loss: 4.2379\n",
      "Epoch: 012/050 | Batch 00100/00302 | Loss: 4.6963\n",
      "Epoch: 012/050 | Batch 00200/00302 | Loss: 4.9334\n",
      "Epoch: 012/050 | Batch 00300/00302 | Loss: 5.2484\n",
      "Epoch: 013/050 | Batch 00000/00302 | Loss: 4.4581\n",
      "Epoch: 013/050 | Batch 00100/00302 | Loss: 5.2094\n",
      "Epoch: 013/050 | Batch 00200/00302 | Loss: 4.6397\n",
      "Epoch: 013/050 | Batch 00300/00302 | Loss: 4.9073\n",
      "Epoch: 014/050 | Batch 00000/00302 | Loss: 4.6358\n",
      "Epoch: 014/050 | Batch 00100/00302 | Loss: 4.7681\n",
      "Epoch: 014/050 | Batch 00200/00302 | Loss: 4.9963\n",
      "Epoch: 014/050 | Batch 00300/00302 | Loss: 5.0665\n",
      "Epoch: 015/050 | Batch 00000/00302 | Loss: 4.7404\n",
      "Epoch: 015/050 | Batch 00100/00302 | Loss: 4.4780\n",
      "Epoch: 015/050 | Batch 00200/00302 | Loss: 4.7565\n",
      "Epoch: 015/050 | Batch 00300/00302 | Loss: 4.8403\n",
      "Epoch: 016/050 | Batch 00000/00302 | Loss: 4.7552\n",
      "Epoch: 016/050 | Batch 00100/00302 | Loss: 4.4750\n",
      "Epoch: 016/050 | Batch 00200/00302 | Loss: 4.1772\n",
      "Epoch: 016/050 | Batch 00300/00302 | Loss: 4.5404\n",
      "Epoch: 017/050 | Batch 00000/00302 | Loss: 4.4729\n",
      "Epoch: 017/050 | Batch 00100/00302 | Loss: 4.2524\n",
      "Epoch: 017/050 | Batch 00200/00302 | Loss: 5.1593\n",
      "Epoch: 017/050 | Batch 00300/00302 | Loss: 4.8495\n",
      "Epoch: 018/050 | Batch 00000/00302 | Loss: 4.7334\n",
      "Epoch: 018/050 | Batch 00100/00302 | Loss: 5.0489\n",
      "Epoch: 018/050 | Batch 00200/00302 | Loss: 4.9370\n",
      "Epoch: 018/050 | Batch 00300/00302 | Loss: 4.9166\n",
      "Epoch: 019/050 | Batch 00000/00302 | Loss: 4.5320\n",
      "Epoch: 019/050 | Batch 00100/00302 | Loss: 4.5803\n",
      "Epoch: 019/050 | Batch 00200/00302 | Loss: 5.0694\n",
      "Epoch: 019/050 | Batch 00300/00302 | Loss: 5.1918\n",
      "Epoch: 020/050 | Batch 00000/00302 | Loss: 4.3164\n",
      "Epoch: 020/050 | Batch 00100/00302 | Loss: 4.3129\n",
      "Epoch: 020/050 | Batch 00200/00302 | Loss: 4.8940\n",
      "Epoch: 020/050 | Batch 00300/00302 | Loss: 4.6240\n",
      "Epoch: 021/050 | Batch 00000/00302 | Loss: 4.6554\n",
      "Epoch: 021/050 | Batch 00100/00302 | Loss: 4.5754\n",
      "Epoch: 021/050 | Batch 00200/00302 | Loss: 4.2494\n",
      "Epoch: 021/050 | Batch 00300/00302 | Loss: 4.4139\n",
      "Epoch: 022/050 | Batch 00000/00302 | Loss: 4.5939\n",
      "Epoch: 022/050 | Batch 00100/00302 | Loss: 4.8722\n",
      "Epoch: 022/050 | Batch 00200/00302 | Loss: 4.5926\n",
      "Epoch: 022/050 | Batch 00300/00302 | Loss: 4.7732\n",
      "Epoch: 023/050 | Batch 00000/00302 | Loss: 4.9602\n",
      "Epoch: 023/050 | Batch 00100/00302 | Loss: 4.7865\n",
      "Epoch: 023/050 | Batch 00200/00302 | Loss: 4.2457\n",
      "Epoch: 023/050 | Batch 00300/00302 | Loss: 4.7842\n",
      "Epoch: 024/050 | Batch 00000/00302 | Loss: 3.9784\n",
      "Epoch: 024/050 | Batch 00100/00302 | Loss: 4.8791\n",
      "Epoch: 024/050 | Batch 00200/00302 | Loss: 4.8140\n",
      "Epoch: 024/050 | Batch 00300/00302 | Loss: 4.8723\n",
      "Epoch: 025/050 | Batch 00000/00302 | Loss: 4.7064\n",
      "Epoch: 025/050 | Batch 00100/00302 | Loss: 4.2488\n",
      "Epoch: 025/050 | Batch 00200/00302 | Loss: 4.9531\n",
      "Epoch: 025/050 | Batch 00300/00302 | Loss: 4.7890\n",
      "Epoch: 026/050 | Batch 00000/00302 | Loss: 4.3657\n",
      "Epoch: 026/050 | Batch 00100/00302 | Loss: 4.7363\n",
      "Epoch: 026/050 | Batch 00200/00302 | Loss: 4.8781\n",
      "Epoch: 026/050 | Batch 00300/00302 | Loss: 4.8560\n",
      "Epoch: 027/050 | Batch 00000/00302 | Loss: 4.6943\n",
      "Epoch: 027/050 | Batch 00100/00302 | Loss: 4.3785\n",
      "Epoch: 027/050 | Batch 00200/00302 | Loss: 4.6909\n",
      "Epoch: 027/050 | Batch 00300/00302 | Loss: 5.0351\n",
      "Epoch: 028/050 | Batch 00000/00302 | Loss: 4.4228\n",
      "Epoch: 028/050 | Batch 00100/00302 | Loss: 4.8149\n",
      "Epoch: 028/050 | Batch 00200/00302 | Loss: 4.6828\n",
      "Epoch: 028/050 | Batch 00300/00302 | Loss: 5.0447\n",
      "Epoch: 029/050 | Batch 00000/00302 | Loss: 4.8556\n",
      "Epoch: 029/050 | Batch 00100/00302 | Loss: 4.7738\n",
      "Epoch: 029/050 | Batch 00200/00302 | Loss: 4.5169\n",
      "Epoch: 029/050 | Batch 00300/00302 | Loss: 5.2221\n",
      "Epoch: 030/050 | Batch 00000/00302 | Loss: 4.3140\n",
      "Epoch: 030/050 | Batch 00100/00302 | Loss: 4.9966\n",
      "Epoch: 030/050 | Batch 00200/00302 | Loss: 4.5455\n",
      "Epoch: 030/050 | Batch 00300/00302 | Loss: 4.6912\n",
      "Epoch: 031/050 | Batch 00000/00302 | Loss: 4.7885\n",
      "Epoch: 031/050 | Batch 00100/00302 | Loss: 4.7144\n",
      "Epoch: 031/050 | Batch 00200/00302 | Loss: 4.8193\n",
      "Epoch: 031/050 | Batch 00300/00302 | Loss: 4.3107\n",
      "Epoch: 032/050 | Batch 00000/00302 | Loss: 4.4007\n",
      "Epoch: 032/050 | Batch 00100/00302 | Loss: 4.1229\n",
      "Epoch: 032/050 | Batch 00200/00302 | Loss: 4.7376\n",
      "Epoch: 032/050 | Batch 00300/00302 | Loss: 4.6447\n",
      "Epoch: 033/050 | Batch 00000/00302 | Loss: 4.5478\n",
      "Epoch: 033/050 | Batch 00100/00302 | Loss: 4.4156\n",
      "Epoch: 033/050 | Batch 00200/00302 | Loss: 5.0518\n",
      "Epoch: 033/050 | Batch 00300/00302 | Loss: 5.0053\n",
      "Epoch: 034/050 | Batch 00000/00302 | Loss: 4.6980\n",
      "Epoch: 034/050 | Batch 00100/00302 | Loss: 4.6093\n",
      "Epoch: 034/050 | Batch 00200/00302 | Loss: 4.1177\n",
      "Epoch: 034/050 | Batch 00300/00302 | Loss: 4.8598\n",
      "Epoch: 035/050 | Batch 00000/00302 | Loss: 4.3728\n",
      "Epoch: 035/050 | Batch 00100/00302 | Loss: 4.7500\n",
      "Epoch: 035/050 | Batch 00200/00302 | Loss: 4.8649\n",
      "Epoch: 035/050 | Batch 00300/00302 | Loss: 4.4082\n",
      "Epoch: 036/050 | Batch 00000/00302 | Loss: 4.2192\n",
      "Epoch: 036/050 | Batch 00100/00302 | Loss: 4.8156\n",
      "Epoch: 036/050 | Batch 00200/00302 | Loss: 4.5280\n",
      "Epoch: 036/050 | Batch 00300/00302 | Loss: 4.7417\n",
      "Epoch: 037/050 | Batch 00000/00302 | Loss: 4.0187\n",
      "Epoch: 037/050 | Batch 00100/00302 | Loss: 4.5149\n",
      "Epoch: 037/050 | Batch 00200/00302 | Loss: 4.8251\n",
      "Epoch: 037/050 | Batch 00300/00302 | Loss: 4.3771\n",
      "Epoch: 038/050 | Batch 00000/00302 | Loss: 4.4069\n",
      "Epoch: 038/050 | Batch 00100/00302 | Loss: 4.4358\n",
      "Epoch: 038/050 | Batch 00200/00302 | Loss: 4.1975\n",
      "Epoch: 038/050 | Batch 00300/00302 | Loss: 4.5042\n",
      "Epoch: 039/050 | Batch 00000/00302 | Loss: 4.2105\n",
      "Epoch: 039/050 | Batch 00100/00302 | Loss: 4.8306\n",
      "Epoch: 039/050 | Batch 00200/00302 | Loss: 4.9135\n",
      "Epoch: 039/050 | Batch 00300/00302 | Loss: 4.6590\n",
      "Epoch: 040/050 | Batch 00000/00302 | Loss: 4.1934\n",
      "Epoch: 040/050 | Batch 00100/00302 | Loss: 4.2345\n",
      "Epoch: 040/050 | Batch 00200/00302 | Loss: 4.5257\n",
      "Epoch: 040/050 | Batch 00300/00302 | Loss: 4.8285\n",
      "Epoch: 041/050 | Batch 00000/00302 | Loss: 4.1850\n",
      "Epoch: 041/050 | Batch 00100/00302 | Loss: 4.6449\n",
      "Epoch: 041/050 | Batch 00200/00302 | Loss: 4.5787\n",
      "Epoch: 041/050 | Batch 00300/00302 | Loss: 4.0541\n",
      "Epoch: 042/050 | Batch 00000/00302 | Loss: 4.1903\n",
      "Epoch: 042/050 | Batch 00100/00302 | Loss: 4.9387\n",
      "Epoch: 042/050 | Batch 00200/00302 | Loss: 4.9377\n",
      "Epoch: 042/050 | Batch 00300/00302 | Loss: 4.9727\n",
      "Epoch: 043/050 | Batch 00000/00302 | Loss: 4.8173\n",
      "Epoch: 043/050 | Batch 00100/00302 | Loss: 4.0914\n",
      "Epoch: 043/050 | Batch 00200/00302 | Loss: 4.9931\n",
      "Epoch: 043/050 | Batch 00300/00302 | Loss: 5.1867\n",
      "Epoch: 044/050 | Batch 00000/00302 | Loss: 4.7749\n",
      "Epoch: 044/050 | Batch 00100/00302 | Loss: 4.7350\n",
      "Epoch: 044/050 | Batch 00200/00302 | Loss: 4.8146\n",
      "Epoch: 044/050 | Batch 00300/00302 | Loss: 5.1419\n",
      "Epoch: 045/050 | Batch 00000/00302 | Loss: 4.6821\n",
      "Epoch: 045/050 | Batch 00100/00302 | Loss: 4.7511\n",
      "Epoch: 045/050 | Batch 00200/00302 | Loss: 4.3573\n",
      "Epoch: 045/050 | Batch 00300/00302 | Loss: 4.5913\n",
      "Epoch: 046/050 | Batch 00000/00302 | Loss: 4.1146\n",
      "Epoch: 046/050 | Batch 00100/00302 | Loss: 4.7672\n",
      "Epoch: 046/050 | Batch 00200/00302 | Loss: 4.2664\n",
      "Epoch: 046/050 | Batch 00300/00302 | Loss: 5.1007\n",
      "Epoch: 047/050 | Batch 00000/00302 | Loss: 3.9370\n",
      "Epoch: 047/050 | Batch 00100/00302 | Loss: 4.8315\n",
      "Epoch: 047/050 | Batch 00200/00302 | Loss: 4.5877\n",
      "Epoch: 047/050 | Batch 00300/00302 | Loss: 4.8792\n",
      "Epoch: 048/050 | Batch 00000/00302 | Loss: 4.9568\n",
      "Epoch: 048/050 | Batch 00100/00302 | Loss: 4.7746\n",
      "Epoch: 048/050 | Batch 00200/00302 | Loss: 4.9511\n",
      "Epoch: 048/050 | Batch 00300/00302 | Loss: 5.0030\n",
      "Epoch: 049/050 | Batch 00000/00302 | Loss: 4.5055\n",
      "Epoch: 049/050 | Batch 00100/00302 | Loss: 4.9350\n",
      "Epoch: 049/050 | Batch 00200/00302 | Loss: 4.4826\n",
      "Epoch: 049/050 | Batch 00300/00302 | Loss: 4.7984\n",
      "Epoch: 050/050 | Batch 00000/00302 | Loss: 4.7467\n",
      "Epoch: 050/050 | Batch 00100/00302 | Loss: 4.2988\n",
      "Epoch: 050/050 | Batch 00200/00302 | Loss: 4.4278\n",
      "Epoch: 050/050 | Batch 00300/00302 | Loss: 4.6709\n"
     ]
    }
   ],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (t, o) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=t.size(0))\n",
    "            output, hidden = model(t.to(device), hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), o.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "    torch.save(model.state_dict(), \"poetry-gen.pth\")\n",
    "\n",
    "\n",
    "train(model, num_epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"poetry-gen.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['海', '内', '存', '知', '己']\n",
      "海内存知己出天门横，水摇天门关，水明秋浦秋。风吹断断断断肠断断，心断断断不能断。\n",
      "['江']\n",
      "江水横天上，天门落天门。天上有所欢，不知长门宫里人。\n",
      "['山']\n",
      "山水横天门，天上有云间。天上有所得，不知长门宫里人。\n",
      "['烟']\n",
      "烟水明镜中，天上有风尘。\n"
     ]
    }
   ],
   "source": [
    "def make_one_hot_vec_target(word, word_to_index):\n",
    "    rst = autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "    return rst\n",
    "\n",
    "\n",
    "def generate_text(start_word=\"<START>\", top_k=1):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for word in words:\n",
    "            input_vector = make_one_hot_vec_target(word, word_to_index).unsqueeze(0)\n",
    "            generated_text += word\n",
    "\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output, hidden_state = model(input_vector.to(device), hidden_state)\n",
    "            top_values, top_indices = output.data.topk(top_k)\n",
    "            if top_k == 1:\n",
    "                selected_index = top_indices.item()\n",
    "            else:\n",
    "                top_indices = top_indices.squeeze()\n",
    "                selected_index = top_indices[random.randint(0, top_k - 1)].item()\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            input_vector = make_one_hot_vec_target(next_word, word_to_index).unsqueeze(0)\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"海内存知己\", top_k=2))\n",
    "print(generate_text(\"江\"))\n",
    "print(generate_text(\"山\"))\n",
    "print(generate_text(\"烟\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
