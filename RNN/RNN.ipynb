{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 128\n",
    "max_length = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3482\n",
      "data_size 1287\n"
     ]
    }
   ],
   "source": [
    "with open(\"poems.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    poems = json.load(f)\n",
    "\n",
    "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "poems = [list(poem) + [\"<EOP>\"] for poem in poems]\n",
    "index_tensors = {\n",
    "    word: torch.LongTensor([word_to_index[word]]) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(poem):\n",
    "\n",
    "    inputs = [index_tensors[poem[i - 1]] for i in range(1, len(poem))]\n",
    "    outputs = [index_tensors[poem[i]] for i in range(1, len(poem))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_inputs = nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_outputs = nn.utils.rnn.pad_sequence(\n",
    "        outputs, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_inputs, padded_outputs\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 在循环神经网络（RNN）中\n",
    "        # 当前时刻的隐藏状态是由当前时刻的输入和上一个时刻的隐藏状态共同决定的。\n",
    "        # RNN 的核心就是隐藏状态的更新\n",
    "        self.input_to_hidden = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.tanh(self.input_to_hidden(combined))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = RNN(embedding_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # embedding: 1,5 -> 1,5,256\n",
    "        embeds = self.embeddings(input)\n",
    "        batch_size, seq_len, _ = embeds.size()\n",
    "        outputs = []\n",
    "        for i in range(seq_len):\n",
    "            # torch.Size([1, 256]) torch.Size([1, 512])\n",
    "            hidden = self.rnn(embeds[:, i, :], hidden)\n",
    "            outputs.append(hidden)\n",
    "        rnn_out = torch.stack(outputs, dim=1)\n",
    "        # print(rnn_out.size())\n",
    "        # torch.Size([1, 5, 512])\n",
    "        output = self.linear1(F.relu(rnn_out))\n",
    "        # print(output.size())\n",
    "        # torch.Size([1, 5, 3482])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        return torch.zeros(batch_size, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, scheduler, vocab_size):\n",
    "    log_dict = {\n",
    "        \"train_loss_per_epoch\": [],\n",
    "        \"train_perplexity_per_epoch\": [],\n",
    "    }\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Current Learning Rate: {current_lr:.6f}\"\n",
    "        )\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=inputs.size(0))\n",
    "            output, hidden = model(inputs.to(device), hidden)\n",
    "\n",
    "            # print(output.shape, targets.shape)\n",
    "            # torch.Size([16, 120, 3482]) torch.Size([16, 120])\n",
    "            # print(output.view(-1, vocab_size).shape, targets.view(-1).shape)\n",
    "            # torch.Size([1920, 3482]) torch.Size([1920])\n",
    "            # 使用view函数调整输出和目标的形状以匹配损失函数的期望输入\n",
    "            # output的原始形状是[批次大小, 序列长度, 词汇表大小]，targets的原始形状是[批次大小, 序列长度]\n",
    "            # view(-1, vocab_size)将output重塑为[批次大小*序列长度, 词汇表大小]，以匹配每个时间步的预测\n",
    "            # targets通过view(-1)被重塑为[批次大小*序列长度]，这样每个预测都有一个对应的目标值\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if not batch_idx % 50:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx + 1:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "        log_dict[\"train_loss_per_epoch\"].append(avg_loss)\n",
    "        log_dict[\"train_perplexity_per_epoch\"].append(perplexity)\n",
    "\n",
    "        print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/ 60:.2f} min\")\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PoetryModel                              [1, 1, 3482]              --\n",
       "├─Embedding: 1-1                         [1, 1, 256]               891,392\n",
       "├─RNN: 1-2                               [1, 512]                  --\n",
       "│    └─Linear: 2-1                       [1, 512]                  393,728\n",
       "│    └─Tanh: 2-2                         [1, 512]                  --\n",
       "├─Linear: 1-3                            [1, 1, 3482]              1,786,266\n",
       "==========================================================================================\n",
       "Total params: 3,071,386\n",
       "Trainable params: 3,071,386\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 3.07\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 12.29\n",
       "Estimated Total Size (MB): 12.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "def plot_training_stats(log_dict):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log_dict[\"train_loss_per_epoch\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(log_dict[\"train_perplexity_per_epoch\"], label=\"Training Perplexity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training Perplexity\")\n",
    "    plt.savefig(\"training_stats.svg\")\n",
    "    plt.show()\n",
    "\n",
    "model = PoetryModel(vocab_size=len(word_to_index), embedding_dim=256, hidden_dim=512)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<START>\"], reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=9, verbose=True\n",
    ")\n",
    "# log_dict = train(\n",
    "#     model, num_epochs, data_loader, optimizer, criterion, scheduler, vocab_size\n",
    "# )\n",
    "# plot_training_stats(log_dict)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "inputs = torch.tensor([[1]]).to(device)\n",
    "hidden = model.initHidden(device=device, batch_size=inputs.size(0))\n",
    "summary(model, input_data=(inputs, hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长安一片月，万户捣衣声。秋风吹不尽，总是玉关情。何日平胡虏，良人罢远征。\n",
      "江祖一片石，青天扫画屏。题诗留万古，绿字锦苔生。\n",
      "月露发光彩，此时方见秋。夜凉金气应，天静火星流。蛩响偏依井，萤飞直过楼。相知尽白首，清景复追游。\n",
      "泉眼不动月，长江夜夜深。独有余霞意，相期在不见。\n",
      "日观化门来，登连城下尘。借问剡昔相，常恐沧年间。高谈出佳人，从此游应迷。\n",
      "烟: 0.7222\n",
      "吹: 0.1804\n",
      "露: 0.0974\n",
      "风\n",
      "纪: 0.9608\n",
      "里: 0.0236\n",
      "起: 0.0156\n",
      "风烟\n",
      "南: 0.9899\n",
      "江: 0.0058\n",
      "海: 0.0043\n",
      "风烟纪\n",
      "城: 0.9989\n",
      "山: 0.0008\n",
      "都: 0.0003\n",
      "风烟纪南\n",
      "，: 0.9996\n",
      "。: 0.0003\n",
      "头: 0.0001\n",
      "风烟纪南城\n",
      "尘: 0.9960\n",
      "水: 0.0031\n",
      "旌: 0.0008\n",
      "风烟纪南城，\n",
      "土: 0.9919\n",
      "水: 0.0045\n",
      "户: 0.0035\n",
      "风烟纪南城，尘\n",
      "荆: 0.9969\n",
      "青: 0.0019\n",
      "今: 0.0012\n",
      "风烟纪南城，尘土\n",
      "门: 0.9994\n",
      "青: 0.0004\n",
      "城: 0.0002\n",
      "风烟纪南城，尘土荆\n",
      "路: 0.9990\n",
      "城: 0.0006\n",
      "东: 0.0004\n",
      "风烟纪南城，尘土荆门\n",
      "。: 1.0000\n",
      "，: 0.0000\n",
      "劒: 0.0000\n",
      "风烟纪南城，尘土荆门路\n",
      "天: 0.9995\n",
      "相: 0.0003\n",
      "江: 0.0002\n",
      "风烟纪南城，尘土荆门路。\n",
      "寒: 0.9949\n",
      "山: 0.0038\n",
      "河: 0.0014\n",
      "风烟纪南城，尘土荆门路。天\n",
      "多: 0.6247\n",
      "猎: 0.3635\n",
      "不: 0.0118\n",
      "风烟纪南城，尘土荆门路。天寒\n",
      "兽: 0.8848\n",
      "猎: 0.0994\n",
      "北: 0.0158\n",
      "风烟纪南城，尘土荆门路。天寒猎\n",
      "者: 0.9928\n",
      "灭: 0.0063\n",
      "扇: 0.0009\n",
      "风烟纪南城，尘土荆门路。天寒猎兽\n",
      "，: 1.0000\n",
      "出: 0.0000\n",
      "庭: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者\n",
      "走: 0.9733\n",
      "海: 0.0229\n",
      "鸟: 0.0039\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，\n",
      "上: 0.9997\n",
      "动: 0.0002\n",
      "杀: 0.0001\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走\n",
      "樊: 0.9728\n",
      "元: 0.0160\n",
      "占: 0.0112\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上\n",
      "姬: 0.9932\n",
      "年: 0.0037\n",
      "应: 0.0032\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊\n",
      "墓: 0.9964\n",
      "夫: 0.0020\n",
      "笙: 0.0015\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬\n",
      "。: 1.0000\n",
      "，: 0.0000\n",
      "意: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬墓\n",
      "<EOP>: 0.9999\n",
      "玉: 0.0000\n",
      "日: 0.0000\n",
      "风烟纪南城，尘土荆门路。天寒猎兽者，走上樊姬墓。\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_word=\"<START>\", top_k=1, temperature=0.7, log=False):\n",
    "    generated_text = \"\"\n",
    "    index_tensors_list = []\n",
    "    for word in start_word:\n",
    "        index_tensors_list.append(index_tensors[word].unsqueeze(0))\n",
    "        generated_text += word\n",
    "\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_length - len(generated_text)):\n",
    "\n",
    "            input_tensor = torch.tensor(index_tensors_list).unsqueeze(0).to(device)\n",
    "\n",
    "            output, hidden_state = model(input_tensor.to(device), hidden_state)\n",
    "            # print(output.shape)\n",
    "            # torch.Size([1, 5, 3482])\n",
    "            # 切片\n",
    "            last_word = output[:, -1, :]\n",
    "            # print(last_word.shape)\n",
    "            # torch.Size([1, 3482])\n",
    "            last_word = last_word.view(-1)\n",
    "            # print(last_word.shape)\n",
    "            # torch.Size([3482])\n",
    "            \n",
    "            # 调整温度\n",
    "            # softmax 函数倾向于增强输入向量中最大值的影响\n",
    "            scaled_logits = last_word / temperature\n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "            probabilities, top_indices = probabilities.data.topk(top_k)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "            probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "            probabilities_np = probabilities.cpu().numpy()\n",
    "            indices_np = top_indices.cpu().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "            next_word = index_to_word[selected_index]\n",
    "\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "\n",
    "            index_tensors_list = [index_tensors[next_word]]\n",
    "            generated_text += next_word\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"长安一片月\", top_k=1))\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "print(generate_text(\"月\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"日\", top_k=30))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
