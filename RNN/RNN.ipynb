{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['菩萨蛮 其一\\n小山重叠金明灭，鬓云欲度香腮雪。\\n懒起画蛾眉，弄妆梳洗迟。\\n照花前后镜，花面交相映。\\n新帖绣罗襦，双双金鹧鸪。', '菩萨蛮 其二\\n水晶帘里玻璃枕，暖香惹梦鸳鸯锦。\\n江上柳如烟，雁飞残月天。\\n藕丝秋色浅，人胜参差剪。\\n双鬓隔香红，玉钗头上风。', '菩萨蛮 其三\\n蕊黄无限当山额，宿妆隐笑纱窗隔。\\n相见牡丹时，暂来还别离。\\n翠钗金作股，钗上蝶双舞。\\n心事竟谁知，月明花满枝。', '菩萨蛮 其四\\n翠翘金缕双㶉𫛶，水纹细起春池碧。\\n池上海棠梨，雨晴红满枝。\\n绣衫遮笑靥，烟草粘飞蝶。\\n青琐对芳菲，玉关音信稀。', '菩萨蛮 其五\\n杏花含露团香雪，绿杨陌上多离别。\\n灯在月胧明，觉来闻晓莺。\\n玉钩褰翠幕，妆浅旧眉薄。\\n春梦正关情，镜中蝉鬓轻。']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/train.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "paragraphs = [para.strip() for para in text.split(\"<|endoftext|>\") if para.strip()]\n",
    "\n",
    "print(paragraphs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小: 9234\n"
     ]
    }
   ],
   "source": [
    "char_to_id = {}\n",
    "id_to_char = {}\n",
    "\n",
    "\n",
    "# 遍历数据，更新字符映射\n",
    "chars = sorted(set(text))\n",
    "char_to_id = {ch: i + 2 for i, ch in enumerate(chars)}\n",
    "id_to_char = {i + 2: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "char_to_id[\"<pad>\"] = 0\n",
    "char_to_id[\"<eos>\"] = 1\n",
    "id_to_char[0] = \"<pad>\"\n",
    "id_to_char[1] = \"<eos>\"\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "print(\"字典大小: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6333, 6360, 6613, 3, 712, 300, 2, 1907, 1959, 7758, 1033, 7761, 3116, 4234, 8820, 8499, 380, 3654, 2216, 8361, 6011, 8095, 86, 2, 2605, 7254, 4693, 6621, 4899, 8820, 2257, 1645, 3450, 3891, 7502, 86, 2, 4319, 6161, 839, 1065, 7918, 8820, 6161, 8152, 390, 4889, 3124, 86, 2, 3061, 2140, 5666, 5758, 6882, 8820, 1022, 1022, 7761, 8694, 8642, 86, 1], [6333, 6360, 6613, 3, 712, 377, 2, 3764, 3169, 2141, 7757, 4538, 4619, 3301, 8820, 3183, 8361, 2526, 3442, 8650, 8646, 7884, 86, 2, 3786, 306, 3357, 1642, 4278, 8820, 8077, 8296, 3701, 3219, 1596, 86, 2, 6514, 322, 5186, 6122, 3920, 8820, 407, 5948, 1014, 2115, 853, 86, 2, 1022, 8499, 8059, 8361, 5603, 8820, 4510, 7808, 1605, 306, 8283, 86, 1], [6333, 6360, 6613, 3, 712, 305, 2, 6463, 8732, 3088, 8029, 2289, 1959, 8270, 8820, 1865, 1645, 8058, 5331, 5618, 5283, 8059, 86, 2, 4889, 6904, 4398, 338, 3101, 8820, 3176, 3273, 7496, 817, 5176, 86, 2, 5831, 7808, 7761, 498, 5913, 8820, 7808, 306, 6681, 1022, 6084, 86, 2, 2348, 376, 5311, 7071, 4992, 8820, 3219, 3116, 6161, 4110, 3306, 86, 1], [6333, 6360, 6613, 3, 712, 1376, 2, 5831, 5826, 7761, 5708, 1022, 191, 9170, 8820, 3764, 5626, 5638, 7254, 3125, 3787, 5077, 86, 2, 3787, 306, 3947, 3469, 3444, 8820, 8093, 3168, 5603, 4110, 3306, 86, 2, 5666, 6767, 7585, 5331, 8154, 8820, 4278, 6249, 5506, 8296, 6681, 86, 2, 8143, 4569, 1894, 6162, 6337, 8820, 4510, 709, 8213, 562, 5210, 86, 1], [6333, 6360, 6613, 3, 712, 382, 2, 3260, 6161, 1086, 8132, 1380, 8361, 8095, 8820, 5690, 3274, 8027, 306, 1591, 5176, 817, 86, 2, 4235, 1408, 3219, 5955, 3116, 8820, 6911, 3273, 7964, 3150, 6308, 86, 2, 4510, 7818, 6862, 5831, 2170, 8820, 1645, 3920, 3092, 4899, 6481, 86, 2, 3125, 3442, 3681, 709, 2493, 8820, 7918, 331, 6661, 8499, 7437, 86, 1]]\n"
     ]
    }
   ],
   "source": [
    "# df[\"char_id_list\"] = df[\"Comment\"].apply(\n",
    "# lambda text: [char_to_id[char] for char in list(text)] + [char_to_id[\"<eos>\"]]\n",
    "# )\n",
    "# df.head()\n",
    "\n",
    "char_id_lists = []\n",
    "for item in paragraphs:\n",
    "    char_ids = [char_to_id[char] for char in item] + [char_to_id[\"<eos>\"]]\n",
    "    char_id_lists.append(char_ids)\n",
    "\n",
    "print(char_id_lists[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "embed_dim = 50\n",
    "hidden_dim = 30\n",
    "lr = 0.001\n",
    "grad_clip = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"now using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # x = self.sequences.iloc[index][:-1]\n",
    "        # y = self.sequences.iloc[index][1:]\n",
    "        x = self.sequences[index][:-1]\n",
    "        y = self.sequences[index][1:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_x = [torch.tensor(data[0]) for data in batch]\n",
    "    batch_y = [torch.tensor(data[1]) for data in batch]\n",
    "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
    "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
    "\n",
    "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch_x, batch_first=True, padding_value=char_to_id[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch_y, batch_first=True, padding_value=char_to_id[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset(df[\"char_id_list\"])\n",
    "dataset = Dataset(char_id_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # initialize the weights\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size)\n",
    "        # initialize the hidden state\n",
    "        self.h = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=char_to_id[\"<pad>\"],\n",
    "        )\n",
    "\n",
    "        self.rnn_layer1 = torch.nn.LSTM(\n",
    "            input_size=embed_dim, hidden_size=hidden_dim, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.rnn_layer2 = torch.nn.LSTM(\n",
    "            input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "\n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        batch_x = self.embedding(batch_x)\n",
    "\n",
    "        batch_x_lens = batch_x_lens.cpu()\n",
    "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            batch_x, batch_x_lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        batch_x, _ = self.rnn_layer1(batch_x)\n",
    "        batch_x, _ = self.rnn_layer2(batch_x)\n",
    "\n",
    "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x, batch_first=True)\n",
    "\n",
    "        batch_x = self.linear(batch_x)\n",
    "\n",
    "        return batch_x\n",
    "\n",
    "    def generator(self, start_char, max_len=50, top_n=5):\n",
    "        char_list = [char_to_id[start_char]]\n",
    "        next_char = None\n",
    "\n",
    "        while len(char_list) < max_len:\n",
    "            x = torch.LongTensor(char_list).unsqueeze(0)\n",
    "            x = self.embedding(x)\n",
    "            _, (ht, _) = self.rnn_layer1(x)\n",
    "            _, (ht, _) = self.rnn_layer2(ht)\n",
    "            y = self.linear(ht.squeeze(0))\n",
    "\n",
    "            # 获取前 top_n 大的字符的索引\n",
    "            top_n_values, top_n_indices = torch.topk(y, top_n)\n",
    "            top_n_indices = top_n_indices.cpu().numpy()\n",
    "\n",
    "            # 随机选择一个索引\n",
    "            if top_n > 1:\n",
    "                next_char = np.random.choice(top_n_indices[0])\n",
    "            else:\n",
    "                next_char = top_n_indices[0][0]\n",
    "\n",
    "            if next_char == char_to_id[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "            char_list.append(next_char)\n",
    "\n",
    "        return [id_to_char[ch_id] for ch_id in char_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "model = CharRNN(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    ignore_index=char_to_id[\"<pad>\"], reduction=\"mean\"\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 0000/8876 | Loss: 5.8432\n",
      "Epoch: 001/100 | Batch 0100/8876 | Loss: 5.5244\n",
      "Epoch: 001/100 | Batch 0200/8876 | Loss: 5.7933\n",
      "Epoch: 001/100 | Batch 0300/8876 | Loss: 6.0157\n",
      "Epoch: 001/100 | Batch 0400/8876 | Loss: 5.8704\n",
      "Epoch: 001/100 | Batch 0500/8876 | Loss: 5.6514\n",
      "Epoch: 001/100 | Batch 0600/8876 | Loss: 5.8754\n",
      "Epoch: 001/100 | Batch 0700/8876 | Loss: 6.0158\n",
      "Epoch: 001/100 | Batch 0800/8876 | Loss: 5.8273\n",
      "Epoch: 001/100 | Batch 0900/8876 | Loss: 5.8007\n",
      "Epoch: 001/100 | Batch 1000/8876 | Loss: 6.0238\n",
      "Epoch: 001/100 | Batch 1100/8876 | Loss: 5.7819\n",
      "Epoch: 001/100 | Batch 1200/8876 | Loss: 5.8684\n",
      "Epoch: 001/100 | Batch 1300/8876 | Loss: 5.9937\n",
      "Epoch: 001/100 | Batch 1400/8876 | Loss: 5.7791\n",
      "Epoch: 001/100 | Batch 1500/8876 | Loss: 5.7885\n",
      "Epoch: 001/100 | Batch 1600/8876 | Loss: 5.4704\n",
      "Epoch: 001/100 | Batch 1700/8876 | Loss: 5.5085\n",
      "Epoch: 001/100 | Batch 1800/8876 | Loss: 5.6321\n",
      "Epoch: 001/100 | Batch 1900/8876 | Loss: 5.8457\n",
      "Epoch: 001/100 | Batch 2000/8876 | Loss: 5.7386\n",
      "Epoch: 001/100 | Batch 2100/8876 | Loss: 5.9396\n",
      "Epoch: 001/100 | Batch 2200/8876 | Loss: 5.7588\n",
      "Epoch: 001/100 | Batch 2300/8876 | Loss: 5.7377\n",
      "Epoch: 001/100 | Batch 2400/8876 | Loss: 5.8746\n",
      "Epoch: 001/100 | Batch 2500/8876 | Loss: 5.8129\n",
      "Epoch: 001/100 | Batch 2600/8876 | Loss: 5.8587\n",
      "Epoch: 001/100 | Batch 2700/8876 | Loss: 5.3748\n",
      "Epoch: 001/100 | Batch 2800/8876 | Loss: 5.6932\n",
      "Epoch: 001/100 | Batch 2900/8876 | Loss: 5.7757\n",
      "Epoch: 001/100 | Batch 3000/8876 | Loss: 5.4359\n",
      "Epoch: 001/100 | Batch 3100/8876 | Loss: 5.8037\n",
      "Epoch: 001/100 | Batch 3200/8876 | Loss: 5.8342\n",
      "Epoch: 001/100 | Batch 3300/8876 | Loss: 5.9765\n",
      "Epoch: 001/100 | Batch 3400/8876 | Loss: 5.6886\n",
      "Epoch: 001/100 | Batch 3500/8876 | Loss: 5.4974\n",
      "Epoch: 001/100 | Batch 3600/8876 | Loss: 5.5428\n",
      "Epoch: 001/100 | Batch 3700/8876 | Loss: 5.6893\n",
      "Epoch: 001/100 | Batch 3800/8876 | Loss: 5.2729\n",
      "Epoch: 001/100 | Batch 3900/8876 | Loss: 5.3974\n",
      "Epoch: 001/100 | Batch 4000/8876 | Loss: 5.6379\n",
      "Epoch: 001/100 | Batch 4100/8876 | Loss: 5.4305\n",
      "Epoch: 001/100 | Batch 4200/8876 | Loss: 5.7778\n",
      "Epoch: 001/100 | Batch 4300/8876 | Loss: 5.3075\n",
      "Epoch: 001/100 | Batch 4400/8876 | Loss: 5.4197\n",
      "Epoch: 001/100 | Batch 4500/8876 | Loss: 5.8475\n",
      "Epoch: 001/100 | Batch 4600/8876 | Loss: 5.9421\n",
      "Epoch: 001/100 | Batch 4700/8876 | Loss: 5.8048\n",
      "Epoch: 001/100 | Batch 4800/8876 | Loss: 5.8911\n",
      "Epoch: 001/100 | Batch 4900/8876 | Loss: 5.7747\n",
      "Epoch: 001/100 | Batch 5000/8876 | Loss: 5.8953\n",
      "Epoch: 001/100 | Batch 5100/8876 | Loss: 5.4886\n",
      "Epoch: 001/100 | Batch 5200/8876 | Loss: 5.6656\n",
      "Epoch: 001/100 | Batch 5300/8876 | Loss: 5.5485\n",
      "Epoch: 001/100 | Batch 5400/8876 | Loss: 5.6040\n",
      "Epoch: 001/100 | Batch 5500/8876 | Loss: 5.3236\n",
      "Epoch: 001/100 | Batch 5600/8876 | Loss: 5.1707\n",
      "Epoch: 001/100 | Batch 5700/8876 | Loss: 5.7135\n",
      "Epoch: 001/100 | Batch 5800/8876 | Loss: 5.6084\n",
      "Epoch: 001/100 | Batch 5900/8876 | Loss: 5.6576\n",
      "Epoch: 001/100 | Batch 6000/8876 | Loss: 5.4328\n",
      "Epoch: 001/100 | Batch 6100/8876 | Loss: 5.6580\n",
      "Epoch: 001/100 | Batch 6200/8876 | Loss: 5.1625\n",
      "Epoch: 001/100 | Batch 6300/8876 | Loss: 5.2710\n",
      "Epoch: 001/100 | Batch 6400/8876 | Loss: 5.7809\n",
      "Epoch: 001/100 | Batch 6500/8876 | Loss: 5.3711\n",
      "Epoch: 001/100 | Batch 6600/8876 | Loss: 5.8396\n",
      "Epoch: 001/100 | Batch 6700/8876 | Loss: 5.6141\n",
      "Epoch: 001/100 | Batch 6800/8876 | Loss: 5.6507\n",
      "Epoch: 001/100 | Batch 6900/8876 | Loss: 5.8328\n",
      "Epoch: 001/100 | Batch 7000/8876 | Loss: 5.5366\n",
      "Epoch: 001/100 | Batch 7100/8876 | Loss: 5.2098\n",
      "Epoch: 001/100 | Batch 7200/8876 | Loss: 5.4847\n",
      "Epoch: 001/100 | Batch 7300/8876 | Loss: 5.7081\n",
      "Epoch: 001/100 | Batch 7400/8876 | Loss: 5.3644\n",
      "Epoch: 001/100 | Batch 7500/8876 | Loss: 5.3951\n",
      "Epoch: 001/100 | Batch 7600/8876 | Loss: 5.7755\n",
      "Epoch: 001/100 | Batch 7700/8876 | Loss: 5.6193\n",
      "Epoch: 001/100 | Batch 7800/8876 | Loss: 5.6176\n",
      "Epoch: 001/100 | Batch 7900/8876 | Loss: 5.3689\n",
      "Epoch: 001/100 | Batch 8000/8876 | Loss: 5.1531\n",
      "Epoch: 001/100 | Batch 8100/8876 | Loss: 5.3335\n",
      "Epoch: 001/100 | Batch 8200/8876 | Loss: 5.3989\n",
      "Epoch: 001/100 | Batch 8300/8876 | Loss: 5.3855\n",
      "Epoch: 001/100 | Batch 8400/8876 | Loss: 5.5336\n",
      "Epoch: 001/100 | Batch 8500/8876 | Loss: 5.3697\n",
      "Epoch: 001/100 | Batch 8600/8876 | Loss: 5.4360\n",
      "Epoch: 001/100 | Batch 8700/8876 | Loss: 5.4578\n",
      "Epoch: 001/100 | Batch 8800/8876 | Loss: 5.4752\n",
      "月中\n",
      "君人年一月山春人颂颂十十二事其其\n",
      "春风花山水春中颂颂\n",
      "无无无无自自其事，不是无有有不人中游中居\n",
      "Epoch: 002/100 | Batch 0000/8876 | Loss: 5.1958\n",
      "Epoch: 002/100 | Batch 0100/8876 | Loss: 5.4026\n",
      "Epoch: 002/100 | Batch 0200/8876 | Loss: 5.3142\n",
      "Epoch: 002/100 | Batch 0300/8876 | Loss: 5.4407\n",
      "Epoch: 002/100 | Batch 0400/8876 | Loss: 5.5479\n",
      "Epoch: 002/100 | Batch 0500/8876 | Loss: 5.5033\n",
      "Epoch: 002/100 | Batch 0600/8876 | Loss: 5.8474\n",
      "Epoch: 002/100 | Batch 0700/8876 | Loss: 5.0399\n",
      "Epoch: 002/100 | Batch 0800/8876 | Loss: 5.4215\n",
      "Epoch: 002/100 | Batch 0900/8876 | Loss: 5.5071\n",
      "Epoch: 002/100 | Batch 1000/8876 | Loss: 5.6126\n",
      "Epoch: 002/100 | Batch 1100/8876 | Loss: 5.2097\n",
      "Epoch: 002/100 | Batch 1200/8876 | Loss: 5.2786\n",
      "Epoch: 002/100 | Batch 1300/8876 | Loss: 5.6193\n",
      "Epoch: 002/100 | Batch 1400/8876 | Loss: 5.5178\n",
      "Epoch: 002/100 | Batch 1500/8876 | Loss: 5.3480\n",
      "Epoch: 002/100 | Batch 1600/8876 | Loss: 5.4963\n",
      "Epoch: 002/100 | Batch 1700/8876 | Loss: 5.1659\n",
      "Epoch: 002/100 | Batch 1800/8876 | Loss: 5.3950\n",
      "Epoch: 002/100 | Batch 1900/8876 | Loss: 5.8224\n",
      "Epoch: 002/100 | Batch 2000/8876 | Loss: 5.4312\n",
      "Epoch: 002/100 | Batch 2100/8876 | Loss: 5.3272\n",
      "Epoch: 002/100 | Batch 2200/8876 | Loss: 5.1693\n",
      "Epoch: 002/100 | Batch 2300/8876 | Loss: 5.1552\n",
      "Epoch: 002/100 | Batch 2400/8876 | Loss: 5.2299\n",
      "Epoch: 002/100 | Batch 2500/8876 | Loss: 5.6266\n",
      "Epoch: 002/100 | Batch 2600/8876 | Loss: 5.5106\n",
      "Epoch: 002/100 | Batch 2700/8876 | Loss: 5.7363\n",
      "Epoch: 002/100 | Batch 2800/8876 | Loss: 5.5701\n",
      "Epoch: 002/100 | Batch 2900/8876 | Loss: 5.5254\n",
      "Epoch: 002/100 | Batch 3000/8876 | Loss: 5.7657\n",
      "Epoch: 002/100 | Batch 3100/8876 | Loss: 5.6139\n",
      "Epoch: 002/100 | Batch 3200/8876 | Loss: 5.6174\n",
      "Epoch: 002/100 | Batch 3300/8876 | Loss: 5.2411\n",
      "Epoch: 002/100 | Batch 3400/8876 | Loss: 5.4063\n",
      "Epoch: 002/100 | Batch 3500/8876 | Loss: 5.3212\n",
      "Epoch: 002/100 | Batch 3600/8876 | Loss: 5.5934\n",
      "Epoch: 002/100 | Batch 3700/8876 | Loss: 4.8366\n",
      "Epoch: 002/100 | Batch 3800/8876 | Loss: 5.5622\n",
      "Epoch: 002/100 | Batch 3900/8876 | Loss: 5.2209\n",
      "Epoch: 002/100 | Batch 4000/8876 | Loss: 5.0864\n",
      "Epoch: 002/100 | Batch 4100/8876 | Loss: 5.2685\n",
      "Epoch: 002/100 | Batch 4200/8876 | Loss: 5.7500\n",
      "Epoch: 002/100 | Batch 4300/8876 | Loss: 5.4033\n",
      "Epoch: 002/100 | Batch 4400/8876 | Loss: 5.4577\n",
      "Epoch: 002/100 | Batch 4500/8876 | Loss: 5.1840\n",
      "Epoch: 002/100 | Batch 4600/8876 | Loss: 5.2970\n",
      "Epoch: 002/100 | Batch 4700/8876 | Loss: 5.2272\n",
      "Epoch: 002/100 | Batch 4800/8876 | Loss: 5.6083\n",
      "Epoch: 002/100 | Batch 4900/8876 | Loss: 5.4110\n",
      "Epoch: 002/100 | Batch 5000/8876 | Loss: 5.2341\n",
      "Epoch: 002/100 | Batch 5100/8876 | Loss: 5.2215\n",
      "Epoch: 002/100 | Batch 5200/8876 | Loss: 4.9778\n",
      "Epoch: 002/100 | Batch 5300/8876 | Loss: 5.0641\n",
      "Epoch: 002/100 | Batch 5400/8876 | Loss: 5.9514\n",
      "Epoch: 002/100 | Batch 5500/8876 | Loss: 5.1582\n",
      "Epoch: 002/100 | Batch 5600/8876 | Loss: 5.4771\n",
      "Epoch: 002/100 | Batch 5700/8876 | Loss: 5.2881\n",
      "Epoch: 002/100 | Batch 5800/8876 | Loss: 5.5857\n",
      "Epoch: 002/100 | Batch 5900/8876 | Loss: 5.3197\n",
      "Epoch: 002/100 | Batch 6000/8876 | Loss: 5.6524\n",
      "Epoch: 002/100 | Batch 6100/8876 | Loss: 5.3585\n",
      "Epoch: 002/100 | Batch 6200/8876 | Loss: 5.6966\n",
      "Epoch: 002/100 | Batch 6300/8876 | Loss: 4.9652\n",
      "Epoch: 002/100 | Batch 6400/8876 | Loss: 5.2539\n",
      "Epoch: 002/100 | Batch 6500/8876 | Loss: 5.4773\n",
      "Epoch: 002/100 | Batch 6600/8876 | Loss: 5.2789\n",
      "Epoch: 002/100 | Batch 6700/8876 | Loss: 5.4903\n",
      "Epoch: 002/100 | Batch 6800/8876 | Loss: 5.3087\n",
      "Epoch: 002/100 | Batch 6900/8876 | Loss: 5.0728\n",
      "Epoch: 002/100 | Batch 7000/8876 | Loss: 5.4412\n",
      "Epoch: 002/100 | Batch 7100/8876 | Loss: 5.6091\n",
      "Epoch: 002/100 | Batch 7200/8876 | Loss: 5.4751\n",
      "Epoch: 002/100 | Batch 7300/8876 | Loss: 5.1850\n",
      "Epoch: 002/100 | Batch 7400/8876 | Loss: 5.5500\n",
      "Epoch: 002/100 | Batch 7500/8876 | Loss: 5.4059\n",
      "Epoch: 002/100 | Batch 7600/8876 | Loss: 5.1729\n",
      "Epoch: 002/100 | Batch 7700/8876 | Loss: 5.4363\n",
      "Epoch: 002/100 | Batch 7800/8876 | Loss: 4.9250\n",
      "Epoch: 002/100 | Batch 7900/8876 | Loss: 5.1370\n",
      "Epoch: 002/100 | Batch 8000/8876 | Loss: 5.1152\n",
      "Epoch: 002/100 | Batch 8100/8876 | Loss: 5.5553\n",
      "Epoch: 002/100 | Batch 8200/8876 | Loss: 5.1268\n",
      "Epoch: 002/100 | Batch 8300/8876 | Loss: 5.5143\n",
      "Epoch: 002/100 | Batch 8400/8876 | Loss: 5.5518\n",
      "Epoch: 002/100 | Batch 8500/8876 | Loss: 5.2991\n",
      "Epoch: 002/100 | Batch 8600/8876 | Loss: 5.2527\n",
      "Epoch: 002/100 | Batch 8700/8876 | Loss: 5.3628\n",
      "Epoch: 002/100 | Batch 8800/8876 | Loss: 5.3018\n",
      "月雨中中中晚晚来中三事\n",
      "无日无有日夜风三十十十\n",
      "一年今日一月春二十二篇韵\n",
      "有来何见人乐颂一十三二韵分\n",
      "Epoch: 003/100 | Batch 0000/8876 | Loss: 5.1442\n",
      "Epoch: 003/100 | Batch 0100/8876 | Loss: 5.1678\n",
      "Epoch: 003/100 | Batch 0200/8876 | Loss: 5.4064\n",
      "Epoch: 003/100 | Batch 0300/8876 | Loss: 5.1637\n",
      "Epoch: 003/100 | Batch 0400/8876 | Loss: 5.2849\n",
      "Epoch: 003/100 | Batch 0500/8876 | Loss: 5.6749\n",
      "Epoch: 003/100 | Batch 0600/8876 | Loss: 4.8380\n",
      "Epoch: 003/100 | Batch 0700/8876 | Loss: 5.3730\n",
      "Epoch: 003/100 | Batch 0800/8876 | Loss: 5.0655\n",
      "Epoch: 003/100 | Batch 0900/8876 | Loss: 5.5814\n",
      "Epoch: 003/100 | Batch 1000/8876 | Loss: 5.0970\n",
      "Epoch: 003/100 | Batch 1100/8876 | Loss: 5.1407\n",
      "Epoch: 003/100 | Batch 1200/8876 | Loss: 5.4287\n",
      "Epoch: 003/100 | Batch 1300/8876 | Loss: 5.3029\n",
      "Epoch: 003/100 | Batch 1400/8876 | Loss: 5.1074\n",
      "Epoch: 003/100 | Batch 1500/8876 | Loss: 5.4779\n",
      "Epoch: 003/100 | Batch 1600/8876 | Loss: 5.3192\n",
      "Epoch: 003/100 | Batch 1700/8876 | Loss: 5.2498\n",
      "Epoch: 003/100 | Batch 1800/8876 | Loss: 5.2720\n",
      "Epoch: 003/100 | Batch 1900/8876 | Loss: 5.3665\n",
      "Epoch: 003/100 | Batch 2000/8876 | Loss: 5.2744\n",
      "Epoch: 003/100 | Batch 2100/8876 | Loss: 5.1275\n",
      "Epoch: 003/100 | Batch 2200/8876 | Loss: 5.4208\n",
      "Epoch: 003/100 | Batch 2300/8876 | Loss: 5.6753\n",
      "Epoch: 003/100 | Batch 2400/8876 | Loss: 5.5467\n",
      "Epoch: 003/100 | Batch 2500/8876 | Loss: 5.2946\n",
      "Epoch: 003/100 | Batch 2600/8876 | Loss: 4.8881\n",
      "Epoch: 003/100 | Batch 2700/8876 | Loss: 5.8458\n",
      "Epoch: 003/100 | Batch 2800/8876 | Loss: 5.5058\n",
      "Epoch: 003/100 | Batch 2900/8876 | Loss: 5.4318\n",
      "Epoch: 003/100 | Batch 3000/8876 | Loss: 5.7880\n",
      "Epoch: 003/100 | Batch 3100/8876 | Loss: 5.4369\n",
      "Epoch: 003/100 | Batch 3200/8876 | Loss: 4.8413\n",
      "Epoch: 003/100 | Batch 3300/8876 | Loss: 5.7403\n",
      "Epoch: 003/100 | Batch 3400/8876 | Loss: 5.3033\n",
      "Epoch: 003/100 | Batch 3500/8876 | Loss: 5.6258\n",
      "Epoch: 003/100 | Batch 3600/8876 | Loss: 5.4822\n",
      "Epoch: 003/100 | Batch 3700/8876 | Loss: 5.4539\n",
      "Epoch: 003/100 | Batch 3800/8876 | Loss: 5.3327\n",
      "Epoch: 003/100 | Batch 3900/8876 | Loss: 5.0066\n",
      "Epoch: 003/100 | Batch 4000/8876 | Loss: 5.4005\n",
      "Epoch: 003/100 | Batch 4100/8876 | Loss: 5.3800\n",
      "Epoch: 003/100 | Batch 4200/8876 | Loss: 5.0863\n",
      "Epoch: 003/100 | Batch 4300/8876 | Loss: 4.9433\n",
      "Epoch: 003/100 | Batch 4400/8876 | Loss: 5.5189\n",
      "Epoch: 003/100 | Batch 4500/8876 | Loss: 5.4949\n",
      "Epoch: 003/100 | Batch 4600/8876 | Loss: 5.3398\n",
      "Epoch: 003/100 | Batch 4700/8876 | Loss: 4.9352\n",
      "Epoch: 003/100 | Batch 4800/8876 | Loss: 5.3485\n",
      "Epoch: 003/100 | Batch 4900/8876 | Loss: 5.2135\n",
      "Epoch: 003/100 | Batch 5000/8876 | Loss: 5.2757\n",
      "Epoch: 003/100 | Batch 5100/8876 | Loss: 5.8759\n",
      "Epoch: 003/100 | Batch 5200/8876 | Loss: 5.3232\n",
      "Epoch: 003/100 | Batch 5300/8876 | Loss: 5.3737\n",
      "Epoch: 003/100 | Batch 5400/8876 | Loss: 4.9004\n",
      "Epoch: 003/100 | Batch 5500/8876 | Loss: 5.5965\n",
      "Epoch: 003/100 | Batch 5600/8876 | Loss: 5.6813\n",
      "Epoch: 003/100 | Batch 5700/8876 | Loss: 5.2325\n",
      "Epoch: 003/100 | Batch 5800/8876 | Loss: 5.1788\n",
      "Epoch: 003/100 | Batch 5900/8876 | Loss: 5.2837\n",
      "Epoch: 003/100 | Batch 6000/8876 | Loss: 5.3162\n",
      "Epoch: 003/100 | Batch 6100/8876 | Loss: 5.5506\n",
      "Epoch: 003/100 | Batch 6200/8876 | Loss: 5.1147\n",
      "Epoch: 003/100 | Batch 6300/8876 | Loss: 5.3359\n",
      "Epoch: 003/100 | Batch 6400/8876 | Loss: 5.4025\n",
      "Epoch: 003/100 | Batch 6500/8876 | Loss: 5.4139\n",
      "Epoch: 003/100 | Batch 6600/8876 | Loss: 5.3475\n",
      "Epoch: 003/100 | Batch 6700/8876 | Loss: 4.6183\n",
      "Epoch: 003/100 | Batch 6800/8876 | Loss: 5.5139\n",
      "Epoch: 003/100 | Batch 6900/8876 | Loss: 5.6128\n",
      "Epoch: 003/100 | Batch 7000/8876 | Loss: 5.4826\n",
      "Epoch: 003/100 | Batch 7100/8876 | Loss: 5.3731\n",
      "Epoch: 003/100 | Batch 7200/8876 | Loss: 5.5015\n",
      "Epoch: 003/100 | Batch 7300/8876 | Loss: 5.5454\n",
      "Epoch: 003/100 | Batch 7400/8876 | Loss: 5.5388\n",
      "Epoch: 003/100 | Batch 7500/8876 | Loss: 5.4645\n",
      "Epoch: 003/100 | Batch 7600/8876 | Loss: 5.3633\n",
      "Epoch: 003/100 | Batch 7700/8876 | Loss: 5.3442\n",
      "Epoch: 003/100 | Batch 7800/8876 | Loss: 5.3564\n",
      "Epoch: 003/100 | Batch 7900/8876 | Loss: 5.0941\n",
      "Epoch: 003/100 | Batch 8000/8876 | Loss: 5.4445\n",
      "Epoch: 003/100 | Batch 8100/8876 | Loss: 5.2821\n",
      "Epoch: 003/100 | Batch 8200/8876 | Loss: 5.2932\n",
      "Epoch: 003/100 | Batch 8300/8876 | Loss: 5.3656\n",
      "Epoch: 003/100 | Batch 8400/8876 | Loss: 5.5608\n",
      "Epoch: 003/100 | Batch 8500/8876 | Loss: 5.2545\n",
      "Epoch: 003/100 | Batch 8600/8876 | Loss: 5.2440\n",
      "Epoch: 003/100 | Batch 8700/8876 | Loss: 5.5295\n",
      "Epoch: 003/100 | Batch 8800/8876 | Loss: 5.1928\n",
      "月花上上调儿。醉天前庭阳花花儿\n",
      "我今朝人来，今日是我来不的的那的不的那的那着那的你个我的是你个你不见\n",
      "Epoch: 004/100 | Batch 0000/8876 | Loss: 5.0944\n",
      "Epoch: 004/100 | Batch 0100/8876 | Loss: 5.5726\n",
      "Epoch: 004/100 | Batch 0200/8876 | Loss: 5.0632\n",
      "Epoch: 004/100 | Batch 0300/8876 | Loss: 5.6089\n",
      "Epoch: 004/100 | Batch 0400/8876 | Loss: 5.0666\n",
      "Epoch: 004/100 | Batch 0500/8876 | Loss: 4.9684\n",
      "Epoch: 004/100 | Batch 0600/8876 | Loss: 5.3224\n",
      "Epoch: 004/100 | Batch 0700/8876 | Loss: 5.2458\n",
      "Epoch: 004/100 | Batch 0800/8876 | Loss: 5.1882\n",
      "Epoch: 004/100 | Batch 0900/8876 | Loss: 5.7378\n",
      "Epoch: 004/100 | Batch 1000/8876 | Loss: 5.0194\n",
      "Epoch: 004/100 | Batch 1100/8876 | Loss: 5.2547\n",
      "Epoch: 004/100 | Batch 1200/8876 | Loss: 5.3708\n",
      "Epoch: 004/100 | Batch 1300/8876 | Loss: 5.2838\n",
      "Epoch: 004/100 | Batch 1400/8876 | Loss: 5.5392\n",
      "Epoch: 004/100 | Batch 1500/8876 | Loss: 5.3276\n",
      "Epoch: 004/100 | Batch 1600/8876 | Loss: 5.2785\n",
      "Epoch: 004/100 | Batch 1700/8876 | Loss: 5.4201\n",
      "Epoch: 004/100 | Batch 1800/8876 | Loss: 5.0839\n",
      "Epoch: 004/100 | Batch 1900/8876 | Loss: 4.8761\n",
      "Epoch: 004/100 | Batch 2000/8876 | Loss: 5.4088\n",
      "Epoch: 004/100 | Batch 2100/8876 | Loss: 4.9891\n",
      "Epoch: 004/100 | Batch 2200/8876 | Loss: 5.4853\n",
      "Epoch: 004/100 | Batch 2300/8876 | Loss: 5.2920\n",
      "Epoch: 004/100 | Batch 2400/8876 | Loss: 5.1284\n",
      "Epoch: 004/100 | Batch 2500/8876 | Loss: 5.2504\n",
      "Epoch: 004/100 | Batch 2600/8876 | Loss: 5.1943\n",
      "Epoch: 004/100 | Batch 2700/8876 | Loss: 5.0731\n",
      "Epoch: 004/100 | Batch 2800/8876 | Loss: 5.4359\n",
      "Epoch: 004/100 | Batch 2900/8876 | Loss: 5.0561\n",
      "Epoch: 004/100 | Batch 3000/8876 | Loss: 4.7747\n",
      "Epoch: 004/100 | Batch 3100/8876 | Loss: 5.1808\n",
      "Epoch: 004/100 | Batch 3200/8876 | Loss: 5.0800\n",
      "Epoch: 004/100 | Batch 3300/8876 | Loss: 5.2290\n",
      "Epoch: 004/100 | Batch 3400/8876 | Loss: 5.2642\n",
      "Epoch: 004/100 | Batch 3500/8876 | Loss: 5.0660\n",
      "Epoch: 004/100 | Batch 3600/8876 | Loss: 5.0327\n",
      "Epoch: 004/100 | Batch 3700/8876 | Loss: 5.1621\n",
      "Epoch: 004/100 | Batch 3800/8876 | Loss: 5.1543\n",
      "Epoch: 004/100 | Batch 3900/8876 | Loss: 5.4498\n",
      "Epoch: 004/100 | Batch 4000/8876 | Loss: 5.4621\n",
      "Epoch: 004/100 | Batch 4100/8876 | Loss: 5.2688\n",
      "Epoch: 004/100 | Batch 4200/8876 | Loss: 5.3278\n",
      "Epoch: 004/100 | Batch 4300/8876 | Loss: 5.0107\n",
      "Epoch: 004/100 | Batch 4400/8876 | Loss: 5.3889\n",
      "Epoch: 004/100 | Batch 4500/8876 | Loss: 5.5055\n",
      "Epoch: 004/100 | Batch 4600/8876 | Loss: 5.2792\n",
      "Epoch: 004/100 | Batch 4700/8876 | Loss: 5.4005\n",
      "Epoch: 004/100 | Batch 4800/8876 | Loss: 5.3514\n",
      "Epoch: 004/100 | Batch 4900/8876 | Loss: 5.0489\n",
      "Epoch: 004/100 | Batch 5000/8876 | Loss: 5.1258\n",
      "Epoch: 004/100 | Batch 5100/8876 | Loss: 5.4863\n",
      "Epoch: 004/100 | Batch 5200/8876 | Loss: 5.0104\n",
      "Epoch: 004/100 | Batch 5300/8876 | Loss: 4.9260\n",
      "Epoch: 004/100 | Batch 5400/8876 | Loss: 5.1129\n",
      "Epoch: 004/100 | Batch 5500/8876 | Loss: 5.3076\n",
      "Epoch: 004/100 | Batch 5600/8876 | Loss: 5.2851\n",
      "Epoch: 004/100 | Batch 5700/8876 | Loss: 5.1159\n",
      "Epoch: 004/100 | Batch 5800/8876 | Loss: 5.0246\n",
      "Epoch: 004/100 | Batch 5900/8876 | Loss: 5.5278\n",
      "Epoch: 004/100 | Batch 6000/8876 | Loss: 5.4140\n",
      "Epoch: 004/100 | Batch 6100/8876 | Loss: 5.0942\n",
      "Epoch: 004/100 | Batch 6200/8876 | Loss: 5.3651\n",
      "Epoch: 004/100 | Batch 6300/8876 | Loss: 5.4944\n",
      "Epoch: 004/100 | Batch 6400/8876 | Loss: 4.6951\n",
      "Epoch: 004/100 | Batch 6500/8876 | Loss: 5.4761\n",
      "Epoch: 004/100 | Batch 6600/8876 | Loss: 5.0310\n",
      "Epoch: 004/100 | Batch 6700/8876 | Loss: 5.3508\n",
      "Epoch: 004/100 | Batch 6800/8876 | Loss: 5.1040\n",
      "Epoch: 004/100 | Batch 6900/8876 | Loss: 5.3640\n",
      "Epoch: 004/100 | Batch 7000/8876 | Loss: 5.3362\n",
      "Epoch: 004/100 | Batch 7100/8876 | Loss: 5.6880\n",
      "Epoch: 004/100 | Batch 7200/8876 | Loss: 4.8637\n",
      "Epoch: 004/100 | Batch 7300/8876 | Loss: 5.0567\n",
      "Epoch: 004/100 | Batch 7400/8876 | Loss: 5.1975\n",
      "Epoch: 004/100 | Batch 7500/8876 | Loss: 5.2489\n",
      "Epoch: 004/100 | Batch 7600/8876 | Loss: 5.3198\n",
      "Epoch: 004/100 | Batch 7700/8876 | Loss: 4.9833\n",
      "Epoch: 004/100 | Batch 7800/8876 | Loss: 5.5097\n",
      "Epoch: 004/100 | Batch 7900/8876 | Loss: 5.0701\n",
      "Epoch: 004/100 | Batch 8000/8876 | Loss: 5.3395\n",
      "Epoch: 004/100 | Batch 8100/8876 | Loss: 5.0187\n",
      "Epoch: 004/100 | Batch 8200/8876 | Loss: 5.1871\n",
      "Epoch: 004/100 | Batch 8300/8876 | Loss: 5.1010\n",
      "Epoch: 004/100 | Batch 8400/8876 | Loss: 5.0297\n",
      "Epoch: 004/100 | Batch 8500/8876 | Loss: 5.2489\n",
      "Epoch: 004/100 | Batch 8600/8876 | Loss: 5.1110\n",
      "Epoch: 004/100 | Batch 8700/8876 | Loss: 5.4156\n",
      "Epoch: 004/100 | Batch 8800/8876 | Loss: 4.7486\n",
      "月水居晚\n",
      "可是今岁夜夜凉四\n",
      "相复今有不有乐其二\n",
      "自是山中不自乐诗二\n",
      "可复春日有日年一二韵韵\n",
      "Epoch: 005/100 | Batch 0000/8876 | Loss: 5.0767\n",
      "Epoch: 005/100 | Batch 0100/8876 | Loss: 5.1190\n",
      "Epoch: 005/100 | Batch 0200/8876 | Loss: 5.2914\n",
      "Epoch: 005/100 | Batch 0300/8876 | Loss: 5.2621\n",
      "Epoch: 005/100 | Batch 0400/8876 | Loss: 5.1695\n",
      "Epoch: 005/100 | Batch 0500/8876 | Loss: 5.2272\n",
      "Epoch: 005/100 | Batch 0600/8876 | Loss: 5.2657\n",
      "Epoch: 005/100 | Batch 0700/8876 | Loss: 5.4030\n",
      "Epoch: 005/100 | Batch 0800/8876 | Loss: 5.0933\n",
      "Epoch: 005/100 | Batch 0900/8876 | Loss: 5.2285\n",
      "Epoch: 005/100 | Batch 1000/8876 | Loss: 5.3068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m             model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, data_loader, optimizer, criterion, vocab_size, grad_clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# batch_x_lens = batch_x_lens.to(device)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# batch_y_lens = batch_y_lens.to(device)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m batch_pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m batch_pred_y \u001b[38;5;241m=\u001b[39m batch_pred_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n\u001b[1;32m     21\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[69], line 26\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[0;34m(self, batch_x, batch_x_lens)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_x, batch_x_lens):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_lens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 37\u001b[0m, in \u001b[0;36mCharRNN.encoder\u001b[0;34m(self, batch_x, batch_x_lens)\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(\n\u001b[1;32m     33\u001b[0m     batch_x, batch_x_lens, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m batch_x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_layer1(batch_x)\n\u001b[0;32m---> 37\u001b[0m batch_x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_layer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m batch_x, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(batch_x, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(batch_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights_have_changed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    762\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n\u001b[1;32m    764\u001b[0m     orig_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase._weights_have_changed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m weights_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names):\n\u001b[0;32m--> 246\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ref() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m weight:\n\u001b[1;32m    248\u001b[0m         weights_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size, grad_clip=1.0):\n",
    "    ###################\n",
    "    # 训练 #\n",
    "    ###################\n",
    "    min_loss = np.Inf\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model = model.to(device)\n",
    "        for batch_idx, (batch_x, batch_y, batch_x_lens, batch_y_lens) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 将数据移动到GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            # batch_x_lens = batch_x_lens.to(device)\n",
    "            # batch_y_lens = batch_y_lens.to(device)\n",
    "\n",
    "            batch_pred_y = model(batch_x, batch_x_lens)\n",
    "\n",
    "            batch_pred_y = batch_pred_y.view(-1, vocab_size)\n",
    "            batch_y = batch_y.view(-1)\n",
    "\n",
    "            loss = criterion(batch_pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch:03d}/{num_epochs:03d} | Batch {batch_idx:04d}/{len(data_loader):04d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(model.state_dict(), \"char_rnn_model.pth\")\n",
    "        # 每个epoch结束后进行生成测试\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.cpu()\n",
    "            generated_text = model.generator(\"月\")\n",
    "            print(\"\".join(generated_text))\n",
    "            model.train()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "train(model, epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "月夜雨\n",
      "春凉不不见无人，一月不无何无时。泣何年年人不是也一日\n",
      "年有一生不如天生事中事里不未不不似二首\n",
      "\n",
      "月月上上\n",
      "年无不知无时来不，不知不能知情心苦。\n",
      "春入天不不不见去处好后\n",
      "天的天明无来时，一笑相知非人\n",
      "\n",
      "月雨中有书中不作\n",
      "一朝无无何事名士，何有一时来时情无无无写。・我你不得相相思相成也也不相相，(醉云江\n",
      "\n",
      "月雨日二咏二韵杂题 明夜水水二十五首呈德仲之寺三绝 呈万第八首二首\n",
      "人无不如何事事不是不用一月作一月\n",
      "\n",
      "月日雨二十四绝韵 明日二下之一卷\n",
      "年朝，山，有无名，人无知情苦草二韵四咏 。圯\n",
      "他千千尺天天天。 \n",
      "\n",
      "月夜作三十四古 成\n",
      "人不能能知人不，无今见相怜无。泣无人事人不物\n",
      "春入春月落，清光香时。酒酒作\n",
      "年朝\n",
      "\n",
      "月日出\n",
      "一无今不知，春风香无。怀\n",
      "春风明风，无何知名士酒写。圯丸上\n",
      "你(带云叶平州州平生上庭子子州王\n",
      "\n",
      "月日夜起之韵四卷二咏\n",
      "人生，不得尽好一事\n",
      "天的无不知无人，无是不知人无心。儿外时，平风香开，玉金天无\n",
      "\n",
      "月月\n",
      "人生，不见长秀无用\n",
      "人不知无无，天然知难知。追日，无何似无。蒙生日日月中不可寄\n",
      "年是天子如无，\n",
      "\n",
      "月雨上二日有中有作一一首四绝 \n",
      "秋光雨香香香草花雪而一壑\n",
      "人知是难能成中人事，不见人人间不难人。\n",
      "人\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        print(\"\".join(model.generator(\"月\")))\n",
    "        print()\n",
    "    for i in range(10):\n",
    "        print(\"\".join(model.generator(\"天\")))\n",
    "        print()\n",
    "    for i in range(10):\n",
    "        print(\"\".join(model.generator(\"人\")))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"char_rnn_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
