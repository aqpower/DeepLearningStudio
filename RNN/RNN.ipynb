{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义RNN单元的参数\n",
    "input_size = 3  # 输入向量的维度\n",
    "hidden_size = 2  # 隐藏状态的维度\n",
    "output_size = 1  # 输出向量的维度\n",
    "\n",
    "# 初始化权重矩阵\n",
    "W_xh = np.random.randn(hidden_size, input_size)  # 输入到隐藏状态的权重\n",
    "W_hh = np.random.randn(hidden_size, hidden_size)  # 隐藏状态到隐藏状态的权重\n",
    "W_hy = np.random.randn(output_size, hidden_size)  # 隐藏状态到输出的权重\n",
    "\n",
    "# 初始化偏置\n",
    "b_h = np.zeros((hidden_size, 1))  # 隐藏状态的偏置\n",
    "b_y = np.zeros((output_size, 1))  # 输出的偏置\n",
    "\n",
    "\n",
    "def rnn_step(x_t, h_prev):\n",
    "    # 计算隐藏状态\n",
    "    h_t = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, h_prev) + b_h)\n",
    "    # 计算输出\n",
    "    y_t = np.dot(W_hy, h_t) + b_y\n",
    "    return h_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入序列\n",
    "x_sequence = [np.random.randn(input_size, 1) for _ in range(5)]  # 假设长度为5的输入序列\n",
    "# 真实输出\n",
    "y_sequence = [np.random.randn(output_size, 1) for _ in range(5)]\n",
    "\n",
    "\n",
    "def forward_propagation(x_sequence):\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "    y_preds = []\n",
    "    h_states = []\n",
    "    for x_t in x_sequence:\n",
    "        h_t, y_t = rnn_step(x_t, h_prev)\n",
    "        y_preds.append(y_t)\n",
    "        h_states.append(h_t)\n",
    "        h_prev = h_t\n",
    "    return y_preds, h_states\n",
    "\n",
    "\n",
    "def calculate_loss(y_preds, y_true):\n",
    "    loss = 0\n",
    "    for y_pred, y_t in zip(y_preds, y_true):\n",
    "        loss += np.sum((y_pred - y_t) ** 2)\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x_sequence, y_sequence, y_preds, h_states):\n",
    "    dW_xh = np.zeros_like(W_xh)\n",
    "    dW_hh = np.zeros_like(W_hh)\n",
    "    dW_hy = np.zeros_like(W_hy)\n",
    "    db_h = np.zeros_like(b_h)\n",
    "    db_y = np.zeros_like(b_y)\n",
    "\n",
    "    dh_next = np.zeros_like(h_states[0])\n",
    "\n",
    "    for t in reversed(range(len(x_sequence))):\n",
    "        dy = y_preds[t] - y_sequence[t]\n",
    "        dW_hy += np.dot(dy, h_states[t].T)\n",
    "        db_y += dy\n",
    "\n",
    "        dh = np.dot(W_hy.T, dy) + dh_next\n",
    "        dh_raw = (1 - h_states[t] ** 2) * dh  # tanh的导数\n",
    "\n",
    "        db_h += dh_raw\n",
    "        dW_xh += np.dot(dh_raw, x_sequence[t].T)\n",
    "        dW_hh += np.dot(dh_raw, h_states[t - 1].T)\n",
    "\n",
    "        dh_next = np.dot(W_hh.T, dh_raw)\n",
    "\n",
    "    return dW_xh, dW_hh, dW_hy, db_h, db_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义学习率\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def update_parameters(dW_xh, dW_hh, dW_hy, db_h, db_y):\n",
    "    global W_xh, W_hh, W_hy, b_h, b_y\n",
    "    W_xh -= learning_rate * dW_xh\n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_hy -= learning_rate * dW_hy\n",
    "    b_h -= learning_rate * db_h\n",
    "    b_y -= learning_rate * db_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train_rnn(x_sequences, y_sequences, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_sequence, y_sequence in zip(x_sequences, y_sequences):\n",
    "            # 前向传播\n",
    "            y_preds, h_states = forward_propagation(x_sequence)\n",
    "            # 计算损失\n",
    "            loss = calculate_loss(y_preds, y_sequence)\n",
    "            total_loss += loss\n",
    "            # 反向传播\n",
    "            dW_xh, dW_hh, dW_hy, db_h, db_y = backward_propagation(\n",
    "                x_sequence, y_sequence, y_preds, h_states\n",
    "            )\n",
    "            # 更新参数\n",
    "            update_parameters(dW_xh, dW_hh, dW_hy, db_h, db_y)\n",
    "\n",
    "        # 打印每个epoch的平均损失\n",
    "        average_loss = total_loss / len(x_sequences)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.1649\n",
      "Epoch 2/100, Loss: 1.0315\n",
      "Epoch 3/100, Loss: 1.0217\n",
      "Epoch 4/100, Loss: 1.0187\n",
      "Epoch 5/100, Loss: 1.0173\n",
      "Epoch 6/100, Loss: 1.0165\n",
      "Epoch 7/100, Loss: 1.0159\n",
      "Epoch 8/100, Loss: 1.0155\n",
      "Epoch 9/100, Loss: 1.0153\n",
      "Epoch 10/100, Loss: 1.0150\n",
      "Epoch 11/100, Loss: 1.0149\n",
      "Epoch 12/100, Loss: 1.0147\n",
      "Epoch 13/100, Loss: 1.0145\n",
      "Epoch 14/100, Loss: 1.0144\n",
      "Epoch 15/100, Loss: 1.0143\n",
      "Epoch 16/100, Loss: 1.0142\n",
      "Epoch 17/100, Loss: 1.0141\n",
      "Epoch 18/100, Loss: 1.0140\n",
      "Epoch 19/100, Loss: 1.0139\n",
      "Epoch 20/100, Loss: 1.0138\n",
      "Epoch 21/100, Loss: 1.0137\n",
      "Epoch 22/100, Loss: 1.0136\n",
      "Epoch 23/100, Loss: 1.0135\n",
      "Epoch 24/100, Loss: 1.0134\n",
      "Epoch 25/100, Loss: 1.0134\n",
      "Epoch 26/100, Loss: 1.0133\n",
      "Epoch 27/100, Loss: 1.0132\n",
      "Epoch 28/100, Loss: 1.0131\n",
      "Epoch 29/100, Loss: 1.0131\n",
      "Epoch 30/100, Loss: 1.0130\n",
      "Epoch 31/100, Loss: 1.0129\n",
      "Epoch 32/100, Loss: 1.0129\n",
      "Epoch 33/100, Loss: 1.0128\n",
      "Epoch 34/100, Loss: 1.0128\n",
      "Epoch 35/100, Loss: 1.0127\n",
      "Epoch 36/100, Loss: 1.0126\n",
      "Epoch 37/100, Loss: 1.0126\n",
      "Epoch 38/100, Loss: 1.0125\n",
      "Epoch 39/100, Loss: 1.0125\n",
      "Epoch 40/100, Loss: 1.0124\n",
      "Epoch 41/100, Loss: 1.0124\n",
      "Epoch 42/100, Loss: 1.0123\n",
      "Epoch 43/100, Loss: 1.0123\n",
      "Epoch 44/100, Loss: 1.0122\n",
      "Epoch 45/100, Loss: 1.0122\n",
      "Epoch 46/100, Loss: 1.0121\n",
      "Epoch 47/100, Loss: 1.0121\n",
      "Epoch 48/100, Loss: 1.0120\n",
      "Epoch 49/100, Loss: 1.0120\n",
      "Epoch 50/100, Loss: 1.0119\n",
      "Epoch 51/100, Loss: 1.0119\n",
      "Epoch 52/100, Loss: 1.0118\n",
      "Epoch 53/100, Loss: 1.0118\n",
      "Epoch 54/100, Loss: 1.0118\n",
      "Epoch 55/100, Loss: 1.0117\n",
      "Epoch 56/100, Loss: 1.0117\n",
      "Epoch 57/100, Loss: 1.0116\n",
      "Epoch 58/100, Loss: 1.0116\n",
      "Epoch 59/100, Loss: 1.0115\n",
      "Epoch 60/100, Loss: 1.0115\n",
      "Epoch 61/100, Loss: 1.0114\n",
      "Epoch 62/100, Loss: 1.0114\n",
      "Epoch 63/100, Loss: 1.0113\n",
      "Epoch 64/100, Loss: 1.0113\n",
      "Epoch 65/100, Loss: 1.0112\n",
      "Epoch 66/100, Loss: 1.0112\n",
      "Epoch 67/100, Loss: 1.0111\n",
      "Epoch 68/100, Loss: 1.0110\n",
      "Epoch 69/100, Loss: 1.0110\n",
      "Epoch 70/100, Loss: 1.0109\n",
      "Epoch 71/100, Loss: 1.0108\n",
      "Epoch 72/100, Loss: 1.0108\n",
      "Epoch 73/100, Loss: 1.0107\n",
      "Epoch 74/100, Loss: 1.0106\n",
      "Epoch 75/100, Loss: 1.0106\n",
      "Epoch 76/100, Loss: 1.0105\n",
      "Epoch 77/100, Loss: 1.0105\n",
      "Epoch 78/100, Loss: 1.0104\n",
      "Epoch 79/100, Loss: 1.0103\n",
      "Epoch 80/100, Loss: 1.0103\n",
      "Epoch 81/100, Loss: 1.0102\n",
      "Epoch 82/100, Loss: 1.0102\n",
      "Epoch 83/100, Loss: 1.0101\n",
      "Epoch 84/100, Loss: 1.0101\n",
      "Epoch 85/100, Loss: 1.0101\n",
      "Epoch 86/100, Loss: 1.0100\n",
      "Epoch 87/100, Loss: 1.0100\n",
      "Epoch 88/100, Loss: 1.0099\n",
      "Epoch 89/100, Loss: 1.0099\n",
      "Epoch 90/100, Loss: 1.0099\n",
      "Epoch 91/100, Loss: 1.0099\n",
      "Epoch 92/100, Loss: 1.0098\n",
      "Epoch 93/100, Loss: 1.0098\n",
      "Epoch 94/100, Loss: 1.0098\n",
      "Epoch 95/100, Loss: 1.0098\n",
      "Epoch 96/100, Loss: 1.0098\n",
      "Epoch 97/100, Loss: 1.0098\n",
      "Epoch 98/100, Loss: 1.0098\n",
      "Epoch 99/100, Loss: 1.0097\n",
      "Epoch 100/100, Loss: 1.0097\n"
     ]
    }
   ],
   "source": [
    "# 生成训练数据\n",
    "x_sequences = [[np.random.randn(input_size, 1) for _ in range(5)] for _ in range(100)]\n",
    "y_sequences = [[np.random.randn(output_size, 1) for _ in range(5)] for _ in range(100)]\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "train_rnn(x_sequences, y_sequences, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山一程，水一程，身向榆关那畔行，夜深千帐灯 风一更，雪一更，聒碎乡心梦不成，故园无此声 人生若只如初见，何事秋风悲画扇 等闲变却故人心，却道故心人易变 骊山语罢清宵半，泪雨霖铃终不怨 何如薄幸锦衣郎，\n",
      "字符集大小: 1855\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"./data/纳兰性德诗集.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 提取所有诗句\n",
    "texts = \"\"\n",
    "for poem in data:\n",
    "    for para in poem[\"para\"]:\n",
    "        texts += para + \" \"  # 使用空格分隔段落\n",
    "\n",
    "# 合并所有诗句为一个长文本\n",
    "text = \"\".join(texts)\n",
    "\n",
    "# 打印部分文本\n",
    "print(text[:100])\n",
    "\n",
    "# 创建字符映射\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# 打印字符集大小\n",
    "print(f\"字符集大小: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "input_size = len(chars)  # 输入大小为字符集的大小\n",
    "hidden_size = 100  # 隐藏层大小\n",
    "output_size = len(chars)  # 输出大小为字符集的大小\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重初始化\n",
    "W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_y = np.zeros((output_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(char, char_to_ix):\n",
    "    vec = np.zeros((len(char_to_ix), 1))\n",
    "    vec[char_to_ix[char]] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def forward_backward_propagation(inputs, targets, h_prev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    loss = 0\n",
    "\n",
    "    # 前向传播\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = one_hot_encoding(inputs[t], char_to_ix)\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t - 1]) + b_h)\n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # 计算softmax\n",
    "        loss += -np.log(ps[t][char_to_ix[targets[t]], 0])  # 计算交叉熵损失\n",
    "\n",
    "    # 反向传播\n",
    "    dW_xh, dW_hh, dW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "    db_h, db_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[char_to_ix[targets[t]]] -= 1  # 计算softmax梯度\n",
    "        dW_hy += np.dot(dy, hs[t].T)\n",
    "        db_y += dy\n",
    "\n",
    "        dh = np.dot(W_hy.T, dy) + dh_next\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh  # tanh的梯度\n",
    "        db_h += dh_raw\n",
    "        dW_xh += np.dot(dh_raw, xs[t].T)\n",
    "        dW_hh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        dh_next = np.dot(W_hh.T, dh_raw)\n",
    "\n",
    "    for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # 防止梯度爆炸\n",
    "\n",
    "    return loss, dW_xh, dW_hh, dW_hy, db_h, db_y, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(dW_xh, dW_hh, dW_hy, db_h, db_y):\n",
    "    global W_xh, W_hh, W_hy, b_h, b_y\n",
    "    W_xh -= learning_rate * dW_xh\n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_hy -= learning_rate * dW_hy\n",
    "    b_h -= learning_rate * db_h\n",
    "    b_y -= learning_rate * db_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"基于给定的初始隐藏状态h和种子字符索引seed_ix生成n个字符\"\"\"\n",
    "    x = np.zeros((input_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(input_size), p=p.ravel())\n",
    "        x = np.zeros((input_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    txt = \"\".join(ix_to_char[ix] for ix in ixes)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(data, epochs=10):\n",
    "    n, p = 0, 0\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if p + 25 + 1 >= len(data) or n == 0:\n",
    "            h_prev = np.zeros((hidden_size, 1))  # 重置RNN隐藏状态\n",
    "            p = 0  # 回到数据起点\n",
    "\n",
    "        inputs = data[p : p + 25]\n",
    "        targets = data[p + 1 : p + 26]\n",
    "\n",
    "        loss, dW_xh, dW_hh, dW_hy, db_h, db_y, h_prev = forward_backward_propagation(\n",
    "            inputs, targets, h_prev\n",
    "        )\n",
    "        update_parameters(dW_xh, dW_hh, dW_hy, db_h, db_y)\n",
    "\n",
    "        p += 25  # 移动数据窗口\n",
    "        n += 1\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Iteration {n}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Iteration 100, Loss: 167.0605\n",
      "Epoch 200, Iteration 200, Loss: 166.0205\n",
      "Epoch 300, Iteration 300, Loss: 150.7272\n",
      "Epoch 400, Iteration 400, Loss: 155.5009\n",
      "Epoch 500, Iteration 500, Loss: 142.5543\n",
      "Epoch 600, Iteration 600, Loss: 143.6045\n",
      "Epoch 700, Iteration 700, Loss: 155.8265\n",
      "Epoch 800, Iteration 800, Loss: 169.2960\n",
      "Epoch 900, Iteration 900, Loss: 157.1262\n",
      "Epoch 1000, Iteration 1000, Loss: 163.7645\n",
      "Epoch 1100, Iteration 1100, Loss: 151.6745\n",
      "Epoch 1200, Iteration 1200, Loss: 160.0344\n",
      "Epoch 1300, Iteration 1300, Loss: 153.8980\n",
      "Epoch 1400, Iteration 1400, Loss: 150.3319\n",
      "Epoch 1500, Iteration 1500, Loss: 173.1241\n",
      "Epoch 1600, Iteration 1600, Loss: 157.6420\n",
      "Epoch 1700, Iteration 1700, Loss: 156.4189\n",
      "Epoch 1800, Iteration 1800, Loss: 154.4341\n",
      "Epoch 1900, Iteration 1900, Loss: 158.9368\n",
      "Epoch 2000, Iteration 2000, Loss: 172.3141\n",
      "Epoch 2100, Iteration 2100, Loss: 150.0004\n",
      "Epoch 2200, Iteration 2200, Loss: 178.2223\n",
      "Epoch 2300, Iteration 2300, Loss: 169.0898\n",
      "Epoch 2400, Iteration 2400, Loss: 168.6741\n",
      "Epoch 2500, Iteration 2500, Loss: 154.8429\n",
      "Epoch 2600, Iteration 2600, Loss: 168.7515\n",
      "Epoch 2700, Iteration 2700, Loss: 150.8451\n",
      "Epoch 2800, Iteration 2800, Loss: 178.1717\n",
      "Epoch 2900, Iteration 2900, Loss: 146.7791\n",
      "Epoch 3000, Iteration 3000, Loss: 155.8367\n",
      "Epoch 3100, Iteration 3100, Loss: 157.6907\n",
      "Epoch 3200, Iteration 3200, Loss: 143.5936\n",
      "Epoch 3300, Iteration 3300, Loss: 166.4783\n",
      "Epoch 3400, Iteration 3400, Loss: 155.0367\n",
      "Epoch 3500, Iteration 3500, Loss: 159.8326\n",
      "Epoch 3600, Iteration 3600, Loss: 151.7206\n",
      "Epoch 3700, Iteration 3700, Loss: 162.1467\n",
      "Epoch 3800, Iteration 3800, Loss: 159.4389\n",
      "Epoch 3900, Iteration 3900, Loss: 153.0368\n",
      "Epoch 4000, Iteration 4000, Loss: 159.9382\n",
      "Epoch 4100, Iteration 4100, Loss: 171.3848\n",
      "Epoch 4200, Iteration 4200, Loss: 157.4336\n",
      "Epoch 4300, Iteration 4300, Loss: 155.1299\n",
      "Epoch 4400, Iteration 4400, Loss: 155.5697\n",
      "Epoch 4500, Iteration 4500, Loss: 165.8144\n",
      "Epoch 4600, Iteration 4600, Loss: 151.6656\n",
      "Epoch 4700, Iteration 4700, Loss: 149.1839\n",
      "Epoch 4800, Iteration 4800, Loss: 152.2193\n",
      "Epoch 4900, Iteration 4900, Loss: 170.5237\n",
      "Epoch 5000, Iteration 5000, Loss: 152.8277\n",
      "Epoch 5100, Iteration 5100, Loss: 152.1859\n",
      "Epoch 5200, Iteration 5200, Loss: 150.2791\n",
      "Epoch 5300, Iteration 5300, Loss: 159.9924\n",
      "Epoch 5400, Iteration 5400, Loss: 151.0568\n",
      "Epoch 5500, Iteration 5500, Loss: 168.7391\n",
      "Epoch 5600, Iteration 5600, Loss: 164.8685\n",
      "Epoch 5700, Iteration 5700, Loss: 152.6723\n",
      "Epoch 5800, Iteration 5800, Loss: 177.2693\n",
      "Epoch 5900, Iteration 5900, Loss: 147.0375\n",
      "Epoch 6000, Iteration 6000, Loss: 165.3861\n",
      "Epoch 6100, Iteration 6100, Loss: 139.0865\n",
      "Epoch 6200, Iteration 6200, Loss: 164.8788\n",
      "Epoch 6300, Iteration 6300, Loss: 151.2705\n",
      "Epoch 6400, Iteration 6400, Loss: 153.8473\n",
      "Epoch 6500, Iteration 6500, Loss: 156.4786\n",
      "Epoch 6600, Iteration 6600, Loss: 159.5812\n",
      "Epoch 6700, Iteration 6700, Loss: 154.9233\n",
      "Epoch 6800, Iteration 6800, Loss: 157.5151\n",
      "Epoch 6900, Iteration 6900, Loss: 151.1309\n",
      "Epoch 7000, Iteration 7000, Loss: 173.4901\n",
      "Epoch 7100, Iteration 7100, Loss: 151.3279\n",
      "Epoch 7200, Iteration 7200, Loss: 168.5079\n",
      "Epoch 7300, Iteration 7300, Loss: 166.8987\n",
      "Epoch 7400, Iteration 7400, Loss: 154.4559\n",
      "Epoch 7500, Iteration 7500, Loss: 155.9190\n",
      "Epoch 7600, Iteration 7600, Loss: 156.8819\n",
      "Epoch 7700, Iteration 7700, Loss: 171.2107\n",
      "Epoch 7800, Iteration 7800, Loss: 148.2165\n",
      "Epoch 7900, Iteration 7900, Loss: 172.3014\n",
      "Epoch 8000, Iteration 8000, Loss: 153.8560\n",
      "Epoch 8100, Iteration 8100, Loss: 145.4284\n",
      "Epoch 8200, Iteration 8200, Loss: 154.0726\n",
      "Epoch 8300, Iteration 8300, Loss: 149.6581\n",
      "Epoch 8400, Iteration 8400, Loss: 162.7975\n",
      "Epoch 8500, Iteration 8500, Loss: 150.4676\n",
      "Epoch 8600, Iteration 8600, Loss: 155.5719\n",
      "Epoch 8700, Iteration 8700, Loss: 152.4721\n",
      "Epoch 8800, Iteration 8800, Loss: 150.5313\n",
      "Epoch 8900, Iteration 8900, Loss: 175.7780\n",
      "Epoch 9000, Iteration 9000, Loss: 157.4108\n",
      "Epoch 9100, Iteration 9100, Loss: 159.2384\n",
      "Epoch 9200, Iteration 9200, Loss: 149.1256\n",
      "Epoch 9300, Iteration 9300, Loss: 173.6820\n",
      "Epoch 9400, Iteration 9400, Loss: 166.1246\n",
      "Epoch 9500, Iteration 9500, Loss: 142.7127\n",
      "Epoch 9600, Iteration 9600, Loss: 178.6087\n",
      "Epoch 9700, Iteration 9700, Loss: 159.3324\n",
      "Epoch 9800, Iteration 9800, Loss: 164.0676\n",
      "Epoch 9900, Iteration 9900, Loss: 181.4998\n",
      "Epoch 10000, Iteration 10000, Loss: 161.5564\n",
      "生成的文本:\n",
      "月海丝少取倚似 清看老\n"
     ]
    }
   ],
   "source": [
    "# 训练完成后生成文本\n",
    "n_epochs = 10000  # 设置训练轮数\n",
    "train_rnn(text, epochs=n_epochs)\n",
    "\n",
    "# 设置种子字符和生成长度\n",
    "seed_char = \"月\"  # 可以选择任何一个字符作为种子字符\n",
    "seed_ix = char_to_ix[seed_char]\n",
    "generated_text_length = 10  # 生成文本的长度\n",
    "\n",
    "# 生成文本\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "sampled_text = sample(h_prev, seed_ix, generated_text_length)\n",
    "print(f\"生成的文本:\\n{sampled_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
