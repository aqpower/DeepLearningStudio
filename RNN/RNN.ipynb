{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "max_length = 99\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = opencc.OpenCC(\"t2s\")\n",
    "\n",
    "\n",
    "def sentenceParse(para):\n",
    "    para = re.sub(r\"（.*?）\", \"\", para)\n",
    "    para = re.sub(r\"{.*?}\", \"\", para)\n",
    "    para = re.sub(r\"《.*?》\", \"\", para)\n",
    "    para = re.sub(r\"[\\[\\]]\", \"\", para)\n",
    "    para = \"\".join([s for s in para if s not in \"0123456789-\"])\n",
    "    para = re.sub(r\"。。\", \"。\", para)\n",
    "    para = converter.convert(para)\n",
    "    if \"𫗋\" in para:\n",
    "        return \"\"\n",
    "    return para\n",
    "\n",
    "\n",
    "def parseRawData(author=None, constrain=None):\n",
    "    def handleJson(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        rst = []\n",
    "        for poetry in data:\n",
    "            if author and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "\n",
    "            paragraphs = poetry.get(\"paragraphs\")\n",
    "            if any(\n",
    "                len(tr) != constrain and len(tr) != 0\n",
    "                for s in paragraphs\n",
    "                for tr in re.split(\"[，！。]\", s)\n",
    "                if constrain is not None\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            pdata = \"\".join(paragraphs)\n",
    "            pdata = sentenceParse(pdata)\n",
    "            if pdata:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    src_path = Path(\"./data/chinese-poetry-master/全唐诗/\")\n",
    "    for file_path in src_path.glob(\"poet.tang*\"):\n",
    "        data.extend(handleJson(file_path))\n",
    "    # for file_path in src_path.glob(\"poet.song*\"):\n",
    "    # data.extend(handleJson(file_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = parseRawData(author=\"李白\")  # All if author=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 3514\n",
      "data_size 1206\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "word_to_index = {}\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index[\"<EOP>\"] = len(word_to_index)  # End Of Poem token\n",
    "word_to_index[\"<START>\"] = len(word_to_index)  # Start token\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size)\n",
    "print(\"data_size\", len(poems))\n",
    "\n",
    "\n",
    "# 将句子转换为列表形式，并添加结束符\n",
    "def sentence_to_list(sentence):\n",
    "    return list(sentence) + [\"<EOP>\"]\n",
    "\n",
    "\n",
    "poems = [sentence_to_list(poem) for poem in poems]\n",
    "\n",
    "\n",
    "# 创建单词到one-hot向量的映射\n",
    "def create_one_hot_vector(word, word_to_index):\n",
    "    return torch.autograd.Variable(torch.LongTensor([word_to_index[word]]))\n",
    "\n",
    "\n",
    "one_hot_vectors = {\n",
    "    word: create_one_hot_vector(word, word_to_index) for word in word_to_index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sequence, one_hot_encoding):\n",
    "    # 打印原始序列（可选）\n",
    "    # print(sequence)\n",
    "\n",
    "    # 使用列表推导式生成输入和输出的 one-hot 编码\n",
    "    inputs = [one_hot_encoding[sequence[i - 1]] for i in range(1, len(sequence))]\n",
    "    outputs = [one_hot_encoding[sequence[i]] for i in range(1, len(sequence))]\n",
    "\n",
    "    # 将输入和输出列表合并为张量\n",
    "    encoded_inputs = torch.cat(inputs)\n",
    "    encoded_outputs = torch.cat(outputs)\n",
    "\n",
    "    return encoded_inputs, encoded_outputs\n",
    "\n",
    "\n",
    "# generate_sample(poems[0], one_hot_vectors)\n",
    "\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, poems, transform=None):\n",
    "        self.poems = poems\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        input_data, output_data = generate_sample(poem, one_hot_vectors)\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "        return input_data, output_data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # 统一长度以进行批处理\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    padded_targets = nn.utils.rnn.pad_sequence(\n",
    "        targets, batch_first=True, padding_value=word_to_index[\"<START>\"]\n",
    "    )\n",
    "    return padded_sequences, padded_targets\n",
    "\n",
    "\n",
    "dataset = PoetryDataset(poems)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )  # Enable batch_first\n",
    "        self.linear1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # Adjusted for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.embeddings(input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # Adjusted view for batch processing, removing hard-coded lengths\n",
    "        output = self.linear1(F.relu(lstm_out.contiguous().view(-1, self.hidden_dim)))\n",
    "        # output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        # Reshape output to (batch_size, seq_len, vocab_size) for compatibility\n",
    "        output = output.view(input.size(0), input.size(1), -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, device, batch_size=1):\n",
    "        # 在LSTM网络中，每个单元有两个隐藏状态：一个是长期状态（通常表示为c），另一个是短期状态（通常表示为h）。\n",
    "        # 这两个状态共同帮助LSTM单元记住信息并处理复杂的序列依赖。\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoetryModel(len(word_to_index), 256, 256)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = torch.nn.NLLLoss(ignore_index=word_to_index[\"<START>\"], reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, data_loader, optimizer, criterion, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (sequence, target) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            hidden = model.initHidden(device=device, batch_size=sequence.size(0))\n",
    "            output, hidden = model(sequence.to(device), hidden)\n",
    "            loss = criterion(output.view(-1, vocab_size), target.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if not batch_idx % 100:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | Batch {batch_idx:05d}/{len(data_loader):05d} | Loss: {loss:.4f}\"\n",
    "                )\n",
    "    torch.save(model.state_dict(), \"poetry-gen.pth\")\n",
    "\n",
    "\n",
    "# train(model, num_epochs, data_loader, optimizer, criterion, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"poetry-gen.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['江']\n",
      "江水如碧海，长啸入云烟。高峰望远道，云门隔古道。人生古可道，云尽空余哀。\n",
      "['泉']\n",
      "泉水东北流，波荡双鸳鸯。飞燕燕汉国，飞龙与天通。人生寒松草，草木不相连。坐思天上月，空余碧玉道。云行无知老，吾将何时？。\n",
      "['泉']\n",
      "泉花满水国，水明湖江流。清光出江海，流水游江流。月光碧山月，水清清猨月。长松风落日，清夜夜清风。闲入玉窗里，云在罗罗衣。何言是我意，独去难为别。\n",
      "['泉']\n",
      "泉月东楼湖，风流天上来。青山几度月，日月照古人。遥见一里月，三千久空山。一为李白发，何由入秋浦。山公一问石，东海扬中州。落子结归去，沙去空天人。\n",
      "['沾', '衣', '欲', '湿', '杏', '花', '雨']\n",
      "沾衣欲湿杏花雨，春风拂槛露华香。若飞云汉去，宫花艳舞香。\n",
      "['风']\n",
      "吹: 0.6101\n",
      "日: 0.2246\n",
      "云: 0.1653\n",
      "风吹\n",
      "玉: 0.5230\n",
      "落: 0.3101\n",
      "花: 0.1669\n",
      "风吹玉\n",
      "关: 0.4486\n",
      "笛: 0.3014\n",
      "树: 0.2500\n",
      "风吹玉关\n",
      "道: 0.4830\n",
      "西: 0.2705\n",
      "山: 0.2465\n",
      "风吹玉关西\n",
      "，: 0.6674\n",
      "入: 0.2822\n",
      "海: 0.0504\n",
      "风吹玉关西入\n",
      "吴: 0.5667\n",
      "汉: 0.3348\n",
      "胡: 0.0985\n",
      "风吹玉关西入吴\n",
      "关: 0.4777\n",
      "越: 0.2828\n",
      "云: 0.2394\n",
      "风吹玉关西入吴关\n",
      "，: 0.9440\n",
      "之: 0.0398\n",
      "城: 0.0163\n",
      "风吹玉关西入吴关，\n",
      "云: 0.4393\n",
      "南: 0.3470\n",
      "见: 0.2137\n",
      "风吹玉关西入吴关，南\n",
      "云: 0.8726\n",
      "行: 0.0846\n",
      "国: 0.0429\n",
      "风吹玉关西入吴关，南云\n",
      "莫: 0.3643\n",
      "日: 0.3494\n",
      "白: 0.2862\n",
      "风吹玉关西入吴关，南云白\n",
      "日: 0.8426\n",
      "马: 0.1348\n",
      "帝: 0.0226\n",
      "风吹玉关西入吴关，南云白日\n",
      "之: 0.5122\n",
      "出: 0.2900\n",
      "夜: 0.1977\n",
      "风吹玉关西入吴关，南云白日出\n",
      "天: 0.5100\n",
      "处: 0.2594\n",
      "夜: 0.2306\n",
      "风吹玉关西入吴关，南云白日出处\n",
      "处: 0.4709\n",
      "归: 0.3631\n",
      "生: 0.1659\n",
      "风吹玉关西入吴关，南云白日出处生\n",
      "。: 0.9977\n",
      "，: 0.0019\n",
      "？: 0.0004\n",
      "风吹玉关西入吴关，南云白日出处生。\n",
      "天: 0.5042\n",
      "白: 0.2664\n",
      "三: 0.2294\n",
      "风吹玉关西入吴关，南云白日出处生。白\n",
      "云: 0.6943\n",
      "日: 0.1870\n",
      "杨: 0.1187\n",
      "风吹玉关西入吴关，南云白日出处生。白日\n",
      "暮: 0.3754\n",
      "高: 0.3490\n",
      "当: 0.2756\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮\n",
      "高: 0.6882\n",
      "行: 0.1727\n",
      "月: 0.1391\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高\n",
      "心: 0.4420\n",
      "高: 0.2812\n",
      "天: 0.2768\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天\n",
      "上: 0.8527\n",
      "下: 0.1098\n",
      "，: 0.0375\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上\n",
      "月: 0.8369\n",
      "雪: 0.0859\n",
      "天: 0.0771\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月\n",
      "，: 0.9999\n",
      "。: 0.0001\n",
      "何: 0.0000\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，\n",
      "人: 0.4628\n",
      "不: 0.2884\n",
      "独: 0.2488\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独\n",
      "宿: 0.4913\n",
      "天: 0.3477\n",
      "坐: 0.1610\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿\n",
      "天: 0.8921\n",
      "孤: 0.0639\n",
      "吴: 0.0440\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天\n",
      "门: 0.5088\n",
      "地: 0.3123\n",
      "台: 0.1789\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门\n",
      "月: 0.4669\n",
      "中: 0.3642\n",
      "长: 0.1689\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门月\n",
      "中: 0.4600\n",
      "下: 0.2748\n",
      "人: 0.2652\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门月人\n",
      "行: 0.4001\n",
      "间: 0.3622\n",
      "来: 0.2377\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门月人行\n",
      "。: 0.9993\n",
      "？: 0.0003\n",
      "，: 0.0003\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门月人行。\n",
      "长: 0.5138\n",
      "<EOP>: 0.3531\n",
      "水: 0.1331\n",
      "风吹玉关西入吴关，南云白日出处生。白日暮高天上月，独宿天门月人行。\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_word=\"<START>\", top_k=1, log=False):\n",
    "    generated_text = \"\"\n",
    "    words = []\n",
    "    for word in start_word:\n",
    "        words += [word]\n",
    "    print(words)\n",
    "    hidden_state = model.initHidden(device=device)\n",
    "    with torch.no_grad():\n",
    "        vectors_list = []\n",
    "        for word in words:\n",
    "            word_vector = torch.LongTensor([word_to_index[word]]).unsqueeze(0)\n",
    "            vectors_list.append(word_vector)\n",
    "            generated_text += word\n",
    "\n",
    "        input_vector = torch.cat(vectors_list, dim=1)\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output, hidden_state = model(input_vector.to(device), hidden_state)\n",
    "            last_word = output[:, -1, :]\n",
    "            last_word = last_word.view(-1)\n",
    "            top_values, top_indices = last_word.data.topk(top_k)\n",
    "\n",
    "            probabilities = torch.exp(top_values)\n",
    "            top_words = [index_to_word[index.item()] for index in top_indices]\n",
    "\n",
    "            probabilities_np = probabilities.cpu().detach().numpy()\n",
    "            probabilities_np = probabilities_np / probabilities_np.sum()\n",
    "            indices_np = top_indices.cpu().detach().numpy()\n",
    "            if log:\n",
    "                for word, prob in zip(top_words, probabilities_np):\n",
    "                    print(f\"{word}: {prob:.4f}\")\n",
    "                    \n",
    "            selected_index = np.random.choice(indices_np, p=probabilities_np)\n",
    "\n",
    "            next_word = index_to_word[selected_index]\n",
    "            if next_word == \"<EOP>\":\n",
    "                break\n",
    "            generated_text += next_word\n",
    "            if log:\n",
    "                print(generated_text)\n",
    "            # * 需要升一个维\n",
    "            input_vector = torch.LongTensor([word_to_index[next_word]]).unsqueeze(0)\n",
    "\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "print(generate_text(\"江\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=1))\n",
    "print(generate_text(\"泉\", top_k=3))\n",
    "print(generate_text(\"泉\", top_k=30))\n",
    "print(generate_text(\"沾衣欲湿杏花雨\", top_k=3))\n",
    "print(generate_text(\"风\", top_k=3, log=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
