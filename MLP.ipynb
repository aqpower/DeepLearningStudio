{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(\n",
    "        self, num_input_node, num_hidden_node, num_output_node, learning_rate=0.01\n",
    "    ):\n",
    "        \"\"\"initialize the multi-layer perceptron\n",
    "\n",
    "        Args:\n",
    "            num_input_node: number of input nodes\n",
    "            num_hidden_node: number of hidden nodes\n",
    "            num_output_node: number of output nodes\n",
    "            learning_rate: learning rate. Defaults to 0.01\n",
    "        \"\"\"\n",
    "        self.num_input_node = num_input_node\n",
    "        self.num_hidden_node = num_hidden_node\n",
    "        self.num_output_node = num_output_node\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.wih = np.random.normal(\n",
    "            0.0,\n",
    "            pow(self.num_hidden_node, -0.5),\n",
    "            (self.num_hidden_node, self.num_input_node),\n",
    "        )\n",
    "        self.who = np.random.normal(\n",
    "            0,\n",
    "            pow(self.num_output_node, -0.5),\n",
    "            (self.num_output_node, self.num_hidden_node),\n",
    "        )\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.activation_function = lambda x: expit(x)\n",
    "\n",
    "    def predict(self, input_item):\n",
    "        \"\"\"predict the output of the neural network\n",
    "\n",
    "        Args:\n",
    "            inputs: input data\n",
    "        \"\"\"\n",
    "        out_hidden = self.activation_function(np.dot(self.wih, input_item))\n",
    "        out_output = self.activation_function(np.dot(self.who, out_hidden))\n",
    "\n",
    "        ground_truth = np.argmax(out_output)\n",
    "        return ground_truth\n",
    "\n",
    "    def train(self, input, label, iter_nums=20):\n",
    "        \"\"\"train the neural network using input data and label\n",
    "\n",
    "        Args:\n",
    "            input: input data\n",
    "            label: label\n",
    "            iter_nums (int, optional): train iteration counts. Defaults to 10.\n",
    "        \"\"\"\n",
    "        # correct_cnt = 0\n",
    "        for i in range(iter_nums):\n",
    "            # step 1: predict the output\n",
    "            out_hidden = self.activation_function(np.dot(self.wih, input))\n",
    "            out_output = self.activation_function(np.dot(self.who, out_hidden))\n",
    "\n",
    "            # step 2: compute the error\n",
    "            error_output = label - out_output\n",
    "            error_hidden = np.dot(self.who.T, error_output)\n",
    "\n",
    "            # step 3: update the weights\n",
    "            self.who += self.lr * np.dot(\n",
    "                (error_output * out_output * (1.0 - out_output)),\n",
    "                np.transpose(out_hidden),\n",
    "            )\n",
    "            self.wih += self.lr * np.dot(\n",
    "                (error_hidden * out_hidden * (1.0 - out_hidden)), np.transpose(input)\n",
    "            )\n",
    "\n",
    "            # calculate the correct radix\n",
    "            # ground_truth = np.argmax(out_output)\n",
    "            # label_val = np.where(label == 1)[0][0]\n",
    "            # if ground_truth == label_val:\n",
    "            # correct_cnt += 1\n",
    "\n",
    "            # if i % 49 == 0:\n",
    "            #     print(\n",
    "            #         f\"iteration {i} label:{label_val} ground truth:{ ground_truth}, Correct: {label_val == ground_truth}\"\n",
    "            #     )\n",
    "        # print(f\"correct radix is {correct_cnt / iter_nums}\")\n",
    "\n",
    "    def evaluate_model(self, test_data):\n",
    "        \"\"\"Evaluate the MLP model on test_data.\n",
    "\n",
    "        Args:\n",
    "            test_data (list): List of test samples where each sample is a tuple (input, label).\n",
    "\n",
    "        Returns:\n",
    "            accuracy (float): Accuracy of the model on the test dataset.\n",
    "            average_loss (float): Average loss of the model on the test dataset.\n",
    "        \"\"\"\n",
    "        correct_predictions = 0\n",
    "        total_loss = 0\n",
    "        num_samples = len(test_data)\n",
    "\n",
    "        for _, sample in enumerate(test_data):\n",
    "            input_data, true_label = sample\n",
    "            input_data = np.array(input_data) / 255.0\n",
    "            input_data = input_data.reshape(28 * 28, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            hidden_output = self.activation_function(np.dot(self.wih, input_data))\n",
    "            final_output = self.activation_function(np.dot(self.who, hidden_output))\n",
    "\n",
    "            # Predict the label\n",
    "            predicted_label = self.predict(input_data)\n",
    "\n",
    "            # Check if the prediction is correct\n",
    "            if predicted_label == true_label:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            # Create a one-hot encoded vector for the true label\n",
    "            true_label_one_hot = np.zeros(final_output.shape)\n",
    "            true_label_one_hot[true_label] = 1\n",
    "\n",
    "            # Compute the loss for this sample\n",
    "            sample_loss = -np.sum(true_label_one_hot * np.log(final_output))\n",
    "            total_loss += sample_loss\n",
    "\n",
    "        # Calculate average loss and accuracy\n",
    "        average_loss = total_loss / num_samples\n",
    "        accuracy = correct_predictions / num_samples\n",
    "\n",
    "        # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        # print(f\"Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "        return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_data = MNIST(root=\"data\", train=True, download=True)\n",
    "test_data = MNIST(root=\"data\", train=False, download=True)\n",
    "train_list = list(train_data)\n",
    "test_list = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(lr):\n",
    "    num_input_node = 28 * 28\n",
    "    num_hidden_node = 80\n",
    "    num_output_node = 10\n",
    "\n",
    "    mlp_model = MLP(num_input_node, num_hidden_node, num_output_node, lr)\n",
    "\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    iteration_list = []\n",
    "\n",
    "    # train only one train sample during a train epoch\n",
    "    for i, item in enumerate(train_list):\n",
    "        input = np.array(item[0])\n",
    "        # Normalize the input because the input is the pixel value of the image, and the activation function is sigmoid\n",
    "        input = (input / 255.0).reshape(28 * 28, 1)\n",
    "        label = np.zeros((10, 1))\n",
    "        label[item[1]] = 1\n",
    "\n",
    "        mlp_model.train(input, label)\n",
    "\n",
    "        # evaluate the model every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy, train_loss = mlp_model.evaluate_model(train_list)\n",
    "            test_accuracy, test_loss = mlp_model.evaluate_model(test_list)\n",
    "\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            train_loss_list.append(train_loss)\n",
    "            test_loss_list.append(test_loss)\n",
    "            iteration_list.append(i)\n",
    "\n",
    "            print(\n",
    "                f\"•----- lr {lr:<5} Iteration {i:<5} ----- Train Accuracy: {train_accuracy:<6.2f} ----- Test Accuracy: {test_accuracy:<6.2f} ----- Train Loss: {train_loss:<6.2f} ----- Test Loss: {test_loss:<6.2f} -----•\"\n",
    "            )\n",
    "\n",
    "    # visualize the accuracy plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        iteration_list,\n",
    "        train_accuracy_list,\n",
    "        label=\"Training Accuracy\",\n",
    "        color=\"blue\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        iteration_list,\n",
    "        test_accuracy_list,\n",
    "        label=\"Testing Accuracy\",\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Iterations\", fontsize=14)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "    plt.title(f\"Training and Testing Accuracy (Learning Rate: {lr})\", fontsize=16)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # save the plot as svg and emf \n",
    "    svg_path_accuracy = f\"./images/mnist_train_lr{lr}_accuracy_plot.svg\"\n",
    "    emf_path_accuracy = f\"./images/mnist_train_lr{lr}_accuracy_plot.emf\"\n",
    "    plt.savefig(svg_path_accuracy)\n",
    "    subprocess.run(\n",
    "        f\"inkscape --export-filename={emf_path_accuracy} {svg_path_accuracy}\",\n",
    "        shell=True,\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # visualize the loss plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        iteration_list,\n",
    "        train_loss_list,\n",
    "        label=\"Training Loss\",\n",
    "        color=\"red\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        iteration_list,\n",
    "        test_loss_list,\n",
    "        label=\"Testing Loss\",\n",
    "        color=\"orange\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.ylim(0, max(max(train_loss_list), max(test_loss_list)) * 1.1)\n",
    "    plt.xlabel(\"Iterations\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "    plt.title(f\"Training and Testing Loss (Learning Rate: {lr})\", fontsize=16)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    svg_path_loss = f\"./images/mnist_train_lr{lr}_loss_plot.svg\"\n",
    "    emf_path_loss = f\"./images/mnist_train_lr{lr}_loss_plot.emf\"\n",
    "    plt.savefig(svg_path_loss)\n",
    "    subprocess.run(\n",
    "        f\"inkscape --export-filename={emf_path_loss} {svg_path_loss}\", shell=True\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•----- lr 0.001 Iteration 0     ----- Train Accuracy: 0.11   ----- Test Accuracy: 0.11   ----- Train Loss: 1.03   ----- Test Loss: 1.03   -----•\n",
      "•----- lr 0.001 Iteration 100   ----- Train Accuracy: 0.37   ----- Test Accuracy: 0.36   ----- Train Loss: 1.81   ----- Test Loss: 1.84   -----•\n",
      "•----- lr 0.001 Iteration 200   ----- Train Accuracy: 0.52   ----- Test Accuracy: 0.51   ----- Train Loss: 1.56   ----- Test Loss: 1.57   -----•\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train at different learning rates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_mlp(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_mlp(\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[0;34m(lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# evaluate the model every 100 iterations\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     train_accuracy, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     test_accuracy, test_loss \u001b[38;5;241m=\u001b[39m mlp_model\u001b[38;5;241m.\u001b[39mevaluate_model(test_list)\n\u001b[1;32m     29\u001b[0m     train_accuracy_list\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "Cell \u001b[0;32mIn[7], line 118\u001b[0m, in \u001b[0;36mMLP.evaluate_model\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m    115\u001b[0m     true_label_one_hot[true_label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Compute the loss for this sample\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     sample_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_label_one_hot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sample_loss\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Calculate average loss and accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train at different learning rates\n",
    "train_mlp(0.001)\n",
    "train_mlp(0.01)\n",
    "train_mlp(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
