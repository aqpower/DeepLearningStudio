{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from matplotlib.ticker import MultipleLocator, FuncFormatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_input_node, num_hidden_node, num_output_node, learning_rate):\n",
    "        \"\"\"initialize the multi-layer perceptron\n",
    "\n",
    "        Args:\n",
    "            num_input_node: number of input nodes\n",
    "            num_hidden_node: number of hidden nodes\n",
    "            num_output_node: number of output nodes\n",
    "        \"\"\"\n",
    "        self.num_input_node = num_input_node\n",
    "        self.num_hidden_node = num_hidden_node\n",
    "        self.num_output_node = num_output_node\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.wih = np.random.normal(\n",
    "            0.0,\n",
    "            pow(self.num_hidden_node, -0.5),\n",
    "            (self.num_hidden_node, self.num_input_node),\n",
    "        )\n",
    "        self.who = np.random.normal(\n",
    "            0,\n",
    "            pow(self.num_output_node, -0.5),\n",
    "            (self.num_output_node, self.num_hidden_node),\n",
    "        )\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.activation_function = lambda x: expit(x)\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\"predict the output of the neural network\n",
    "\n",
    "        Args:\n",
    "            inputs: input data\n",
    "        \"\"\"\n",
    "        out_hidden = self.activation_function(np.dot(self.wih, input))\n",
    "        out_output = self.activation_function(np.dot(self.who, out_hidden))\n",
    "\n",
    "        ground_truth = np.argmax(out_output)\n",
    "        return ground_truth\n",
    "\n",
    "    def train(self, input, label, iter_nums=100):\n",
    "        # correct_cnt = 0\n",
    "        for i in range(iter_nums):\n",
    "            # step 1: predict the output\n",
    "            # print(\"wih shape: \",self.wih.shape)\n",
    "            # print(\"input shape: \", input.shape)\n",
    "            out_hidden = self.activation_function(np.dot(self.wih, input))\n",
    "            out_output = self.activation_function(np.dot(self.who, out_hidden))\n",
    "            # ground_truth = np.argmax(out_output)\n",
    "            # label_val = np.where(label == 1)[0][0]\n",
    "            \n",
    "            # step 2: compute the error\n",
    "            error_output = label - out_output\n",
    "            error_hidden = np.dot(self.who.T, error_output)\n",
    "\n",
    "            # step 3: update the weights\n",
    "            self.who += self.lr * np.dot(\n",
    "                (error_output * out_output * (1.0 - out_output)), np.transpose(out_hidden)\n",
    "            )\n",
    "            self.wih += self.lr * np.dot(\n",
    "                (error_hidden * out_hidden * (1.0 - out_hidden)), np.transpose(input)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            # if ground_truth == label_val:\n",
    "                # correct_cnt += 1\n",
    "\n",
    "            # if i % 49 == 0:\n",
    "            #     print(\n",
    "            #         f\"iteration {i} label:{label_val} ground truth:{ ground_truth}, Correct: {label_val == ground_truth}\"\n",
    "            #     )\n",
    "        # print(f\"the correct radix is {correct_cnt / 100.0}\")\n",
    "        \n",
    "    def test(self, test_list):\n",
    "        correct_cnt = 0\n",
    "        for _, item in enumerate(test_list):\n",
    "            input = np.array(item[0])\n",
    "            # plt.imshow(input, cmap=\"gray\")\n",
    "            # plt.show()\n",
    "            input = (input / 255.0).reshape(28 * 28, 1)\n",
    "            ground_truth = self.predict(input)\n",
    "            # print(\"ground truth: \", ground_truth)\n",
    "            # print(\"label: \", item[1])\n",
    "            if ground_truth == item[1]:\n",
    "                correct_cnt += 1\n",
    "        # print(f\"correct radix: {correct_cnt / len(test_list)}\")\n",
    "        return correct_cnt / len(test_list)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_data = MNIST(root=\"data\", train=True, download=True)\n",
    "test_data = MNIST(root=\"data\", train=False, download=True)\n",
    "train_list = list(train_data)\n",
    "test_list = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(lr):\n",
    "    num_input_node = 28 * 28\n",
    "    num_hidden_node = 99\n",
    "    num_output_node = 10\n",
    "\n",
    "    mlp_model = MLP(num_input_node, num_hidden_node, num_output_node, lr)\n",
    "\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    iteration_list = []\n",
    "\n",
    "    for i, item in enumerate(train_list):\n",
    "        input = np.array(item[0])\n",
    "        # !remember to normalize the input\n",
    "        input = (input / 255.0).reshape(28 * 28, 1)\n",
    "        label = np.zeros((10, 1))\n",
    "        label[item[1]] = 1\n",
    "        mlp_model.train(input, label)\n",
    "        if i % 100 == 0:  # 每训练100个样本就测试一次\n",
    "            train_accuracy = mlp_model.test(train_list)  # 计算训练集上的正确率\n",
    "            test_accuracy = mlp_model.test(test_list)  # 计算测试集上的正确率\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            iteration_list.append(i)  # 记录当前迭代次数\n",
    "            print(f\"=== lr: {lr} test {i} ===\")\n",
    "            print(f\"Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # 可视化正确率变化\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.plot(iteration_list, train_accuracy_list, label=\"Training Accuracy\")\n",
    "    plt.plot(iteration_list, test_accuracy_list, label=\"Testing Accuracy\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Training and Testing Accuracy (Learning Rate: {lr})\")\n",
    "    plt.legend()\n",
    "\n",
    "    svg_path = f\"./images/mnist_train_{lr}_plot.svg\"\n",
    "    emf_path = f\"./images/mnist_train_{lr}_plot.emf\"\n",
    "    plt.savefig(svg_path)\n",
    "    # 使用 Inkscape 将 SVG 转换为 EMF\n",
    "    subprocess.run(f\"inkscape --export-filename={emf_path} {svg_path}\", shell=True)\n",
    "    # 显示图表\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== lr: 0.001 test 0 ===\n",
      "Train Accuracy: 0.07926666666666667, Test Accuracy: 0.078\n",
      "=== lr: 0.001 test 100 ===\n",
      "Train Accuracy: 0.59335, Test Accuracy: 0.5867\n",
      "=== lr: 0.001 test 200 ===\n",
      "Train Accuracy: 0.6940166666666666, Test Accuracy: 0.6834\n",
      "=== lr: 0.001 test 300 ===\n",
      "Train Accuracy: 0.6845666666666667, Test Accuracy: 0.6851\n",
      "=== lr: 0.001 test 400 ===\n",
      "Train Accuracy: 0.73495, Test Accuracy: 0.737\n",
      "=== lr: 0.001 test 500 ===\n",
      "Train Accuracy: 0.7377166666666667, Test Accuracy: 0.7362\n",
      "=== lr: 0.001 test 600 ===\n",
      "Train Accuracy: 0.7963166666666667, Test Accuracy: 0.7979\n",
      "=== lr: 0.001 test 700 ===\n",
      "Train Accuracy: 0.73555, Test Accuracy: 0.727\n",
      "=== lr: 0.001 test 800 ===\n",
      "Train Accuracy: 0.8069333333333333, Test Accuracy: 0.8069\n",
      "=== lr: 0.001 test 900 ===\n",
      "Train Accuracy: 0.8182666666666667, Test Accuracy: 0.8179\n",
      "=== lr: 0.001 test 1000 ===\n",
      "Train Accuracy: 0.7699166666666667, Test Accuracy: 0.7721\n",
      "=== lr: 0.001 test 1100 ===\n",
      "Train Accuracy: 0.82145, Test Accuracy: 0.8252\n",
      "=== lr: 0.001 test 1200 ===\n",
      "Train Accuracy: 0.8226, Test Accuracy: 0.83\n",
      "=== lr: 0.001 test 1300 ===\n",
      "Train Accuracy: 0.8253666666666667, Test Accuracy: 0.8271\n",
      "=== lr: 0.001 test 1400 ===\n",
      "Train Accuracy: 0.8323833333333334, Test Accuracy: 0.8366\n",
      "=== lr: 0.001 test 1500 ===\n",
      "Train Accuracy: 0.85335, Test Accuracy: 0.8568\n",
      "=== lr: 0.001 test 1600 ===\n",
      "Train Accuracy: 0.8480833333333333, Test Accuracy: 0.8522\n",
      "=== lr: 0.001 test 1700 ===\n",
      "Train Accuracy: 0.8403, Test Accuracy: 0.8475\n",
      "=== lr: 0.001 test 1800 ===\n",
      "Train Accuracy: 0.8236, Test Accuracy: 0.8255\n",
      "=== lr: 0.001 test 1900 ===\n",
      "Train Accuracy: 0.8512833333333333, Test Accuracy: 0.8583\n",
      "=== lr: 0.001 test 2000 ===\n",
      "Train Accuracy: 0.8331166666666666, Test Accuracy: 0.8345\n",
      "=== lr: 0.001 test 2100 ===\n",
      "Train Accuracy: 0.84695, Test Accuracy: 0.854\n",
      "=== lr: 0.001 test 2200 ===\n",
      "Train Accuracy: 0.8611666666666666, Test Accuracy: 0.8694\n",
      "=== lr: 0.001 test 2300 ===\n",
      "Train Accuracy: 0.8644833333333334, Test Accuracy: 0.8687\n",
      "=== lr: 0.001 test 2400 ===\n",
      "Train Accuracy: 0.8574666666666667, Test Accuracy: 0.8593\n",
      "=== lr: 0.001 test 2500 ===\n",
      "Train Accuracy: 0.8266, Test Accuracy: 0.8371\n",
      "=== lr: 0.001 test 2600 ===\n",
      "Train Accuracy: 0.8652, Test Accuracy: 0.8715\n",
      "=== lr: 0.001 test 2700 ===\n",
      "Train Accuracy: 0.86955, Test Accuracy: 0.8754\n",
      "=== lr: 0.001 test 2800 ===\n",
      "Train Accuracy: 0.8606166666666667, Test Accuracy: 0.868\n",
      "=== lr: 0.001 test 2900 ===\n",
      "Train Accuracy: 0.8564166666666667, Test Accuracy: 0.8666\n",
      "=== lr: 0.001 test 3000 ===\n",
      "Train Accuracy: 0.878, Test Accuracy: 0.8847\n",
      "=== lr: 0.001 test 3100 ===\n",
      "Train Accuracy: 0.8446333333333333, Test Accuracy: 0.8543\n",
      "=== lr: 0.001 test 3200 ===\n",
      "Train Accuracy: 0.8828833333333334, Test Accuracy: 0.8892\n",
      "=== lr: 0.001 test 3300 ===\n",
      "Train Accuracy: 0.8771, Test Accuracy: 0.8801\n",
      "=== lr: 0.001 test 3400 ===\n",
      "Train Accuracy: 0.8615, Test Accuracy: 0.8669\n",
      "=== lr: 0.001 test 3500 ===\n",
      "Train Accuracy: 0.8755666666666667, Test Accuracy: 0.8769\n",
      "=== lr: 0.001 test 3600 ===\n",
      "Train Accuracy: 0.85855, Test Accuracy: 0.8663\n",
      "=== lr: 0.001 test 3700 ===\n",
      "Train Accuracy: 0.87625, Test Accuracy: 0.8821\n",
      "=== lr: 0.001 test 3800 ===\n",
      "Train Accuracy: 0.8698833333333333, Test Accuracy: 0.875\n",
      "=== lr: 0.001 test 3900 ===\n",
      "Train Accuracy: 0.8877166666666667, Test Accuracy: 0.8909\n",
      "=== lr: 0.001 test 4000 ===\n",
      "Train Accuracy: 0.8812166666666666, Test Accuracy: 0.888\n",
      "=== lr: 0.001 test 4100 ===\n",
      "Train Accuracy: 0.8753, Test Accuracy: 0.8817\n",
      "=== lr: 0.001 test 4200 ===\n",
      "Train Accuracy: 0.8767166666666667, Test Accuracy: 0.883\n",
      "=== lr: 0.001 test 4300 ===\n",
      "Train Accuracy: 0.8680333333333333, Test Accuracy: 0.8744\n",
      "=== lr: 0.001 test 4400 ===\n",
      "Train Accuracy: 0.8891666666666667, Test Accuracy: 0.8906\n",
      "=== lr: 0.001 test 4500 ===\n",
      "Train Accuracy: 0.87645, Test Accuracy: 0.8796\n",
      "=== lr: 0.001 test 4600 ===\n",
      "Train Accuracy: 0.8846333333333334, Test Accuracy: 0.8913\n",
      "=== lr: 0.001 test 4700 ===\n",
      "Train Accuracy: 0.86665, Test Accuracy: 0.8733\n",
      "=== lr: 0.001 test 4800 ===\n",
      "Train Accuracy: 0.8707333333333334, Test Accuracy: 0.8754\n",
      "=== lr: 0.001 test 4900 ===\n",
      "Train Accuracy: 0.8823833333333333, Test Accuracy: 0.8884\n",
      "=== lr: 0.001 test 5000 ===\n",
      "Train Accuracy: 0.8901833333333333, Test Accuracy: 0.8952\n",
      "=== lr: 0.001 test 5100 ===\n",
      "Train Accuracy: 0.88745, Test Accuracy: 0.8924\n",
      "=== lr: 0.001 test 5200 ===\n",
      "Train Accuracy: 0.8891, Test Accuracy: 0.8958\n",
      "=== lr: 0.001 test 5300 ===\n",
      "Train Accuracy: 0.8925166666666666, Test Accuracy: 0.8949\n",
      "=== lr: 0.001 test 5400 ===\n",
      "Train Accuracy: 0.8915, Test Accuracy: 0.8972\n",
      "=== lr: 0.001 test 5500 ===\n",
      "Train Accuracy: 0.89335, Test Accuracy: 0.899\n",
      "=== lr: 0.001 test 5600 ===\n",
      "Train Accuracy: 0.8853, Test Accuracy: 0.8897\n"
     ]
    }
   ],
   "source": [
    "train_mlp(0.001)\n",
    "train_mlp(0.01)\n",
    "train_mlp(0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
